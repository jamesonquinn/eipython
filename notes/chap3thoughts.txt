
Standard deviations: line 41 1 : tensor(0.4330) tensor(0.9354)
tensor([ 0.5000, -0.2500, -0.2500])
 tensor([[-1.0000,  0.0000,  1.0000],
        [-0.5000,  1.0000, -0.5000],
        [ 1.5000, -1.0000, -0.5000]])
 tensor([[-0.5000, -0.2500,  0.7500],
        [ 0.0000,  0.7500, -0.7500],
        [ 2.0000, -1.2500, -0.7500]])
Standard deviations: line 41 2 : tensor(0.4330) tensor(0.9354)
R line 41 1 : <class 'str'> 3
C line 41 1 : <class 'str'> 3
U line 41 1 : <class 'str'> 527
eiresults/scenario_N527.csv from file
svi.step(... line 41 1 : 0 26.35 torch.Size([20, 9])
guide:begin line 41 1 : 26.35 True
types? line 100 1 : [torch.float64, torch.float64]
types? line 100 2 : [torch.float64, torch.float64]
model:end line 41 1 :
lp:  line 41 1 : tensor(-1653064.6835, grad_fn=<AddBackward0>) 542
> c:\users\jameson\dropbox\eipython\eipython\ei2.py(800)guide()
-> if do_print:
(Pdb) logits[1]
tensor([[-0.4542, -0.2751,  0.7452],
        [ 0.0479,  0.6968, -0.6438],
        [ 2.0207, -2.5808, -0.8146]], grad_fn=<SelectBackward>)
(Pdb) inits
{'logsdrcstar': tensor(-0.0668, requires_grad=True), 'logsdprcstar': tensor(-2.9957, requires_grad=True), 'ecstar_raw': tensor([ 0.5000, -0.2500], requires_grad=True), 'ercstar_raw': tensor([[-1.0000,  0.0000],
        [-0.5000,  1.0000]])}
(Pdb) logsdprcstar
tensor(-2.9957, requires_grad=True)
(Pdb) logits[2]
tensor([[-0.5125, -0.2803,  0.7641],
        [ 0.0051,  0.7341, -0.6942],
        [ 1.9868, -1.1282, -0.6440]], grad_fn=<SelectBackward>)
(Pdb) ns[1]
tensor([1138.,   49.,   59.])
(Pdb) vs[1]
tensor([288., 266., 692.])
(Pdb) Q[1]
tensor([[1.7417e-01, 1.9265e-01, 5.4651e-01],
        [1.2118e-02, 2.0735e-02, 6.4720e-03],
        [4.4854e-02, 1.0109e-04, 2.3960e-03]], grad_fn=<SelectBackward>)
(Pdb) ystars[1]
tensor([[2.1701e+02, 2.4004e+02, 6.8095e+02],
        [1.5100e+01, 2.5836e+01, 8.0641e+00],
        [5.5889e+01, 1.2596e-01, 2.9854e+00]], grad_fn=<SelectBackward>)
(Pdb) logresidual[1]
tensor([[ 0.0964, -0.0528, -0.0101],
        [ 0.1008, -0.1121,  0.2236],
        [ 0.0436, -2.8016, -0.1360]], grad_fn=<SelectBackward>)


That outlier at (2,1): I guess it's normal for optimize_Q to be unlike logits near the boundary. But it's maybe a problem. Newton is pulling this in considerably but still an outlier.

Possible quick fix: set EPRCstar_HESSIAN_POINT_FRACTION to something ~0, MAX_NEWTON_STEP to something ~1. Deeper fix: some kind of transformation on residuals? Hmmm... I can think of a good one, using ys and sdprc... a quickie rough precision-weighted average using Poisson approximation...


Hmm. Talked to Mira, and we realized that this is a real problem with optimize_Q, not just with the residuals. So here's my plan:

1. Fix the residuals, using some approximations (poisson for beta-ish likelihood, normal for lognormal prior)
2. Figure out the direct impact of that fix on the estimate of Y
3. Reduce dimension

....

hmmm... Let me try just adding pseudovoters...

logresidual with .5 pseudovoters:
tensor([[ 0.0973, -0.0520, -0.0107],
        [ 0.1032, -0.1231,  0.2536],
        [ 0.0274, -1.2234, -0.0063]], grad_fn=<SelectBackward>)

with .75:
tensor([[ 0.0978, -0.0517, -0.0110],
        [ 0.1044, -0.1283,  0.2676],
        [ 0.0195, -0.8996,  0.0507]], grad_fn=<SelectBackward>)

1:
tensor([[ 0.0983, -0.0513, -0.0113],
        [ 0.1055, -0.1335,  0.2811],
        [ 0.0117, -0.6607,  0.1033]], grad_fn=<SelectBackward>)


Let's go with 1. Time for a run.



...............
OK, I did pseudovoters. Then I did jacobians. Now there are still "spurious" zeros in the raw Hessian. Possibilities:

1. Numeric issues; ie, not the true Hessian.
1a. Non-numeric, but pyro/pytorch bug
2. Bug in my code; ie, the Hessian is correct for what I'm doing, but I'm doing the wrong thing.
3. Bug in my idea; fundamental problem.
4. Somehow not a bug; makes sense for a reason I don't currently understand

Possible responses:
1. Keep debugging.
2. Fake entropy
3. Just write and submit


....................
OK, found/fixed the (embarrassing) bug in cmult. Now, runs several steps, but soon depolytopize fails on negative y values.

Some sanity checks:
1. are goot_inits better than no inits? Here's goot_inits first few lps:

lp:  line 41 1 : tensor(-11888440.4719, grad_fn=<AddBackward0>) tensor(41.9416, grad_fn=<AddBackward0>) 557
lp:  line 41 2 : tensor(-8287659.0503, grad_fn=<AddBackward0>) tensor(39.9708, grad_fn=<AddBackward0>) 557
lp:  line 41 3 : tensor(-7076398.8494, grad_fn=<AddBackward0>) tensor(41.4342, grad_fn=<AddBackward0>) 557
lp:  line 41 4 : tensor(-7814200.0957, grad_fn=<AddBackward0>) tensor(41.7370, grad_fn=<AddBackward0>) 557
lp:  line 41 5 : tensor(-9123709.8657, grad_fn=<AddBackward0>) tensor(44.8238, grad_fn=<AddBackward0>) 557
lp:  line 41 6 : tensor(-7337983.3347, grad_fn=<AddBackward0>) tensor(47.2189, grad_fn=<AddBackward0>) 557
lp:  line 41 7 : tensor(-10552997.7877, grad_fn=<AddBackward0>) tensor(50.7243, grad_fn=<AddBackward0>) 557
lp:  line 41 8 : tensor(-8321066.8970, grad_fn=<AddBackward0>) tensor(48.1704, grad_fn=<AddBackward0>) 557
lp:  line 41 9 : tensor(-7697445.2635, grad_fn=<AddBackward0>) tensor(59.9924, grad_fn=<AddBackward0>) 557
lp:  line 41 10 : tensor(-6405293.0852, grad_fn=<AddBackward0>) tensor(53.3734, grad_fn=<AddBackward0>) 557


no inits:
lp:  line 41 1 : tensor(-11993271.9592, grad_fn=<AddBackward0>) tensor(-0.9074, grad_fn=<AddBackward0>) 557
lp:  line 41 2 : tensor(-8362217.2521, grad_fn=<AddBackward0>) tensor(-0.9446, grad_fn=<AddBackward0>) 557
lp:  line 41 3 : tensor(-7144451.2063, grad_fn=<AddBackward0>) tensor(-1.0523, grad_fn=<AddBackward0>) 557
lp:  line 41 4 : tensor(-7892156.2762, grad_fn=<AddBackward0>) tensor(-0.4799, grad_fn=<AddBackward0>) 557
lp:  line 41 5 : tensor(-9202943.7351, grad_fn=<AddBackward0>) tensor(-0.7450, grad_fn=<AddBackward0>) 557
lp:  line 41 6 : tensor(-7399587.3405, grad_fn=<AddBackward0>) tensor(0.1099, grad_fn=<AddBackward0>) 557
lp:  line 41 7 : tensor(-10652568.5900, grad_fn=<AddBackward0>) tensor(-0.2634, grad_fn=<AddBackward0>) 557
lp:  line 41 8 : tensor(-8387266.9390, grad_fn=<AddBackward0>) tensor(0.0504, grad_fn=<AddBackward0>) 557

goot_inits is better but not by very much. Still bugs out there I guess...? Or maybe the unevenness is just subsampling and actually OK??


base:Yes, I will run. line 5 7 :
Reloading polytopize.
Standard deviations: line 41 1 : tensor(0.4330) tensor(0.9354)
tensor([ 0.5000, -0.2500, -0.2500])
 tensor([[-1.0000,  0.0000,  1.0000],
        [-0.5000,  1.0000, -0.5000],
        [ 1.5000, -1.0000, -0.5000]])
 tensor([[-0.5000, -0.2500,  0.7500],
        [ 0.0000,  0.7500, -0.7500],
        [ 2.0000, -1.2500, -0.7500]])



Yay! a run that worked (but didn't save because of jacobian numeric fail on full data)!


epoch 2310 loss = 9.78E+06, mean_loss=8.41E+06; sds = {'rc': -0.4771363008923635, 'prc': -2.9998692359470174};
 logitstar = tensor([[-0.4636, -0.2819,  0.7455],
        [ 0.0202,  0.7493, -0.7695],

        [ 0.4434, -0.4674,  0.0240]], grad_fn=<CatBackward>)
ps2
 ecstar = tensor([ 0.3429, -0.0690], requires_grad=True)
epoch 2320 loss = 6.96E+06, mean_loss=8.44E+06; sds = {'rc': -0.4790053165415065, 'prc': -3.0003131191623247};
 logitstar = tensor([[-0.4474, -0.2690,  0.7164],
        [ 0.0234,  0.7483, -0.7717],
        [ 0.4240, -0.4793,  0.0553]], grad_fn=<CatBackward>)
Cutoff reached line 41 1 : 8436261.935219077 8431383.930229843
