---
title: "Flexible Ecological Inference using Variational Methods"
author: "Jameson Quinn (work with Mira Bernstein)"
date: "3/13/2019"
output: beamer_presentation

header-includes:
   - \usepackage{tikz}
   - \usetikzlibrary{bayesnet}
   - \usetikzlibrary{arrows}
   - \usetikzlibrary{cd}
   - \usepackage{pgfplots}
   - \usepackage{lastpage}
   - \usepackage{mathrsfs}
   - \usepackage[makeroom]{cancel}
   - \usepackage{wrapfig} 
   - \usetikzlibrary{fit,positioning}
   - \tikzset{font={\fontsize{9pt}{12}\selectfont}}
   - \setbeamertemplate{footline}{\begin{flushright}\thepage/\pageref{LastPage}~~.\\\end{flushright}}
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{xcolor}
   - \definecolor{blue1}{HTML}{4466FF}
   - \definecolor{blue2}{HTML}{0022AA}
   - #\titlegraphic{\includegraphics[width=.5\textwidth,height=.5\textheight]{sallySewing.png}}
   - #
   - \newcommand{\red}[1]{\textcolor{red}{#1}}
   - \newcommand{\look}[1]{\textcolor{purple}{#1}}
   - \newcommand{\blue}[1]{\textcolor{blue}{#1}}
---

```{r, echo=FALSE, error=FALSE, warning=FALSE, results='asis'}
library(knitr)

outputFormat = opts_knit$get("rmarkdown.pandoc.to")
color="red"
if(outputFormat %in% c('latex','beamer')) {
  note = function(x){cat(paste0("\\textcolor{",color,"}{{\\scriptsize ",x,"}}\n\n"))}
} else if(outputFormat %in% c('html','ioslides')) {
  note = function(x){cat(paste0("<font color='",color,"',size='small'>",x,"</font>\n\n"))}
} else {
  cat(outputFormat)
  note = function(x){cat(x)}
}
final=F


note = function(x){cat("")}; final=T #comment out for presenter notes
```

## Big picture overview

## Variational Inference

## Pyro

## Problem with existing (mainstream) VI method

## Introduce Laplace family VI (1)

## Introduce Laplace family VI (2)

## Boosting

## Formal definition of Laplace family (1)

## Toy model results

## Latent variable models (or: why hi-D?)

## Block arrowhead matrices

## Additional methods for LV models: subsampling

## Additional methods for LV models: boosting

## Ch 1 Results

## Ch 2: Ecological inference. Introduction

## EI: example matrices

## Relevance: Thornburg v. Gingles

## History of attempted solutions: ER

## King's EI: 2x2 case (1)

## King's EI: 2x2 case (2)

## Rosen, Jiang, King, & Tanner: RxC case (1)

## Rosen, Jiang, King, & Tanner: RxC case (1)

## Limitations of RJKT...

## ...and our contribution

## Our model

## Modified model

## Polytopize (1)

## Polytopize (2)

## Guide, with amortization

## Testing our EI on simulated data

## EI results (1)

## EI results (2)

## Discussion/future work (Ch. 3)

## Discussion/future work (Ch. 2)

## Thanks




## Directory of extra slides

## Non-meanfield prior work

## Details on toy model

## More on block-arrowhead matrices

## What we expect from subsampling

HARD

## Details of ECHS

## More details on RJKT

## Possible extensions to our EI model

## Boundary issues with polytope; pseudovoters

## How our EI amortization works (1)

## How our EI amortization works (2)

## More EI results






















## END DEFENSE, START OLD PRESENTATION

## Teaser

```{r fig.width=8, fig.height=5,echo=FALSE}
library(png)
library(grid)
img <- readPNG("mira's ei slides.png")
 grid.raster(img)
```

## Thornburg v Gingles, 1986

A majority-minority district must be created if:

1. A minority group is â€œsufficiently numerous and compact to form a majority in a single-member district"; and

2. The minority group is \red{\textbf{"politically cohesive"}}; and

3. The "majority \red{\textbf{votes sufficiently as a bloc}} to enable it ... usually to defeat the minority's preferred candidate."

## Ecological data

\begin{tikzpicture}[
squared notebook/.pic={\clip[postaction={shade,left color=white}](0,0) rectangle (6.5,4);
\draw[ultra thick](0,0) rectangle (6.5,4);}
]
\foreach \x in {2,1.75,1.5,1.25,1,.75,0.5,.25,0}\pic at (\x,\x){squared notebook};
\draw[-latex] (7,0) -- +(1.5,1.5) node[below right,midway,rotate=45] {precinct};
\node[rotate=90] (h) at (.5,2) {Race};
\node (w) at (3.5,3.5) {Candidate};



\node (w) at (3.5,2) {$\begin{array}{l|lll|l}
& R & D & No & \\
\hline
White & \red{?} & \red{?} & \red{?} & 400 \\
Black & \red{?} & \red{?} & \red{?} & 200   \\
Hispanic & \red{?} & \red{?} & \red{?} & 100   \\
Other & \red{?} & \red{?} & \red{?} & 100   \\
\hline
 & 400 & 200 & 200 & 800\end{array}$};

\end{tikzpicture}



## Majority=Majority?

\begin{tikzpicture}[
squared notebook/.pic={\clip[postaction={shade,left color=white}](0,0) rectangle (6.5,4);
\draw[ultra thick](0,0) rectangle (6.5,4);}
]
\foreach \x in {2,1.75,1.5,1.25,1,.75,0.5,.25,0}\pic at (\x,\x){squared notebook};
\draw[-latex] (7,0) -- +(1.5,1.5) node[below right,midway,rotate=45] {precinct};
\node[rotate=90] (h) at (.5,2) {Race};
\node (w) at (3.5,3.5) {Candidate};



\node (w) at (3.5,2) {$\begin{array}{l|lll|l}
& R & D & No & \\
\hline
White & \red{400} & 0 & 0 & 400 \\
Black & 0 & \red{200} & 0 & 200   \\
Hispanic & 0 & 0 & \red{100} & 100   \\
Other & 0 & 0 & \red{100} & 100   \\
\hline
 & 400 & 200 & 200 & 800\end{array}$};

\end{tikzpicture}


## Backwards?

\begin{tikzpicture}[
squared notebook/.pic={\clip[postaction={shade,left color=white}](0,0) rectangle (6.5,4);
\draw[ultra thick](0,0) rectangle (6.5,4);}
]
\foreach \x in {2,1.75,1.5,1.25,1,.75,0.5,.25,0}\pic at (\x,\x){squared notebook};
\draw[-latex] (7,0) -- +(1.5,1.5) node[below right,midway,rotate=45] {precinct};
\node[rotate=90] (h) at (.5,2) {Race};
\node (w) at (3.5,3.5) {Candidate};



\node (w) at (3.5,2) {$\begin{array}{l|lll|l}
& R & D & No & \\
\hline
White & 0 & \red{200} & \red{200} & 400 \\
Black & \red{200} & 0 & 0 & 200   \\
Hispanic & \red{100} & 0 & 0 & 100   \\
Other & \red{100} & 0 & 0 & 100   \\
\hline
 & 400 & 200 & 200 & 800\end{array}$};

\end{tikzpicture}

## Independence?

\begin{tikzpicture}[
squared notebook/.pic={\clip[postaction={shade,left color=white}](0,0) rectangle (6.5,4);
\draw[ultra thick](0,0) rectangle (6.5,4);}
]
\foreach \x in {2,1.75,1.5,1.25,1,.75,0.5,.25,0}\pic at (\x,\x){squared notebook};
\draw[-latex] (7,0) -- +(1.5,1.5) node[below right,midway,rotate=45] {\look{precinct}};
\node[rotate=90] (h) at (.5,2) {Race};
\node (w) at (3.5,3.5) {\look{Candidate}};



\node (w) at (3.5,2) {$\begin{array}{l|lll|l}
& R & D & No & \\
\hline
White & \red{200} & \red{100} & \red{100} & 400 \\
Black & \red{100} & \red{50} & \red{50} & 200   \\
Hispanic & \red{50} & \red{25} & \red{25} & 100   \\
Other & \red{50} & \red{25} & \red{25} & 100   \\
\hline
 & 400 & 200 & 200 & 800\end{array}$};

\end{tikzpicture}

## Structure

* Pose the ecological problem (done)
* Quick review of prior approaches
* A basic, extensible model for EI
* Why and how to reparameterize
* Review of variational inference
* Applying variational inference to EI
* Guide (aka variational distribution) based on observed information

## Ecological regression (for $2\times 2$ cases)


```{r fig.height=5,echo=FALSE}
img <- readPNG("brexit_ER.png")
 grid.raster(img)
```

Brexit voting data. (Example from "The Stats Guy" blog by Adam Jacobs.) Valid under certain (strong) assumptions.

## Ecological regression: uh oh


```{r fig.height=5,echo=FALSE}
img <- readPNG("brexit_ER_oops.png")
 grid.raster(img)
```

Brexit supported by 79% of people without a degree... and -16% of those with one??? Ecological fallacy, Simpson's paradox, etc.

## Infer latents, not parameters

Insight from King, Rosen, Tanner 1999: instead of focusing on population parameters, which are not directly constrained by the data, focus on latent parameters, which are.

Refined by Rosen, King, Jiang, Tanner (2001):

* Fully Bayesian model 

* extends to $R\neq 2 \neq C$

* fast, moment-based estimator

* now widely used.

## Issues with RKJT 2001:

The RKJT model can, in principle, be extended to handle additional factors such as:

* inter-row or inter-column correlations
* covariates
* multiple elections
* exit polling data
* etc. 

However: 

* the moment-based estimator breaks down, 
* MCMC on such a high-dimensional latent space can be challenging.

## Flexible model (1)

\begin{tikzpicture}
  % Define nodes
  \node[latent]           (beta) {$\bm{\beta}_r$};
  \node[const, above=0.4 of beta] (rbighide) {};
  \node[latent, above=of beta] (sdb) {$\sigma_\beta$};
  \node[latent, left=of beta]  (alpha) {$\bm{\alpha}$};
  \node[latent, right=2 of beta]            (gamma) {$\bm{\gamma}_{u,r}$};
  \node[latent, above=of gamma] (sdg) {$\sigma_\gamma$};
  \node[det, below=.7of gamma]            (pi) {$\bm{\pi}_{u,r}$};
  \factor[below=.4 of pi] {multi} {right:$\mathrm{Multi}$} {} {};
  \node[obs, left=of multi]            (n) {$n_{u,r}$};
  \node[latent, below=.5 of multi]            (y) {$\bm{y}_{u,r}$};
  \node[obs, diamond, below=of y]            (v) {$\bm{v}_{u}$};
  \node[const, right=0.6 of y] (ubighide) {};
  \node[const, right=1.2 of y] (rbighide) {};

  % Connect the nodes
  \edge {sdb} {beta} ;
  \edge {sdg} {gamma} ; 
  \edge {alpha,beta,gamma} {pi} ;
  \factoredge {pi,n} {multi} {y} ; 
  \edge {y} {v} ;

  % Plates
  \plate {U} {(gamma)(y)(multi)(pi)(n)(v)(ubighide)} {$U$} ;
  \plate {R} {(beta)(gamma)(y)(rbighide)(U.north east)} {$R$} ;
\end{tikzpicture}

## Flexible model (2)

\begin{tikzpicture}
  % Define nodes
  \node[latent]           (beta) {$\bm{\beta}_r$};
  \node[const, above=0.4 of beta] (rbighide) {};
  \node[latent, above=of beta] (sdb) {$\sigma_\beta$};
  \node[latent, left=of beta]  (alpha) {$\bm{\alpha}$};
  \node[latent, right=2 of beta]            (gamma) {$\bm{\gamma}_{u,r}$};
  \node[latent, above=of gamma] (sdg) {$\sigma_\gamma$};
  \node[det, below=.7of gamma]            (pi) {$\bm{\pi}_{u,r}$};
  \factor[below=.4 of pi] {multi} {right:$\color{red}{\mathrm{CMult}}$} {} {};
  \node[obs, left=of multi]            (n) {$n_{u,r}$};
  \node[latent, below=.5 of multi]            (y) {$\bm{y}_{u,r}$};
  \node[const, right=0.8 of y] (ubighide) {};
  \node[const, right=1.4 of y] (rbighide) {};
  \node[det, below=of y]            (W) {$\color{red}{W_{u}}$};
  \node[obs, diamond, left=.66 of W]            (v) {$\bm{v}_{u}$};

  % Connect the nodes
  \edge {sdb} {beta} ;
  \edge {sdg} {gamma}; 
  \edge {alpha,beta,gamma} {pi} ;
  \factoredge {pi,n} {multi} {y} ; 
  \edge[] {y} {W} ; 
  \edge[] {v} {W} ; 
  \edge[] {y} {v} ; 

  % Plates
  \plate {U} {(gamma)(y)(multi)(pi)(n)(W)(v)(ubighide)} {$U$} ;
  \plate {R} {(beta)(gamma)(y)(rbighide)(U.north east)} {$R$} ;
\end{tikzpicture}

## Flexible model (3)

![](holidaytree_bare.jpg){ width=30% } ![](holidaytree_decorated.png){ width=30% }




$$\vec{y}_{p,r}=y_{p,r,c}\Vert_{c=1}^C\sim \operatorname{CMult}\left(n_{p,r},\frac{exp(\alpha_c+\beta_{r,c}\red{+\lambda_{r,c,p}})\Vert_{c=1}^C}{\sum_{c=1}^Cexp(\alpha_c+\beta_{r,c}\red{+\lambda_{r,c,p}})}\right)$$

$$\alpha_c\sim\mathcal{N}(0,\sigma_\alpha)~~~~~~~\sigma_\alpha\sim\operatorname{Expo}(5)$$

$$\beta_{r,c}\sim\mathcal{N}(0,\sigma_\beta)~~~~~~~\sigma_\beta\sim\operatorname{Expo}(5)$$

$$\red{\lambda_{r,c,p}\sim\mathcal{N}(0,\sigma_\beta)}~~~~~~~\sigma_\lambda\sim\operatorname{Expo}(5)$$
$\lambda$ handles overdispersion. Note: Bayesian Occam's Razor.

## Standard Bayesian approach (simplified)

$\begin{array}{cl}
\operatorname{priors}~\pi & \\
\downarrow & \\
\operatorname{parameters}~\theta & \\
\downarrow & \\
\operatorname{latent~variables}~y & \\
\downarrow & \\
\operatorname{data}~z & 
\end{array}$

## Standard Bayesian approach (cont'd)

$\begin{array}{cl}
\operatorname{priors}~\pi & \\
\downarrow & \operatorname{(parameterizes~distribution)}\\
\operatorname{parameters}~\theta & \operatorname{(unobservable~quantities~of~interest;~low-D)}\\
\downarrow & \operatorname{(parameterizes~distribution)}\\
\operatorname{latent~variables}~y & \operatorname{(unobservable~nuisance~parameters;~high-D)}\\
\downarrow & \operatorname{(parameterizes~distribution)}\\
\operatorname{data}~z & 
\end{array}$

## Ecological Inference

$\begin{array}{cl}
\operatorname{priors}~\pi & \\
\downarrow & \operatorname{(parameterizes~distribution)}\\
\operatorname{parameters}~\theta & \operatorname{(unobservable~\red{nuisance~parameters};~low-D\red{?})}\\
\downarrow & \operatorname{(parameterizes~distribution)}\\
\operatorname{latent~variables}~y & \operatorname{(unobserv\red{ed~quantities~of~interest};~high-D)}\\
\downarrow & \operatorname{\red{(deterministic~function)}}\\
\operatorname{data}~z & 
\end{array}$

A likelihood from a deterministic function is an indicator function! 

## Ecological case

Since the likelihood is just an indicator function, the posterior is just the prior, restricted to the set of values where the likelihood is 1 and renormalized. For each precinct, this set turns out to be a polytope $\mathcal{Y}_{z_p}$ in an $(R-1)(C-1)$ dimensional subspace of the full $\mathbb{R}^{RC}$.

\begin{tikzpicture}[
squared notebook/.pic={\clip[postaction={shade,left color=white}](0,0) rectangle (4,4);
\draw[ultra thick](0,0) rectangle (4,4);}
]
\foreach \x in {.75,0.5,.25,0}\pic at (\x,\x){squared notebook};
\node[rotate=90] (h) at (.2,2) {Race};
\node (w) at (2,3.5) {Candidate};



\node (w) at (2.2,2) {$\begin{array}{lll|l}
R & D & No & \\
\hline
\textcolor{red}{y_{p11}} & \textcolor{blue}{y_{p12}} & y_{p13} & \textcolor{olive}{500} \\
y_{p21} & y_{p22} & y_{p23} & \textcolor{purple}{700}   \\
\hline
\textcolor{red}{400} & \textcolor{blue}{400} & 400 & 1200\end{array}$};











\node[] at (8, -1.1)   (c) {$\mathcal{Y}_{z_p}\subset\mathbb{R}^{(R-1)(C-1)}\longleftrightarrow\mathbb{R}^{RC} $};

%polytope
\draw[style=-] (7,0) -- +(-1,1) node[below left=.25cm,anchor=base,midway,rotate=-45] {$y_{p11}+y_{p12}>\textcolor{purple}{100}$};
\draw[style=-] (7,0) -- +(3,0) node[below=.3cm,anchor=base,midway] {$\textcolor{blue}{y_{p12}}>0$};
\draw[style=-] (10,0) -- +(0,1) node[right = .3cm,anchor=base,midway,rotate=90] {$\textcolor{red}{y_{p11}<400}$};
\draw[style=-] (10,1) -- +(-3,3) node[above right=.15cm,anchor=base,midway,rotate=-45] {$y_{p11}+y_{p12}<\textcolor{olive}{500}$};
\draw[style=-] (7,4) -- +(-1,0) node[above=.2cm,anchor=base,midway] {$\textcolor{blue}{y_{p12}<400}$};
\draw[style=-] (6,1) -- +(0,3) node[left = .2cm,anchor=base,midway,rotate=90] {$\textcolor{red}{y_{p11}}>0$};


\end{tikzpicture}

## Independence point

\begin{tikzpicture}[
squared notebook/.pic={\clip[postaction={shade,left color=white}](0,0) rectangle (4,4);
\draw[ultra thick](0,0) rectangle (4,4);}
]
\foreach \x in {.75,0.5,.25,0}\pic at (\x,\x){squared notebook};
\node[rotate=90] (h) at (.2,2) {Race};
\node (w) at (2,3.5) {Candidate};



\node (w) at (2.2,2) {$\begin{array}{lll|l}
R & D & No & \\
\hline
\red{167} & \red{167} & 167 & 500 \\
233 & 233 & 233 & 700   \\
\hline
400 & 400 & 400 & 1200\end{array}$};



\draw[red,fill=red] (7.667,1.667) circle (.5ex);

\node[] at (7.667,1.267)   (c) {Independence};

%\tkzDefPoint(7.667,1.667){M}
%\tkzLabelPoint[right,below](M){Independence}






\node[] at (8, -1.1)   (c) {$\mathcal{Y}_{z_p}\subset\mathbb{R}^{(R-1)(C-1)}\longleftrightarrow\mathbb{R}^{RC}$};

%polytope
\draw[style=-] (7,0) -- +(-1,1) node[below left=.25cm,anchor=base,midway,rotate=-45] {$y_{p11}+y_{p12}>\textcolor{black}{100}$};
\draw[style=-] (7,0) -- +(3,0) node[below=.3cm,anchor=base,midway] {$\textcolor{black}{y_{p12}}>0$};
\draw[style=-] (10,0) -- +(0,1) node[right = .3cm,anchor=base,midway,rotate=90] {$\textcolor{black}{y_{p11}<400}$};
\draw[style=-] (10,1) -- +(-3,3) node[above right=.15cm,anchor=base,midway,rotate=-45] {$y_{p11}+y_{p12}<\textcolor{black}{500}$};
\draw[style=-] (7,4) -- +(-1,0) node[above=.2cm,anchor=base,midway] {$\textcolor{black}{y_{p12}<400}$};
\draw[style=-] (6,1) -- +(0,3) node[left = .2cm,anchor=base,midway,rotate=90] {$\textcolor{black}{y_{p11}}>0$};


\end{tikzpicture}











## Diffeomorphic function $g(y'):\mathbb{R}^{(R-1)(C-1)}\rightarrow\mathcal{Y}_{z_p}$

\begin{tikzpicture}

%axes
\draw[-latex] (2,2) -- +(2,0) node[below right,midway,rotate=45] {};
\draw[-latex] (2,2) -- +(-2,0) node[below right,midway,rotate=45] {};
\draw[-latex] (2,2) -- +(0,2) node[below right,midway,rotate=45] {};
\draw[-latex] (2,2) -- +(0,-2) node[below right,midway,rotate=45] {};

%vectors in y'
\draw[-latex] (2,2) -- +(.3,.4) node[below right,midway,rotate=45] {};
\draw[-latex] (2.3,2.4) -- +(.3,.4) node[below right,midway,rotate=45] {};
\draw[-latex] (2.6,2.8) -- +(.3,.4) node[below right,midway,rotate=45] {};
\draw[-latex] (2.9,3.2) -- +(.3,.4) node[below right,midway,rotate=45] {};


\draw[-latex] (2,2) -- +(-.4,-.3) node[below right,midway,rotate=45] {};
\draw[-latex] (1.6,1.7) -- +(-.4,-.3) node[below right,midway,rotate=45] {};
\draw[-latex] (1.2,1.4) -- +(-.4,-.3) node[below right,midway,rotate=45] {};
\draw[-latex] (0.8,1.1) -- +(-.4,-.3) node[below right,midway,rotate=45] {};

\draw[-latex] (2,2) -- +(.5,0) node[below right,midway,rotate=45] {};
\draw[-latex] (2.5,2) -- +(.5,0) node[below right,midway,rotate=45] {};
\draw[-latex] (3,2) -- +(.5,0) node[below right,midway,rotate=45] {};
\draw[-latex] (3.5,2) -- +(.5,0) node[below right,midway,rotate=45] {};

%function connector
\node[] at (5, -.7)   (a) {$g(y')$};
\node[] at (5, -1.1)   (b) {$\longrightarrow$};

%labels for spaces
\node[] at (2, -1.1)   (c) {$\mathbb{R}^{(R-1)(C-1)}$};
\node[] at (8, -1.1)   (c) {$\mathcal{Y}_{z_p}\subset\mathbb{R}^{(R-1)(C-1)}\longleftrightarrow\mathbb{R}^{RC}$};


%polytope
\draw[style=-] (7,0) -- +(-1,1) node[below left=.25cm,anchor=base,midway,rotate=-45] {$y_{p11}+y_{p12}>100$};
\draw[style=-] (7,0) -- +(3,0) node[below=.3cm,anchor=base,midway] {$y_{p12}>0$};
\draw[style=-] (10,0) -- +(0,1) node[right = .3cm,anchor=base,midway,rotate=90] {$y_{p11}<400$};
\draw[style=-] (10,1) -- +(-3,3) node[above right=.15cm,anchor=base,midway,rotate=-45] {$y_{p11}+y_{p12}<500$};
\draw[style=-] (7,4) -- +(-1,0) node[above=.2cm,anchor=base,midway] {$y_{p12}<400$};
\draw[style=-] (6,1) -- +(0,3) node[left = .2cm,anchor=base,midway,rotate=90] {$y_{p11}>0$};


%vectors in polytope
\draw[-latex] (7.5,1.5) -- +(.3,.4) node[below right,midway,rotate=45] {};
\draw[-latex] (7.8,1.9) -- +(.2,.266) node[below right,midway,rotate=45] {};
\draw[-latex] (8.0,2.166) -- +(.133,.177) node[below right,midway,rotate=45] {};
\draw[-latex] (8.133,2.344) -- +(.0889,.1185) node[below right,midway,rotate=45] {};



\draw[-latex] (7.5,1.5) -- +(-.4,-.3) node[below right,midway,rotate=45] {};
\draw[-latex] (7.1,1.2) -- +(-.2667,-.2) node[below right,midway,rotate=45] {};
\draw[-latex] (6.833,1.0) -- +(-.1778,-.133) node[below right,midway,rotate=45] {};
\draw[-latex] (6.655,.866) -- +(-.1185,-.0889) node[below right,midway,rotate=45] {};

\draw[-latex] (7.5,1.5) -- +(.5,0) node[below right,midway,rotate=45] {};
\draw[-latex] (8,1.5) -- +(.375,0) node[below right,midway,rotate=45] {};
\draw[-latex] (8.375,1.5) -- +(.281,0) node[below right,midway,rotate=45] {};
\draw[-latex] (8.656,1.5) -- +(.211,0) node[below right,midway,rotate=45] {};

%dots at 0,0 and independence

\draw[red,fill=red] (7.5,1.5) circle (.5ex);
\draw[red,fill=red] (2,2) circle (.5ex);

\end{tikzpicture}

$$\frac{d \Vert g(y')-g(0)\Vert }{d\Vert y'\Vert}=y_{p11}y_{p12}(400-y_{p11})(400-y_{p12})(500-y_{p11}-y_{p12})\cdots$$


## Stochastic variational inference (Hoffman et al., 2013)

Goal: approximate \blue{unnormalized} posterior density $p(\theta,y|z)\propto p(z|\theta,y)p_\pi(\theta,y)$ with sampleable parametric distribution $q_\phi(\theta,y)$. (Called a \textbf{guide} in the pyro SVI package for python)

Maximize negative K-L divergence from guide to \blue{normalized} posterior $p(z|\theta,y)p_\pi(\theta,y)/p(z)$:

$$E_{q_\phi}\left(\log\frac{p(z|\look{\theta},y)p_{\look{\pi}}(\theta,y)}
{q_\phi(\theta,y)p(z)}\right)<0$$

$$E_{q_\phi}\left(\log[p(z|y)p(\theta,y)]-\log
[q_\phi(\theta,y)]-\look{\log(p(z))}\right)<0$$

$$E_{q_\phi}\left(\log[p(z|y)p(\theta,y)]-\log
[q_\phi(\theta,y)]\right)<\log(p(z))$$

LHS is the \textbf{ELBO}; goal is to find $\phi$ which maximizes it.

## ELBO terms

$E_{q_\phi}\left(\log[p(z|y)p(\theta,y)]\right)$ is \textbf{energy} term. Maximized if $q$ is a $\delta$ (dirac mass) at MLE for $(\theta,y|z)$. Unboundedly negative if $q$ has probability mass where $p$ doesn't.

$E_{q_\phi}\left(-\log[q_\phi(\theta,y)]\right)$ is \textbf{entropy} term. Maximized by making q diffuse. For example, if $q$ is $\mathcal{N}(\bm{\mu},\Sigma)$, then this is inversely proportional to $\mathrm{det}(\Sigma)$. In principle unboundedly negative, but in practice, it's easier to control than energy term.

Together, they're maximized if $q_\phi$ "imitates" $p$.

```{r fig.width=5, fig.height=2.5,echo=FALSE}
library(ggplot2)
library(data.table)

combined = data.table()
varidat = data.table(x=(-300:300)/100)
varidat[,series:="1. Log posterior (unnormalized)"]
varidat[,y:=2-log(1+x^2/5)*3+cos(8*atan(x-1.5))/2/(6*(x-1.5)^2+.8)]

combined = rbind(combined,varidat,fill=T)

varidat[,series:="2. Best Gaussian approximation"]
varidat[,y:=-x^2]
combined = rbind(combined,varidat,fill=T)

varidat[,series:="3. Underdispersed"]
varidat[,y:=log(2)-2*x^2]
combined = rbind(combined,varidat,fill=T)

varidat[,series:="4. Overdispersed"]
varidat[,y:=-log(2)-.5*x^2]
combined = rbind(combined,varidat,fill=T)

varidat[,series:="5. Misplaced (local maximum)"]
varidat[,y:=log(16)-16*(x-1.46)^2]
combined = rbind(combined,varidat,fill=T)

filtered = combined[y> -10, ]

qplot(x,y,color=series,data=filtered) + theme_void()
```

## EI case

Reparameterize with $y=g(y')$, and approximate $p(\theta,g(y'))]$ using $q_{\phi,z}(\theta,y')$. ELBO over $y'$ then becomes:

$$E_{q_{\phi,z}}\left(\log[p(\theta,g(y'))\operatorname{det}(J(g(y')))]-\vphantom{\Big|}\log
[q_{\phi,z}(\theta,y')]\right)$$

A common form of variational inference uses a "mean field" guide which factorizes across all parameters and latents; frequently, one that's Normal in each dimension. This ignores the dependence induced by conditioning on the data; which is particularly strong in the case of EI.

## Laplace family

```{r, out.width = "60px", echo=FALSE, results='asis'}
if(T){knitr::include_graphics("stay_puft_2.jpg")}
note("staypuft")
```
*"Choose the form of your posterior"*

Take $q(\theta,y')$ to be a multivariate Normal, and assume that once the ELBO is maximized, its mode $(\hat{\theta},\hat{y})$ coincides with a mode of the posterior. What should its covariance matrix be?

There's an obvious way to approximate a twice-differentiable, unnormalized distribution with a Normal: a Laplace approximation. 

That is, use the observed information matrix:

$$\mathscr{I}(\hat{\bm{y}}',\hat{\theta})=D^2\left(\log[p(\hat{\theta},g(\hat{y}'))\operatorname{det}(J(g(y')))]\right)$$
as the precision matrix of $q$. 

## Graphical posterior


\begin{tikzpicture}

  % Define nodes
  \node[const]           (hats) {$\hat{\bm{\theta}}$};
  \node[const, right=2.5 of hats]            (What) {$\hat{W}_{u}$};
  
  \factor[below=of hats,yshift=-.3cm] {paramdist} {right:$\mathcal{N}$} {} {};
  \node[latent, diamond, aspect=2.5, left=.8 of paramdist]            (Iparams) {$\mathscr{I}^{-1}(\hat{\bm{\theta}})$};
  \node[latent, below=of paramdist]            (params) {$\bm{\theta}$};
  
  
  
  \node[const, below=1.5 of What,xshift=-3cm] (stuff) {};
  
  \factor[below=2.11 of What] {fulldist} {above left:$\mathcal{N}$} {} {};
  \node[latent, diamond, aspect=2.5, right=.8 of fulldist]            (Ifull) {$\mathscr{I}^{-1}(\hat{\bm{\theta}},\hat{W}_{u})$};
  \node[latent, below=of fulldist]            (W) {$W_u$};
  \node[const, right=0.3 of W]            (detour) {};
  \node[det, below=0.6 of W]            (gamma) {$\bm{\gamma}_{u,r}$};
  \node[const, below=0.2 of gamma,xshift=-0.8cm]            (uhide) {};

  % Connect the nodes
  \edge[] {hats} {Iparams} ;
  \edge[-] {hats} {paramdist} ; 
  \edge[-] {Iparams} {paramdist} ; 
  \edge {paramdist} {params} ; 
  \edge {hats,What} {Ifull} ;
  \edge[-] {params} {fulldist}; 
  \edge[-] {Ifull} {fulldist}  ; 
  \edge[-] {What} {fulldist} ; 
  \edge {fulldist} {W} ; 
  \edge[] {W,params} {gamma} ;

  % Plates
  \plate {U} {(gamma)} {$R$} ;
  \plate {U} {(Ifull)(fulldist)(W)(gamma)(uhide)(What)} {$U$} ;
\end{tikzpicture}

## Computability

* Using pyro, a variational inference package for python.
* $\mathscr{I}(\hat{\bm{y}}',\hat{\theta})$ can be calculated using automated differentiation.
* $\mathscr{I}(\hat{\bm{y}}',\hat{\theta})$ is high-dimensional, but due to the structure of the model, sparse (block arrowhead format), so working with it is reasonably efficient. In practice, this means doing sampling "top down" (hyperparameters->parameters->hyperlatents->latents), one precinct at a time at the lower levels.

## Thanks

Thanks to Mira Bernstein, Luke Miratrix, Gary King

## Lower-D posterior (1)

Recall our model:

$\vec{y}_{p,r}=y_{p,r,c}\Vert_{c=1}^C\sim \operatorname{CMult}\left(n_{p,r},\frac{exp(\alpha_c+\beta_{r,c}\red{+\lambda_{r,c,p}})\Vert_{c=1}^C}{\sum_{c=1}^Cexp(\alpha_c+\beta_{r,c}\red{+\lambda_{r,c,p}})}\right)$

$\alpha_c\sim\mathcal{N}(0,\sigma_\alpha)~~~~~~~\sigma_\alpha\sim\operatorname{Expo}(5)$

$\beta_{r,c}\sim\mathcal{N}(0,\sigma_\beta)~~~~~~~\sigma_\beta\sim\operatorname{Expo}(5)$

$\red{\lambda_{r,c,p}\sim\mathcal{N}(0,\sigma_\beta)}~~~~~~~\sigma_\lambda\sim\operatorname{Expo}(5)$

Not only does the dimension of $y$ grow linearly with the number of precincts $P$; because of the latent $\lambda$ parameters, the dimension of $\theta$ does too. This is an issue both in estimating the ELBO and in maximizing it.

## Lower-D posterior (2)

Solution: replace $\lambda_{p,r,c}\Vert_{c=1}^C$ with its MAP value conditional on all the variables connected to it (not conditionally independent): $y_{p,r,c}\Vert_{c=1}^C$, $\alpha_c\Vert_{c=1}^C$, $\beta_{r,c}\Vert_{c=1}^C$, and $\sigma_\lambda$. Because of the form of the model, this is available analytically, and we can trust that the Laplace approximation will still be reasonably good away from the MAP.

This is related to, but somewhat more aggressive than, the idea of "amortized variational inference" developed by Rezende and Mohammed (2015).

