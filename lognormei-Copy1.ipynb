{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro\n",
    "\n",
    "This is a jupyter notebook for testing / coding. So far, each code block is a separate test; unlike an ordinary notebook, they are not meant to run sequentially.\n",
    "\n",
    "First, test to see jupyter is running correctly at all:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi!!nn!\n",
      "C:\\Users\\jameson\\Anaconda3new\\python.exe\n",
      "['C:\\\\Users\\\\jameson\\\\Dropbox\\\\eipython\\\\eipython', 'C:\\\\Users\\\\jameson\\\\Anaconda3new\\\\python37.zip', 'C:\\\\Users\\\\jameson\\\\Anaconda3new\\\\DLLs', 'C:\\\\Users\\\\jameson\\\\Anaconda3new\\\\lib', 'C:\\\\Users\\\\jameson\\\\Anaconda3new', '', 'C:\\\\Users\\\\jameson\\\\Anaconda3new\\\\lib\\\\site-packages', 'C:\\\\Users\\\\jameson\\\\Anaconda3new\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\jameson\\\\Anaconda3new\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\jameson\\\\Anaconda3new\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\jameson\\\\Anaconda3new\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\jameson\\\\.ipython']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hessian'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-132a3d0f462b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyro\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhessian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hessian'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"hi!!nn!\")\n",
    "\n",
    "print(sys.executable)\n",
    "print(sys.path)\n",
    "\n",
    "import pyro\n",
    "import hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, running \"ei\". This started out as a copy of Fritz's code but it's evolved into a working version of ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I will run.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hessian'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a9029f029dcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mei\u001b[0m \u001b[1;31m#import *\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mei\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mei\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcProfile\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\eipython\\eipython\\ei.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmyhessian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrank1torch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimize_Q\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgo_or_nogo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\eipython\\eipython\\myhessian.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m '''\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhessian\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hessian'"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import ei #import *\n",
    "reload(ei)\n",
    "from ei import *\n",
    "import cProfile as profile\n",
    "\n",
    "%prun result = trainGuide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Now, \"toypyro\", a minimal model to narrow in on and reproduce any bugs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss = 9970.42696261406\n",
      "epoch 10 loss = 8222.381203889847\n",
      "epoch 20 loss = 60.06933152675629\n",
      "epoch 30 loss = 1137.6531113386154\n",
      "epoch 40 loss = 1467.7961308956146\n",
      "epoch 50 loss = 503.1370403766632\n",
      "epoch 60 loss = 5752.136534690857\n",
      "epoch 70 loss = 1244.33548527956\n",
      "epoch 80 loss = 8540.950506448746\n",
      "epoch 90 loss = 569.7613765001297\n",
      "epoch 100 loss = 6894.649996995926\n",
      "epoch 110 loss = 4175.3235194683075\n",
      "epoch 120 loss = 472.6241273880005\n",
      "epoch 130 loss = 1902.5091660022736\n",
      "epoch 140 loss = 913.3949070572853\n",
      "epoch 150 loss = 120.05118358135223\n",
      "epoch 160 loss = 24.606423556804657\n",
      "epoch 170 loss = 110.1393538415432\n",
      "epoch 180 loss = 74.07718658447266\n",
      "epoch 190 loss = 5440.085318565369\n",
      "epoch 200 loss = 516.0992064476013\n",
      "epoch 210 loss = 593.9815967679024\n",
      "epoch 220 loss = 344.73766988515854\n",
      "epoch 230 loss = 933.6469010710716\n",
      "epoch 240 loss = 190.28482508659363\n",
      "epoch 250 loss = 852.1514495611191\n",
      "epoch 260 loss = 223.10303181409836\n",
      "epoch 270 loss = 1054.054571211338\n",
      "epoch 280 loss = 67.37258738279343\n",
      "epoch 290 loss = 425.1358232498169\n",
      "epoch 300 loss = 1052.9819769859314\n",
      "epoch 310 loss = 1674.3172267079353\n",
      "epoch 320 loss = 434.36263167858124\n",
      "epoch 330 loss = 371.0104421377182\n",
      "epoch 340 loss = 534.7153202295303\n",
      "epoch 350 loss = 76.14917385578156\n",
      "epoch 360 loss = 1898.6710789203644\n",
      "epoch 370 loss = 190.43798637390137\n",
      "epoch 380 loss = 499.90254777669907\n",
      "epoch 390 loss = 227.2196364402771\n",
      "epoch 400 loss = 549.3110811710358\n",
      "epoch 410 loss = 840.1415543556213\n",
      "epoch 420 loss = 187.75638806819916\n",
      "epoch 430 loss = 8.163513362407684\n",
      "epoch 440 loss = 36.8250629901886\n",
      "epoch 450 loss = 282.2171347141266\n",
      "epoch 460 loss = 88.93707716464996\n",
      "epoch 470 loss = -8.457634627819061\n",
      "epoch 480 loss = 415.9394348859787\n",
      "epoch 490 loss = 27.139925122261047\n",
      "epoch 500 loss = 238.95548915863037\n",
      "epoch 510 loss = 308.2312113046646\n",
      "epoch 520 loss = -13.960336089134216\n",
      "epoch 530 loss = 211.98778748512268\n",
      "epoch 540 loss = 214.59208965301514\n",
      "epoch 550 loss = 434.7178844213486\n",
      "epoch 560 loss = 216.3488790988922\n",
      "epoch 570 loss = 1302.4268074035645\n",
      "epoch 580 loss = 156.7089728116989\n",
      "epoch 590 loss = 215.6293957233429\n",
      "epoch 600 loss = 417.47256302833557\n",
      "epoch 610 loss = 323.62663531303406\n",
      "epoch 620 loss = 898.4031736850739\n",
      "epoch 630 loss = 69.56500446796417\n",
      "epoch 640 loss = -21.20420789718628\n",
      "epoch 650 loss = -14.589676976203918\n",
      "epoch 660 loss = 32.44058358669281\n",
      "epoch 670 loss = 18.092478275299072\n",
      "epoch 680 loss = -8.768603086471558\n",
      "epoch 690 loss = 0.3066471815109253\n",
      "epoch 700 loss = -17.7206848859787\n",
      "epoch 710 loss = 10.318970441818237\n",
      "epoch 720 loss = 27.703640580177307\n",
      "epoch 730 loss = -8.549628853797913\n",
      "epoch 740 loss = 97.89159786701202\n",
      "epoch 750 loss = -23.435295581817627\n",
      "epoch 760 loss = 80.2417802810669\n",
      "epoch 770 loss = 191.0366222858429\n",
      "epoch 780 loss = 1.0039712190628052\n",
      "epoch 790 loss = -14.158635020256042\n",
      "epoch 800 loss = 101.04626989364624\n",
      "epoch 810 loss = -4.6242356300354\n",
      "epoch 820 loss = 67.7615761756897\n",
      "epoch 830 loss = 204.67140352725983\n",
      "epoch 840 loss = 24.472697854042053\n",
      "epoch 850 loss = -9.110898852348328\n",
      "epoch 860 loss = -10.40952980518341\n",
      "epoch 870 loss = 12.897611021995544\n",
      "epoch 880 loss = 81.61308634281158\n",
      "epoch 890 loss = -16.697559475898743\n",
      "epoch 900 loss = 10.683896660804749\n",
      "epoch 910 loss = 58.94023394584656\n",
      "epoch 920 loss = -20.48142147064209\n",
      "epoch 930 loss = 36.798425793647766\n",
      "epoch 940 loss = -4.461576581001282\n",
      "epoch 950 loss = 32.648712038993835\n",
      "epoch 960 loss = -20.80633819103241\n",
      "epoch 970 loss = 13.59885823726654\n",
      "epoch 980 loss = 27.542380213737488\n",
      "epoch 990 loss = -20.179308533668518\n",
      "epoch 1000 loss = -19.005021929740906\n",
      "epoch 1010 loss = -18.2170547246933\n",
      "epoch 1020 loss = -19.43075668811798\n",
      "epoch 1030 loss = -4.924235939979553\n",
      "epoch 1040 loss = -21.92120921611786\n",
      "epoch 1050 loss = 16.83372700214386\n",
      "epoch 1060 loss = -11.871260285377502\n",
      "epoch 1070 loss = -10.325113654136658\n",
      "epoch 1080 loss = -18.82685911655426\n",
      "epoch 1090 loss = 10.077147364616394\n",
      "epoch 1100 loss = -27.32503378391266\n",
      "epoch 1110 loss = -5.696203827857971\n",
      "epoch 1120 loss = -13.577538132667542\n",
      "epoch 1130 loss = -19.523112654685974\n",
      "epoch 1140 loss = -12.977255702018738\n",
      "epoch 1150 loss = -23.81832444667816\n",
      "epoch 1160 loss = -16.85115420818329\n",
      "epoch 1170 loss = -6.799846529960632\n",
      "epoch 1180 loss = -20.49998939037323\n",
      "epoch 1190 loss = -15.655739068984985\n",
      "epoch 1200 loss = -9.904370427131653\n",
      "epoch 1210 loss = -29.566373586654663\n",
      "epoch 1220 loss = 5.4111088514328\n",
      "epoch 1230 loss = -28.05242884159088\n",
      "epoch 1240 loss = -21.681487321853638\n",
      "epoch 1250 loss = -13.319893598556519\n",
      "epoch 1260 loss = -12.41371762752533\n",
      "epoch 1270 loss = -21.702600359916687\n",
      "epoch 1280 loss = -22.059823989868164\n",
      "epoch 1290 loss = -25.3575279712677\n",
      "epoch 1300 loss = -26.640719413757324\n",
      "epoch 1310 loss = -24.549542903900146\n",
      "epoch 1320 loss = -19.065369606018066\n",
      "epoch 1330 loss = -19.86486268043518\n",
      "epoch 1340 loss = -20.076615929603577\n",
      "epoch 1350 loss = -26.849024534225464\n",
      "epoch 1360 loss = -22.73797631263733\n",
      "epoch 1370 loss = -19.59937858581543\n",
      "epoch 1380 loss = -28.688270330429077\n",
      "epoch 1390 loss = -25.230886578559875\n",
      "epoch 1400 loss = -19.165393352508545\n",
      "epoch 1410 loss = -21.86486506462097\n",
      "epoch 1420 loss = -17.583155870437622\n",
      "epoch 1430 loss = -18.787638187408447\n",
      "epoch 1440 loss = -21.011450052261353\n",
      "epoch 1450 loss = -22.13494110107422\n",
      "epoch 1460 loss = -13.651560068130493\n",
      "epoch 1470 loss = -21.351763010025024\n",
      "epoch 1480 loss = -20.056892037391663\n",
      "epoch 1490 loss = -15.752094388008118\n",
      "epoch 1500 loss = -26.150632858276367\n",
      "epoch 1510 loss = -30.08116364479065\n",
      "epoch 1520 loss = -13.490179896354675\n",
      "epoch 1530 loss = -11.820260643959045\n",
      "epoch 1540 loss = -27.708999156951904\n",
      "epoch 1550 loss = -21.760513305664062\n",
      "epoch 1560 loss = -25.024082899093628\n",
      "epoch 1570 loss = -30.239800810813904\n",
      "epoch 1580 loss = -24.265573501586914\n",
      "epoch 1590 loss = -12.849509716033936\n",
      "epoch 1600 loss = -21.990947246551514\n",
      "epoch 1610 loss = -25.473822593688965\n",
      "epoch 1620 loss = -21.74608302116394\n",
      "epoch 1630 loss = -29.68125820159912\n",
      "epoch 1640 loss = -16.449406623840332\n",
      "epoch 1650 loss = -22.486225843429565\n",
      "epoch 1660 loss = -21.300263166427612\n",
      "epoch 1670 loss = -23.194011330604553\n",
      "epoch 1680 loss = -20.318111419677734\n",
      "epoch 1690 loss = -25.038808345794678\n",
      "epoch 1700 loss = -23.52825403213501\n",
      "epoch 1710 loss = -11.93352997303009\n",
      "epoch 1720 loss = -20.955570459365845\n",
      "epoch 1730 loss = -24.713982343673706\n",
      "epoch 1740 loss = -22.206504344940186\n",
      "epoch 1750 loss = -18.968297481536865\n",
      "epoch 1760 loss = -27.074164748191833\n",
      "epoch 1770 loss = -21.310234308242798\n",
      "epoch 1780 loss = -21.19845724105835\n",
      "epoch 1790 loss = -24.367520809173584\n",
      "epoch 1800 loss = -26.015806794166565\n",
      "epoch 1810 loss = -23.054303884506226\n",
      "epoch 1820 loss = -25.64688992500305\n",
      "epoch 1830 loss = -13.319884181022644\n",
      "epoch 1840 loss = -17.32163166999817\n",
      "epoch 1850 loss = -23.7455712556839\n",
      "epoch 1860 loss = -25.545387268066406\n",
      "epoch 1870 loss = -27.497881412506104\n",
      "epoch 1880 loss = -24.89391028881073\n",
      "epoch 1890 loss = -24.854057788848877\n",
      "epoch 1900 loss = -23.371911883354187\n",
      "epoch 1910 loss = -26.387003779411316\n",
      "epoch 1920 loss = -22.207335233688354\n",
      "epoch 1930 loss = -22.02166199684143\n",
      "epoch 1940 loss = -23.83713984489441\n",
      "epoch 1950 loss = -25.31385374069214\n",
      "epoch 1960 loss = -16.196258068084717\n",
      "epoch 1970 loss = -22.292989253997803\n",
      "epoch 1980 loss = -23.85995388031006\n",
      "epoch 1990 loss = -28.101629972457886\n",
      "epoch 2000 loss = -22.794631123542786\n",
      "epoch 2010 loss = -22.766714811325073\n",
      "epoch 2020 loss = -25.436706066131592\n",
      "epoch 2030 loss = -25.03978204727173\n",
      "epoch 2040 loss = -26.8535076379776\n",
      "epoch 2050 loss = -22.176397800445557\n",
      "epoch 2060 loss = -16.28916573524475\n",
      "epoch 2070 loss = -19.07245898246765\n",
      "epoch 2080 loss = -22.417195081710815\n",
      "epoch 2090 loss = -29.459009885787964\n",
      "epoch 2100 loss = -15.73995041847229\n",
      "epoch 2110 loss = -21.550569772720337\n",
      "epoch 2120 loss = -19.20262575149536\n",
      "epoch 2130 loss = -25.475621461868286\n",
      "epoch 2140 loss = -25.69290542602539\n",
      "epoch 2150 loss = -26.19874334335327\n",
      "epoch 2160 loss = -25.563206911087036\n",
      "epoch 2170 loss = -27.548903465270996\n",
      "epoch 2180 loss = -22.519362926483154\n",
      "epoch 2190 loss = -20.045663595199585\n",
      "epoch 2200 loss = -22.438573360443115\n",
      "epoch 2210 loss = -25.288503170013428\n",
      "epoch 2220 loss = -21.962993383407593\n",
      "epoch 2230 loss = -16.933046579360962\n",
      "epoch 2240 loss = -19.901620388031006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2250 loss = -15.773172616958618\n",
      "epoch 2260 loss = -16.03392219543457\n",
      "epoch 2270 loss = -27.897440671920776\n",
      "epoch 2280 loss = -20.802294731140137\n",
      "epoch 2290 loss = -24.83900547027588\n",
      "epoch 2300 loss = -30.2585289478302\n",
      "epoch 2310 loss = -20.315150022506714\n",
      "epoch 2320 loss = -22.15317392349243\n",
      "epoch 2330 loss = -25.159435987472534\n",
      "epoch 2340 loss = -20.672945022583008\n",
      "epoch 2350 loss = -23.544315814971924\n",
      "epoch 2360 loss = -21.54719305038452\n",
      "epoch 2370 loss = -22.543355226516724\n",
      "epoch 2380 loss = -23.42118239402771\n",
      "epoch 2390 loss = -21.949904322624207\n",
      "epoch 2400 loss = -31.188451051712036\n",
      "epoch 2410 loss = -18.388630628585815\n",
      "epoch 2420 loss = -18.03581702709198\n",
      "epoch 2430 loss = -27.299959778785706\n",
      "epoch 2440 loss = -27.348795413970947\n",
      "epoch 2450 loss = -21.49933934211731\n",
      "epoch 2460 loss = -24.72176694869995\n",
      "epoch 2470 loss = -20.447677612304688\n",
      "epoch 2480 loss = -27.689104080200195\n",
      "epoch 2490 loss = -24.22283685207367\n",
      "epoch 2500 loss = -22.203494548797607\n",
      "epoch 2510 loss = -19.911522388458252\n",
      "epoch 2520 loss = -22.637953996658325\n",
      "epoch 2530 loss = -18.87294626235962\n",
      "epoch 2540 loss = -31.51684594154358\n",
      "epoch 2550 loss = -21.241424798965454\n",
      "epoch 2560 loss = -18.018275499343872\n",
      "epoch 2570 loss = -20.61366891860962\n",
      "epoch 2580 loss = -26.1727454662323\n",
      "epoch 2590 loss = -20.367342233657837\n",
      "epoch 2600 loss = -21.66036605834961\n",
      "epoch 2610 loss = -20.375153064727783\n",
      "epoch 2620 loss = -28.787466764450073\n",
      "epoch 2630 loss = -26.659034252166748\n",
      "epoch 2640 loss = -27.07300877571106\n",
      "epoch 2650 loss = -20.351962089538574\n",
      "epoch 2660 loss = -21.9063138961792\n",
      "epoch 2670 loss = -21.56492280960083\n",
      "epoch 2680 loss = -27.147707223892212\n",
      "epoch 2690 loss = -23.924439072608948\n",
      "epoch 2700 loss = -21.84794807434082\n",
      "epoch 2710 loss = -25.21865200996399\n",
      "epoch 2720 loss = -23.206295013427734\n",
      "epoch 2730 loss = -31.85695791244507\n",
      "epoch 2740 loss = -21.257530689239502\n",
      "epoch 2750 loss = -16.897451639175415\n",
      "epoch 2760 loss = -17.47722029685974\n",
      "epoch 2770 loss = -20.478862285614014\n",
      "epoch 2780 loss = -27.533504486083984\n",
      "epoch 2790 loss = -23.544565200805664\n",
      "epoch 2800 loss = -17.046606063842773\n",
      "epoch 2810 loss = -25.670005321502686\n",
      "epoch 2820 loss = -25.69626784324646\n",
      "epoch 2830 loss = -27.408512592315674\n",
      "epoch 2840 loss = -24.09447407722473\n",
      "epoch 2850 loss = -15.136472463607788\n",
      "epoch 2860 loss = -18.049262523651123\n",
      "epoch 2870 loss = -19.774412989616394\n",
      "epoch 2880 loss = -20.50040626525879\n",
      "epoch 2890 loss = -25.857336282730103\n",
      "epoch 2900 loss = -18.43021249771118\n",
      "epoch 2910 loss = -21.843361854553223\n",
      "epoch 2920 loss = -21.852466821670532\n",
      "epoch 2930 loss = -25.457306385040283\n",
      "epoch 2940 loss = -35.21738600730896\n",
      "epoch 2950 loss = -17.756343126296997\n",
      "epoch 2960 loss = -17.89638376235962\n",
      "epoch 2970 loss = -22.20802640914917\n",
      "epoch 2980 loss = -17.484975576400757\n",
      "epoch 2990 loss = -20.95511245727539\n",
      "epoch 3000 loss = -15.643475770950317\n",
      "logScaleHat:\n",
      "-3.4726879596710205\n",
      "logTailHat:\n",
      "0.29934072494506836\n",
      "meanHat:\n",
      "-0.034671951085329056\n",
      "meanSig:\n",
      "0.015605974942445755\n",
      "truthHat:\n",
      "tensor([-0.0286, -0.0322, -0.0375, -0.0357, -0.0525, -0.0504, -0.0500, -0.0541,\n",
      "        -0.0367, -0.0450, -0.0574, -0.0379, -0.0411, -0.0153, -0.0267, -0.0459,\n",
      "        -0.0555, -0.0385, -0.0429, -0.0377, -0.0214, -0.0371, -0.0546, -0.0285,\n",
      "        -0.0435, -0.0514, -0.0516, -0.0364, -0.0439, -0.0525, -0.0455, -0.0388,\n",
      "        -0.0331, -0.0612, -0.0482, -0.0386, -0.0361, -0.0341, -0.0338, -0.0455,\n",
      "        -0.0555, -0.0040, -0.0210, -0.0291], requires_grad=True)\n",
      "truthRho:\n",
      "0.008846916258335114\n",
      "truthSEmult:\n",
      "0.28036901354789734\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmUXOV95vHvU71ICASSTKORJWHJWGMDCRZYBhwyPsQ4QpBFeILHODHoYObIx4Fz8EwWQxIHAibG47EdO8bYJGgQDkEQbA5KLFuIzYTEIAkQixCgRoBpJLSgfeul6jd/3LebQqpedaurW/18zqlTt3713lvv21WqR/fet6oUEZiZmeWhUOsOmJnZ4cOhYmZmuXGomJlZbhwqZmaWG4eKmZnlxqFiZma5caiYmVluHCpmZpYbh4qZmeWmvtYdGGzHHntsTJs2rdbdMDMbVp588sktEdHUW7sRFyrTpk1j5cqVte6GmdmwIun1vrTz4S8zM8uNQ8XMzHLjUDEzs9w4VMzMLDdVCxVJoyUtl/SMpNWS/ibVp0t6QtJaSXdJakz1Uel2c7p/Wtm2rk71lySdW1afk2rNkq6q1ljMzKxvqrmn0gp8IiI+DMwE5kg6E/g68O2ImAFsAy5L7S8DtkXEB4Bvp3ZIOgm4CDgZmAN8X1KdpDrgJuA84CTgs6mtmZnVSNVCJTK7082GdAngE8A9qb4QuCAtz023SfefI0mpvigiWiPiVaAZOD1dmiNiXUS0AYtSWzMzq5GqnlNJexSrgE3AMuAVYHtEdKQmLcDktDwZeAMg3b8DeE95/YB1uqtXxfrt+3joxY3V2ryZ2WGhqqESEcWImAlMIduzOLFSs3Stbu7rb/0gkuZLWilp5ebNm3vveAW//73H+Pxt/tCkmVlPBmX2V0RsBx4BzgTGSer8JP8UYH1abgGmAqT7jwG2ltcPWKe7eqXHvyUiZkXErKamXr9loKItu9sGtJ6Z2UhSzdlfTZLGpeUjgE8Ca4CHgQtTs3nAfWl5cbpNuv+hiIhUvyjNDpsOzACWAyuAGWk2WSPZyfzF1RqPmZn1rprf/TUJWJhmaRWAuyPi3yS9ACyS9FXgaeDW1P5W4EeSmsn2UC4CiIjVku4GXgA6gMsjoggg6QpgKVAHLIiI1VUcj5mZ9aJqoRIRzwKnVqivIzu/cmB9P/DpbrZ1A3BDhfoSYMkhd9bMzHLhT9SbmVluHCpmZpYbh4qZmeXGoWJmZrlxqJiZWW4cKmZmlhuHipmZ5cahYmZmuXGomJlZbhwqZmaWG4eKmZnlxqFiZma5caiYmVluHCpmZpYbh4qZmeXGoWJmZrlxqJiZWW4cKmZmlhuHipmZ5cahYmZmuXGomJlZbhwqZmaWG4dKP0VErbtgZjZkOVTMzCw3VQsVSVMlPSxpjaTVkq5M9WslvSlpVbqcX7bO1ZKaJb0k6dyy+pxUa5Z0VVl9uqQnJK2VdJekxmqNx8zMelfNPZUO4E8i4kTgTOBySSel+74dETPTZQlAuu8i4GRgDvB9SXWS6oCbgPOAk4DPlm3n62lbM4BtwGVVHA8APvplZta9qoVKRGyIiKfS8i5gDTC5h1XmAosiojUiXgWagdPTpTki1kVEG7AImCtJwCeAe9L6C4ELqjMaMzPri0E5pyJpGnAq8EQqXSHpWUkLJI1PtcnAG2WrtaRad/X3ANsjouOAupmZ1UjVQ0XSUcCPgS9FxE7gZuAEYCawAfhmZ9MKq8cA6pX6MF/SSkkrN2/e3M8R9OEBzMwMqHKoSGogC5Q7IuInABGxMSKKEVEC/oHs8BZkexpTy1afAqzvob4FGCep/oD6QSLiloiYFRGzmpqa8hmcmZkdpJqzvwTcCqyJiG+V1SeVNfsU8HxaXgxcJGmUpOnADGA5sAKYkWZ6NZKdzF8c2QdGHgYuTOvPA+6r1njMzKx39b03GbCzgIuB5yStSrW/IJu9NZPsSNJrwBcAImK1pLuBF8hmjl0eEUUASVcAS4E6YEFErE7b+zKwSNJXgafJQszMzGqkaqESEY9R+bzHkh7WuQG4oUJ9SaX1ImId7xw+GxTZDlKlYZmZmT9Rb2ZmuXGomJlZbhwq/eQpxWZm3XOomJlZbhwqZmaWG4dKP/kLJc3MuudQMTOz3DhUzMwsNw4VMzPLjUOln8KTis3MuuVQMTOz3DhUzMwsNw6VfvKUYjOz7jlUzMwsNw4VMzPLjUPFzMxy41AxM7PcOFTMzCw3DhUzM8uNQ6WfPKXYzKx7DhUzM8uNQ8XMzHLjUOknf6GkmVn3HCpmZpabqoWKpKmSHpa0RtJqSVem+gRJyyStTdfjU12SviupWdKzkk4r29a81H6tpHll9Y9Iei6t811JqtZ4zMysd9XcU+kA/iQiTgTOBC6XdBJwFfBgRMwAHky3Ac4DZqTLfOBmyEIIuAY4AzgduKYziFKb+WXrzanieMzMrBdVC5WI2BART6XlXcAaYDIwF1iYmi0ELkjLc4HbI/M4ME7SJOBcYFlEbI2IbcAyYE667+iI+GVEBHB72baqxlOKzcy6NyjnVCRNA04FngAmRsQGyIIHOC41mwy8UbZaS6r1VG+pUDczsxqpeqhIOgr4MfCliNjZU9MKtRhAvVIf5ktaKWnl5s2be+uymZkNUFVDRVIDWaDcERE/SeWN6dAV6XpTqrcAU8tWnwKs76U+pUL9IBFxS0TMiohZTU1NhzQmH/0yM+teNWd/CbgVWBMR3yq7azHQOYNrHnBfWf2SNAvsTGBHOjy2FJgtaXw6QT8bWJru2yXpzPRYl5Rty8zMaqC+its+C7gYeE7SqlT7C+BG4G5JlwG/Aj6d7lsCnA80A3uBSwEiYquk64EVqd11EbE1LX8RuA04AvhZupiZWY1ULVQi4jEqn/cAOKdC+wAu72ZbC4AFFeorgV87hG72W3j6l5lZt/yJejMzy41DxczMcuNQMTOz3DhU+slnVMzMuudQMTOz3DhUzMwsNw6VfvKMYjOz7jlUzMwsNw4VMzPLjUOlv3z4y8ysWw4VMzPLjUPFzMxy41AxM7PcOFT6KXxSxcysWw4VMzPLjUPFzMxy41DpJ3+i3sysew4VMzPLjUPFzMxy41AxM7PcOFT6yadUzMy616dQkXSlpKOVuVXSU5JmV7tzZmY2vPR1T+XzEbETmA00AZcCN1atV2ZmNiz1NVSUrs8H/l9EPFNWG1HCc4rNzLrV11B5UtL9ZKGyVNJYoNTTCpIWSNok6fmy2rWS3pS0Kl3OL7vvaknNkl6SdG5ZfU6qNUu6qqw+XdITktZKuktSY18HbWZm1dHXULkMuAr4aETsBRrIDoH15DZgToX6tyNiZrosAZB0EnARcHJa5/uS6iTVATcB5wEnAZ9NbQG+nrY1A9iW+mhmZjXU11D5GPBSRGyX9Dngr4AdPa0QEY8CW/u4/bnAoohojYhXgWbg9HRpjoh1EdEGLALmShLwCeCetP5C4II+PtYh8cEvM7Pu9TVUbgb2Svow8OfA68DtA3zMKyQ9mw6PjU+1ycAbZW1aUq27+nuA7RHRcUDdzMxqqK+h0hHZGeq5wHci4jvA2AE83s3ACcBMYAPwzVSvdNI/BlCvSNJ8SSslrdy8eXP/emxmZn3W11DZJelq4GLgp+lcR0N/HywiNkZEMSJKwD+QHd6CbE9jalnTKcD6HupbgHGS6g+od/e4t0TErIiY1dTU1N9um5lZH/U1VD4DtJJ9XuUtskNN3+jvg0maVHbzU0DnzLDFwEWSRkmaDswAlgMrgBlpplcj2cn8xWmv6WHgwrT+POC+/vZnIDyj2Myse/W9N4GIeEvSHcBHJf0usDwiejynIulO4GzgWEktwDXA2ZJmkh2qeg34Qtr+akl3Ay8AHcDlEVFM27kCWArUAQsiYnV6iC8DiyR9FXgauLXPozYzs6roU6hI+h9keyaPkJ3P+HtJfxYR93S3TkR8tkK52zf+iLgBuKFCfQmwpEJ9He8cPjMzsyGgT6EC/CXZZ1Q2AUhqAh7gnSm9I4Z/o97MrHt9PadS6AyU5O1+rGtmZiNEX/dUfi5pKXBnuv0ZKhySMjOzka2vJ+r/TNIfAGeRnVO5JSLurWrPhiof/TIz61Zf91SIiB8DP65iX4aF9pJTxcysOz2eF5G0S9LOCpddknYOVieHkrNufKjWXTAzG7J63FOJiIF8FYuZmY1QnsFlZma5caiYmVluHCpmZpYbh4qZmeXGoXKI7l7xBtOu+in724u17oqZWc05VA7R3z3wMgBbdrfWuCdmZrXnUDlEhUL2I5T+nRUzM4fKIWvZtg+AklPFzMyhkpfn3tzBiV/5OZt27q91V8zMasahkpMFj73KvvYij7y8udZdMTOrGYdKTrq+Z9JHwcxsBHOoHILyacTOEjMzh8qAXP2T53hl824+9JWfd9XigBP1rR3+3IqZjTwOlQG4c/mveHHDrnfVOmd/BcErm3fzwb/6Ofc+3VKL7pmZ1YxDJSflOyqdgXP/6o016o2ZWW04VHJS/oOQUu36YWZWSw6VATowODpv+jOQZjaSVS1UJC2QtEnS82W1CZKWSVqbrsenuiR9V1KzpGclnVa2zrzUfq2keWX1j0h6Lq3zXWlw9w/68mAOGDMbaaq5p3IbMOeA2lXAgxExA3gw3QY4D5iRLvOBmyELIeAa4AzgdOCaziBKbeaXrXfgY5mZ2SCrWqhExKPA1gPKc4GFaXkhcEFZ/fbIPA6MkzQJOBdYFhFbI2IbsAyYk+47OiJ+Gdlc3tvLtlUTnftJ3jkxs5FssM+pTIyIDQDp+rhUnwy8UdauJdV6qrdUqA8JPk9vZiPVUDlRX+l9OAZQr7xxab6klZJWbt48eN/NFd5vMbMRZrBDZWM6dEW63pTqLcDUsnZTgPW91KdUqFcUEbdExKyImNXU1HTIg6ik8/DXvz6z3lOKzWzEGuxQWQx0zuCaB9xXVr8kzQI7E9iRDo8tBWZLGp9O0M8Glqb7dkk6M836uqRsW4Pi4CnFWeE/X3n7XZ9ZMTMbSeqrtWFJdwJnA8dKaiGbxXUjcLeky4BfAZ9OzZcA5wPNwF7gUoCI2CrpemBFanddRHSe/P8i2QyzI4CfpcuQ4M+smNlIVbVQiYjPdnPXORXaBnB5N9tZACyoUF8J/Nqh9NHMzPI1VE7UH1a+eMdTte6CmVlNOFRyUunkvI9+mdlI41AZME/xMjM7kEMlJ6+/vbfWXTAzqzmHygC1bHt3iOzY116jnpiZDR0OlQH66k/X9NrGU4rNbKRxqFSVU8XMRhaHipmZ5cahYmZmuXGoDBFf+9kaPnrDA7XuhpnZIana17RY//zwF+tq3QUzs0PmPZUq8uwvMxtpHCpmZpYbh0oVeUfFzEYah4qZmeXGoWJmZrlxqFRRVDhTv3T1W/zo8ddr0Bszs+rzlOJB9oUfPQnAxWe+r8Y9MTPLn/dUzMwsNw4VMzPLjUOlijyl2MxGGoeKmZnlxqFiZma5cahU0ZoNO2vdBTOzQVWTUJH0mqTnJK2StDLVJkhaJmltuh6f6pL0XUnNkp6VdFrZdual9mslzavFWHqycWcrO/dX/u36s258aJB7Y2ZWfbXcU/mtiJgZEbPS7auAByNiBvBgug1wHjAjXeYDN0MWQsA1wBnA6cA1nUE0lLS2l7qWL7jpP7qW39y+rxbdMTOrqqH04ce5wNlpeSHwCPDlVL89so+nPy5pnKRJqe2yiNgKIGkZMAe4c3C73bOC4NrFq5l0zGhWvbG91t0xM6uqWoVKAPdLCuCHEXELMDEiNgBExAZJx6W2k4E3ytZtSbXu6kOKJG77z9dq3Q0zs0FRq8NfZ0XEaWSHti6X9PEe2qpCLXqoH7wBab6klZJWbt68uf+9rZL/ffcqTrt+Wa27YWaWm5qESkSsT9ebgHvJzolsTIe1SNebUvMWYGrZ6lOA9T3UKz3eLRExKyJmNTU15TmUXlVKvk4/eepNtu5pY8MOn18xs8PDoIeKpCMlje1cBmYDzwOLgc4ZXPOA+9LyYuCSNAvsTGBHOky2FJgtaXw6QT871YYU9ZQqyfJXt1a/I2Zmg6AW51QmAvcqe7etB/45In4uaQVwt6TLgF8Bn07tlwDnA83AXuBSgIjYKul6YEVqd13nSfuhRD3uq5iZHV4GPVQiYh3w4Qr1t4FzKtQDuLybbS0AFuTdxzxFH74BrMLPrpiZDUv+RH2V9SUw+hI8ZmbDgUNlCDjUPZV/X7uZ/7v0pXw6Y2Z2CBwqVTYY+yAX37qc7z3cPAiPZGbWM4dKlVX6nfqD2wxCR8zMBoFDZQgoOVXM7DDhUKmy1o5S743MzA4TDpUq6yj24fDXIPTDzGwwOFSq7OPfeLj3Rk4VMztMOFSGgAM/p7JtTxt7WjvYtHM/benw2b62Iru6+cEvM7OhYij9nsqItWV327tun3r9MkY3FNjfXuKUKcfwbMuOrvteu/F3Brt7ZmZ95lAZAr5R9sHFHfuyvZH96RcjywPlQDf+7EV+8ItXqts5M7N+8OGvIaa1vdjj/XevyH6X7OWNuxwoZjbkOFSGmT//8bMAzP72oxXv3763jdaOnoPJzKxaHCqHmZnXLeOSW5d33X7prV1s3Lm/hj0ys5HE51SGmDw+LPlE2Y9+nft3j1JXEK/87fmHvF0zs954T2WIufS2Fb036qdi6d1Tlu9b9SYPvbgx98cxM3OoDDHNm3YPeN0tu1v71O7KRav4/G0rAXj05c1c/28vDPgxzczKOVQOI//9+//Z73UuWbCcWx97tcc29z7d4vMyZtYnDpXDyK+27u3x/uZNu/lhN9OQN+zYx57WDv7x39fRvGlXV31Pawf/665n+KN/fCLXvprZ4ckn6g9T33lgLUtXv9V1e+nqt7hy0dNdH6oE+NqSNV3LH/vaQ13Lf/fAWr7+B6ewYcc+PvPRqQC0bOs5sMzMwKFy2Pr2Ay+/6/YXfvTkQW1++Oi6iuvubu3g8n9+CoBPfyQLlb5827KZmQ9/DUOPrd0yaI9VTD8g1lFyqJhZ7xwqw9Dnbh288xsdpb59bua5lh3c+3RLlXtjZkOdD39Zj65dvLrXNhHB733vMQA+deqUanfJzIawYR8qkuYA3wHqgH+MiBtr3KXDypLn3jnZ/8tX3uZHj7/WVfv9D7+Xxc+s54/OOL6rzf72Ih/6ys/56989ic//5vSu+mtb9lBfJ6aMH3PQY7y5fR/72op84LijqjgSMxsMihi+x8ol1QEvA78NtAArgM9GRLef5ps1a1asXLmy34817aqfDrSbI8ofn30C338km7b82o2/w6ad++koBb9xYza7bOyoej73sffxwYlj+cBxR/Frk4/p+tu+eP0cRjfU1azvZtY9SU9GxKxe2w3zUPkYcG1EnJtuXw0QEV/rbh2HytByREMd+w74uv9PnjiRB9a88zUy094zhuPGjmbCkY2c8f4J/M2/vsB1c09mzYZdnHvyRG597FVOPX48R4+u5+wPHscnv/ULAB79s9/ihiUvMO9j05h5/Dje2rGfIxrrGFVfxxtb9zJ1whga6wsUi8He9g72tRV56MVNnP/rk5hwZCOjG7J2oxvqaBo7CoD2YomCxI597Uw4spGIYPvedsaNaaC9GJQiGFVfQBIApVLQXioxqv7gsCylyQ+FQtY2IrrW605f2phVw0gJlQuBORHxP9Pti4EzIuKK7tYZaKi8umUPxVIw//aVTJkwhrGj6/npsxsG3PdKPn/WdBY/8+ZBvwRptdFQJ9rTVOojG+sIYG9b7z8rcNSo+q51WzuKtBeD0Q0FGusKFApCZNO2O7ctwZGN9bQVSxw9uoFSBPvbiwgoSBQKoiAIYPvedsaPaaAjBVhdQV2h1JO+xFBfw6q3VsUIGusKlCILwVIEpYBSeq+pL4jOyYSd42qoK1AqBQ11BTpKJYqloLG+QHsxyIanrm0F0N5Ror6uQH1BdJQCCeokpHf+Vp1KpWy9xvoCxVIWypH6U0z3RWR9qSu8sw0QbR3Frn7sbeugsa7AEY11QNamFMG+tiKFgmisK1CMoCBl2y5m/Qro+o9GR7GUxq30/BWArB+R/jadf2EpW8qulSbN6ID2BUoRtBdL1BdEQWLX/g6OGdPQ9VilyLaxp7WDF66bQ0PdwOZn9TVUhvs5lUqv74NSUtJ8YD7A8ccff9AKfTH92CMBeOhPz+6q3fSH2XVrRxEhWjuKbN/bTnuxxPgxjexrLzJ2dD2j6utoK5bYua+dvW1F9rZ1cMqUcWzf28aW3W2MG9PA62/v5SPvG89f/95JfO+htXz8vzYx6ZgjuOGnL/AbJxzL/S9s5IzpE3jy9W0c0VjHxKNHs2NfG586dQpb97TyH81v88t1b9O8aTfHjR3F+b8+iXVb9vCHp0/lnx7/FW0dJYoRrNmwk71tRb549gncv/otTpkyjst+czo3P/IKv/fh97J6/Q7+/qFmrjxnBuPHNLBpVysBrNu8m4df3ExbeqFW2sPoVP5mPBAfnDiWlzbu6r1hH40b08Dp0yZw/wsbGTuqng9NGsuK17ZVbDvtPWN4e3cbHaXgglPfy53L3+Do0fX8zimTiIBFK95g7Oh6du3v4LdPmsixR43il69sYda0CdzzZAsf+i9jOfm9x3BEY4E9rUUmHj2alzfuYvyYRsaOrk9vBtmbwiub9rCqZTt/cNoURtUXWLtpF8dPGENdColR9XVdb3id663fvp+JR2d7TY11hfQG2fPfui/PRF//bxm9bC3inb25zjAsKHuzk+gaS+cbZuebcHuxhKSusKgrQFtHicb6wrvWqUtvnI11Bfa1F9/1htv5Rv2O7A1YEnUS+9qLjKov0FEMdrd2MP7IBuq6+qmugOkMwQhorBO7W4s01r/zhj12dH0KxeyxOt+kO/8TMaaxjjqJukKBYikbV/bt40F9oYBEVyhEesPvKGZ7s50BGWn7kfrR+XcvpOAslYJRDVnY1Reyv23n4wtRVyC9jkRDXRaiR42uP+DvUx3DfU9l0A5/mZmNZH3dUxnun1NZAcyQNF1SI3ARsLjGfTIzG7GG9eGviOiQdAWwlGxK8YKI6P2DFWZmVhXDOlQAImIJsKTW/TAzs+F/+MvMzIYQh4qZmeXGoWJmZrlxqJiZWW4cKmZmlpth/eHHgZC0GXh9gKsfCwzeL2RV1+EylsNlHOCxDFWHy1gOdRzvi4im3hqNuFA5FJJW9uUTpcPB4TKWw2Uc4LEMVYfLWAZrHD78ZWZmuXGomJlZbhwq/XNLrTuQo8NlLIfLOMBjGaoOl7EMyjh8TsXMzHLjPRUzM8uNQ6UPJM2R9JKkZklX1bo/fSHpNUnPSVolaWWqTZC0TNLadD0+1SXpu2l8z0o6rcZ9XyBpk6Tny2r97rukean9WknzhtBYrpX0ZnpuVkk6v+y+q9NYXpJ0blm9pq9BSVMlPSxpjaTVkq5M9WH3vPQwluH4vIyWtFzSM2ksf5Pq0yU9kf7Gd6WfBkHSqHS7Od0/rbcx9ltE+NLDhewr9V8B3g80As8AJ9W6X33o92vAsQfU/g9wVVq+Cvh6Wj4f+BnZT+WdCTxR475/HDgNeH6gfQcmAOvS9fi0PH6IjOVa4E8rtD0pvb5GAdPT665uKLwGgUnAaWl5LPBy6u+we156GMtwfF4EHJWWG4An0t/7buCiVP8B8MW0/MfAD9LyRcBdPY1xIH3ynkrvTgeaI2JdRLQBi4C5Ne7TQM0FFqblhcAFZfXbI/M4ME7SpFp0ECAiHgW2HlDub9/PBZZFxNaI2AYsA+ZUv/fv1s1YujMXWBQRrRHxKtBM9vqr+WswIjZExFNpeRewBpjMMHxeehhLd4by8xIRsTvdbEiXAD4B3JPqBz4vnc/XPcA5kkT3Y+w3h0rvJgNvlN1uoecX4FARwP2SnpQ0P9UmRsQGyP5hAcel+nAYY3/7PtTHdEU6LLSg85ARw2Qs6ZDJqWT/Kx7Wz8sBY4Fh+LxIqpO0CthEFtKvANsjoqNCv7r6nO7fAbyHHMfiUOmdKtSGw5S5syLiNOA84HJJH++h7XAdI3Tf96E8ppuBE4CZwAbgm6k+5Mci6Sjgx8CXImJnT00r1Ib6WIbl8xIRxYiYCUwh27s4sVKzdF31sThUetcCTC27PQVYX6O+9FlErE/Xm4B7yV5sGzsPa6XrTan5cBhjf/s+ZMcUERvTG0EJ+AfeOcwwpMciqYHsTfiOiPhJKg/L56XSWIbr89IpIrYDj5CdUxknqfOXfcv71dXndP8xZIdncxuLQ6V3K4AZaTZFI9nJrcU17lOPJB0paWznMjAbeJ6s352zbeYB96XlxcAlacbOmcCOzkMaQ0h/+74UmC1pfDqMMTvVau6A81WfIntuIBvLRWmGznRgBrCcIfAaTMfdbwXWRMS3yu4ads9Ld2MZps9Lk6RxafkI4JNk54geBi5MzQ58XjqfrwuBhyI7U9/dGPtvMGcqDNcL2UyWl8mOVf5lrfvTh/6+n2wmxzPA6s4+kx07fRBYm64npLqAm9L4ngNm1bj/d5Idfmgn+x/UZQPpO/B5shOOzcClQ2gsP0p9fTb9Y55U1v4v01heAs4bKq9B4DfJDoc8C6xKl/OH4/PSw1iG4/NyCvB06vPzwF+n+vvJQqEZ+BdgVKqPTreb0/3v722M/b34E/VmZpYbH/4yM7PcOFTMzCw3DhUzM8uNQ8XMzHLjUDEzs9w4VMyGCUlnS/q3WvfDrCcOFTMzy41DxSxnkj6XfuNilaQfpi/82y3pm5KekvSgpKbUdqakx9OXGN6rd36P5AOSHki/k/GUpBPS5o+SdI+kFyXdkT4dbjZkOFTMciTpROAzZF/oORMoAn8EHAk8FdmXfP4CuCatcjvw5Yg4hezT3J31O4CbIuLDwG+QfSofsm/U/RLZ71+8Hzir6oMy64f63puYWT+cA3wEWJF2Io4g+5LFEnBXavNPwE8kHQOMi4hfpPpC4F/S97ZNjoh7ASJiP0Da3vKIaEm3VwHTgMeqPyyzvnGomOVLwMKIuPpdRekrB7Tr6fuRejqk1Vq2XMT/hm2I8eEvs3w9CFwo6Tjo+g3395H9W+v81tg/BB6LiB3ANkn/LdVxgWNRAAAAlElEQVQvBn4R2W97tEi6IG1jlKQxgzoKswHy/3LMchQRL0j6K7Jf3SyQfTvx5cAe4GRJT5L92t5n0irzgB+k0FgHXJrqFwM/lHRd2sanB3EYZgPmbyk2GwSSdkfEUbXuh1m1+fCXmZnlxnsqZmaWG++pmJlZbhwqZmaWG4eKmZnlxqFiZma5caiYmVluHCpmZpab/w9HeuYTxZQFqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import demo #import *\n",
    "reload(demo)\n",
    "from demo import *\n",
    "import cProfile as profile\n",
    "\n",
    "\n",
    "result = trainGuide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run \"polytopize\", which has all of my pre-Fritz attempts to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9., 12.],\n",
      "        [12., 16.]])\n",
      "tensor(-54.9306)\n",
      "tensor(-1.0986)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ts = torch.tensor\n",
    "import pyro.distributions as dist\n",
    "import pyro\n",
    "import cmult\n",
    "\n",
    "t1 = ts([3.,4.])\n",
    "t2 = ts([2.,5.])\n",
    "\n",
    "m = t1.view(-1,1).mm(t1.view(1,-1))\n",
    "\n",
    "print(m)\n",
    "\n",
    "#means = torch.zeros(3,2)\n",
    "#means[1,1] = 100.\n",
    "with pyro.plate('dumb',3):\n",
    "    #t3 = pyro.sample('irrelevant',dist.Normal(means,ts(1.)).to_event(1))\n",
    "    t3 = pyro.sample('irrelevant',cmult.CMult(50,torch.ones(3,3,3)/3).to_event(2))\n",
    "\n",
    "#print(t3)\n",
    "\n",
    "mult = cmult.CMult(50,torch.ones(3)/3)\n",
    "print(mult.log_prob(ts([0.,50,0])))\n",
    "print(mult.log_prob(ts([0.,1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3764, -0.7578],\n",
      "        [-0.0823,  0.0420],\n",
      "        [ 0.0860,  0.2053]], requires_grad=True)\n",
      "tensor([[-0.0440, -0.0448,  0.1284],\n",
      "        [-0.0490, -0.0334,  0.1120],\n",
      "        [-0.0486, -0.0243,  0.0850],\n",
      "        [-0.0567, -0.0134,  0.0715]], requires_grad=True)\n",
      "tensor([[ 0.0031, -0.0173,  0.0341],\n",
      "        [-0.0019, -0.0059,  0.0177],\n",
      "        [-0.0015,  0.0032, -0.0093],\n",
      "        [-0.0096,  0.0141, -0.0227]], grad_fn=<SubBackward0>)\n",
      "tensor(-3.1610, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "yhat1 = polytopize(4,3,pyro.get_param_store()[\"what_0\"],\n",
    "                 get_indep(4,3,ts([68.,20,25,56]),ts([63.,32,74])))\n",
    "\n",
    "erchat = pyro.get_param_store()[\"erchat\"]\n",
    "\n",
    "print(pyro.get_param_store()[\"what_0\"])\n",
    "print(erchat)\n",
    "print(recenter_rc(erchat))\n",
    "print(pyro.get_param_store()[\"logsdrchat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "p = pstats.Stats(runstats)\n",
    "p.strip_dirs().sort_stats(-1).print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I will run.\n",
      "Yes, I will run.\n",
      "ge fail\n",
      "loc tensor([[ 0.3147],\n",
      "        [ 2.4440],\n",
      "        [-4.0535],\n",
      "        [ 2.2007]])\n",
      "polytopedLoc tensor([[ 2.1063e-01,  2.1136e-02],\n",
      "        [ 2.8514e-01, -1.8626e-09],\n",
      "        [ 3.4959e-01,  9.9957e-02],\n",
      "        [ 3.5462e-01,  1.1892e-02],\n",
      "        [ 2.5462e+00,  3.2228e-01]])\n",
      "ge fail\n",
      "loc tensor([[-5.8855],\n",
      "        [-5.2805],\n",
      "        [ 5.4654],\n",
      "        [ 0.1889]])\n",
      "polytopedLoc tensor([[ 2.8885e-01,  1.1871e+00],\n",
      "        [ 8.5629e-01,  2.0505e+00],\n",
      "        [ 3.9984e-01, -1.4901e-08],\n",
      "        [ 9.5542e-01,  1.5538e+00],\n",
      "        [ 1.0063e+00,  9.9522e-01]])\n",
      "Reloading cmult...\n",
      "callable? <bound method TorchDistributionMixin.__call__ of Multinomial()>\n",
      "callable? <bound method TorchDistributionMixin.__call__ of TorchCMult()>\n",
      "Sampling multinomial: tensor([1., 2.])\n",
      "Sampling cm2: tensor([0., 3.])\n",
      "tensor(5.6022, grad_fn=<NegBackward>) tensor([[112.2500]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import hessian\n",
    "\n",
    "from importlib import reload\n",
    "import polytopize #import *\n",
    "reload(polytopize)\n",
    "from polytopize import *\n",
    "\n",
    "import tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test rank1torch (to get yhat from pi,n,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimize_Q (50 tests): \n",
      "R=3, C=5, tolerance=0.001\n",
      "==================================================\n",
      "Oh no! In test 3, Q has some negative entries:\n",
      "\t trueQ[2][4]=0.00010659269901225343, \n",
      "\t     Q[2][4]=-0.00021605131041724235\n",
      "Oh no! In test 5, Q has some negative entries:\n",
      "\t trueQ[1][4]=0.00011974151857430115, \n",
      "\t     Q[1][4]=-1.1631345842033625e-06\n",
      "Oh no! In test 8, Q has some negative entries:\n",
      "\t trueQ[0][1]=2.882161788875237e-05, \n",
      "\t     Q[0][1]=-0.0004783869662787765\n",
      "Oh no! In test 15, Q has some negative entries:\n",
      "\t trueQ[2][3]=0.0007846675580367446, \n",
      "\t     Q[2][3]=-6.166117964312434e-05\n",
      "Oh no! In test 28, Q has some negative entries:\n",
      "\t trueQ[0][4]=8.13114020274952e-05, \n",
      "\t     Q[0][4]=-0.00018321917741559446\n",
      "Oh no! In test 40, Q has some negative entries:\n",
      "\t trueQ[2][3]=0.00032634526723995805, \n",
      "\t     Q[2][3]=-0.000617634505033493\n",
      "Oh no! In test 47, Q has some negative entries:\n",
      "\t trueQ[2][1]=0.00017936740186996758, \n",
      "\t     Q[2][1]=-0.00041433278238400817\n",
      "Oh no! In test 48, Q has some negative entries:\n",
      "\t trueQ[0][0]=4.524858377408236e-05, \n",
      "\t     Q[0][0]=-0.0006647921400144696\n",
      "\n",
      "Cumulative results for the 50 tests \n",
      "(R=3, C=5, tolerance=0.001):\n",
      "-------------------------------------------\n",
      "Worst error in entry of Q: 0.0019194334745407104\n",
      "\n",
      "To get within tolerance, it took us:\n",
      "002 iterations: ***** 5.0 times\n",
      "003 iterations: ********** 10.0 times\n",
      "004 iterations: *********** 11.0 times\n",
      "005 iterations: ****** 6.0 times\n",
      "006 iterations: **** 4.0 times\n",
      "007 iterations: **** 4.0 times\n",
      "008 iterations: ****** 6.0 times\n",
      "010 iterations: *** 3.0 times\n",
      "013 iterations: * 1.0 times\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import rank1torch #import *\n",
    "reload(rank1torch)\n",
    "from rank1torch import *\n",
    "\n",
    "test_solver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is a cell for directly experimenting with pytorch and/or pyro. Basically, for getting the tensor syntax right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cmult\n",
    "import pyro\n",
    "from importlib import reload\n",
    "reload(cmult)\n",
    "from cmult import CMult\n",
    "\n",
    "#print(CMult(probs=torch.tensor([1., 1., 1., 1.])).log_prob(torch.tensor([1.2, 1.5, 1., 1.])))\n",
    "\n",
    "#print(CMult(100, torch.tensor([ 1., 1., 1., 1.])).sample())\n",
    "\n",
    "R, C = (3,4)\n",
    "\n",
    "y = pyro.distributions.Normal(0.,4.).sample(torch.Size([R-1,C-1]))\n",
    "\n",
    "\n",
    "w = torch.cat((y,-y.sum(1).unsqueeze(1)),1)\n",
    "w = torch.cat((w,-w.sum(0).unsqueeze(0)),0)\n",
    "\n",
    "#print(w)\n",
    "b = w.argmax()\n",
    "print(b)\n",
    "print(w,w[b//C,b % C])\n",
    "\n",
    "tt = torch.zeros(R,C)\n",
    "print(tt,R,C,tt[:(R-1),:(C-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Most SVI problems in pyro are coded as a model, a generic guide (such as: multivariate Gaussian in all parameters), and specific observations/data (passed as arguments to svi.step). For EI, that's going to be different; the observations are going to be built into the guide function, leaving nothing to include in the \"data\" argument to svi.step.\n",
    "\n",
    "That means there is a lot of work for the guide to do. As usual, it must establish reasonable distributional families for the posterior of each of the hyperparameters. But for the latent parameters, the job of the guide is to take a \"relative strength\" number for each race/candidate/precinct combo, and turn that into a number of votes for each combo, such that those numbers obey all the constraints set by observations. This means that for each precinct (considered separately), the latent guide must:\n",
    "\n",
    "-Find the \"center point\" where candidate preference is independent of race.\n",
    "\n",
    "-Find the \"basis vectors\" (actually, there are more than enough of them to form a basis) which determine the directions to move in the space.\n",
    "\n",
    "-For any given set of \"relative strengths\" which is a distance $d$ in a direction $\\theta$, find the first constraint violated when moving in that direction, and the distance $r$ between the origin and that constraint.\n",
    "\n",
    "-Project the \"relative strengths\" onto the numbers of votes, by moving $r(1-e^{-d})$ in direction $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a = zs(2,2,2,2)\n",
    "a[0,1,1,1] = 2\n",
    "print(a[1,1])\n",
    "print(a[0,1])\n",
    "print(torch.max(a))\n",
    "print(torch.distributions.exponential.Exponential(ts([1])).sample(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\",\"world\")\n",
    "a = zs(2,2,2,2)\n",
    "a[0,1,1,1] = 2 \n",
    "print(torch.max(a[0,1])) \n",
    "print(pyro.distributions.Exponential(1.).sample(torch.Size([3])))\n",
    "a.add_(torch.ones(2,2,2))\n",
    "print(torch.exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
