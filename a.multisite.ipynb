{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro\n",
    "\n",
    "Scratch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pyro/__init__.py\n",
      "/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import pyro\n",
    "\n",
    "print(pyro.__file__)\n",
    "\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amortizable hierarchical t model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1114.9764868021011;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "yay -80\n",
      "yay -140\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-771.5305]], grad_fn=<IndexBackward>) tensor([[789.4745]], grad_fn=<IndexBackward>) tensor([[-276.8895]], grad_fn=<IndexBackward>) tensor([[259.0411]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0957]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-776.3513]], grad_fn=<IndexBackward>) tensor([[794.2509]], grad_fn=<IndexBackward>) tensor([[-276.3059]], grad_fn=<IndexBackward>) tensor([[258.2705]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1358]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-777.1024]], grad_fn=<IndexBackward>) tensor([[795.0465]], grad_fn=<IndexBackward>) tensor([[-276.0822]], grad_fn=<IndexBackward>) tensor([[258.0190]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1191]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-778.0443]], grad_fn=<IndexBackward>) tensor([[796.0026]], grad_fn=<IndexBackward>) tensor([[-275.8445]], grad_fn=<IndexBackward>) tensor([[257.7404]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1458]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-779.5526]], grad_fn=<IndexBackward>) tensor([[797.3688]], grad_fn=<IndexBackward>) tensor([[-275.7402]], grad_fn=<IndexBackward>) tensor([[257.5491]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.3748]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-784.7009]], grad_fn=<IndexBackward>) tensor([[801.2920]], grad_fn=<IndexBackward>) tensor([[-275.5828]], grad_fn=<IndexBackward>) tensor([[256.8805]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-2.1112]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[ 40],\n",
      "        [153]]) \n",
      "              1.. tensor([[ 906.0317],\n",
      "        [-783.5305]], grad_fn=<IndexBackward>) tensor([[-925.7383],\n",
      "        [ 800.7679]], grad_fn=<IndexBackward>) tensor([[ 288.9338],\n",
      "        [-275.0769]], grad_fn=<IndexBackward>) tensor([[-269.2373],\n",
      "        [ 256.5736]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0102],\n",
      "        [-1.2658]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-783.4427]], grad_fn=<IndexBackward>) tensor([[800.9654]], grad_fn=<IndexBackward>) tensor([[-274.7126]], grad_fn=<IndexBackward>) tensor([[256.2733]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.9167]], grad_fn=<IndexBackward>)\n",
      "yay -200\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-783.4524]], grad_fn=<IndexBackward>) tensor([[801.2668]], grad_fn=<IndexBackward>) tensor([[-274.3789]], grad_fn=<IndexBackward>) tensor([[256.0011]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.5634]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-784.3168]], grad_fn=<IndexBackward>) tensor([[802.0229]], grad_fn=<IndexBackward>) tensor([[-274.1561]], grad_fn=<IndexBackward>) tensor([[255.6972]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.7529]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -260\n",
      "epoch 100 loss = 1146.9118095636368;\n",
      "mode_hat tensor(0.4247, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3410, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2819, requires_grad=True)\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -20\n",
      "yay -560\n",
      "epoch 200 loss = 902.3052217960358;\n",
      "mode_hat tensor(0.7667, requires_grad=True)\n",
      "ltscale_hat tensor(-0.6244, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4233, requires_grad=True)\n",
      "complaint -40\n",
      "yay -620\n",
      "yay -680\n",
      "yay -740\n",
      "yay -800\n",
      "yay -860\n",
      "complaint -60\n",
      "epoch 300 loss = 1029.0199601650238;\n",
      "mode_hat tensor(0.9636, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9097, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4931, requires_grad=True)\n",
      "yay -920\n",
      "complaint -80\n",
      "yay -980\n",
      "complaint -100\n",
      "yay -1040\n",
      "complaint -120\n",
      "yay -1100\n",
      "complaint -140\n",
      "yay -1160\n",
      "epoch 400 loss = 911.5129956007004;\n",
      "mode_hat tensor(1.0653, requires_grad=True)\n",
      "ltscale_hat tensor(-1.1931, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4677, requires_grad=True)\n",
      "complaint -160\n",
      "yay -1220\n",
      "yay -1280\n",
      "complaint -180\n",
      "yay -1340\n",
      "yay -1400\n",
      "complaint -200\n",
      "yay -1460\n",
      "epoch 500 loss = 1082.9528213739395;\n",
      "mode_hat tensor(1.0653, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5150, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6006, requires_grad=True)\n",
      "Final mean_losses: 1126.488160642747\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.4171, -4.2824], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.6005856394767761\n",
      "ltscale_hat:\n",
      "-1.5150495767593384\n",
      "mode_hat:\n",
      "1.0652893781661987\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "epoch 0 loss = 37581.71555542946;\n",
      "mode_hat tensor(-0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 8995.700284957886;\n",
      "mode_hat tensor(0.2358, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5040, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4884, requires_grad=True)\n",
      "epoch 200 loss = 28029.957255125046;\n",
      "mode_hat tensor(0.4271, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0040, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9680, requires_grad=True)\n",
      "epoch 300 loss = 10403.773786783218;\n",
      "mode_hat tensor(0.5949, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4993, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4347, requires_grad=True)\n",
      "epoch 400 loss = 6871.094324290752;\n",
      "mode_hat tensor(0.7665, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9904, requires_grad=True)\n",
      "ldfraw_hat tensor(1.9170, requires_grad=True)\n",
      "epoch 500 loss = 30553.035027861595;\n",
      "mode_hat tensor(0.8413, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4782, requires_grad=True)\n",
      "ldfraw_hat tensor(2.3658, requires_grad=True)\n",
      "epoch 600 loss = 16239.887854099274;\n",
      "mode_hat tensor(0.7939, requires_grad=True)\n",
      "ltscale_hat tensor(-2.9496, requires_grad=True)\n",
      "ldfraw_hat tensor(2.7681, requires_grad=True)\n",
      "Final mean_losses: 22385.347178032338\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "ldfraw_hat:\n",
      "2.9899110794067383\n",
      "ldfraw_sigma:\n",
      "0.8052737712860107\n",
      "ltscale_hat:\n",
      "-3.2288105487823486\n",
      "ltscale_sigma:\n",
      "0.8304917216300964\n",
      "mode_hat:\n",
      "0.8919063210487366\n",
      "mode_sigma:\n",
      "0.44034138321876526\n",
      "t_part_hat:\n",
      "tensor([-0.2679, -0.0297,  0.0994,  0.2053, -0.2138,  0.2332,  0.2859,  0.2633,\n",
      "         0.0520,  0.2362], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([0.9562, 0.9287, 1.0097, 0.8918, 1.0888, 1.0035, 1.0283, 0.9693, 0.9522,\n",
      "        0.9910], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unamortized_laplace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "epoch 0 loss = 2072.3795664310455;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 1703.8440679311752;\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3304, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2016, requires_grad=True)\n",
      "epoch 200 loss = 1163.0583981275558;\n",
      "mode_hat tensor(1.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5834, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3087, requires_grad=True)\n",
      "epoch 300 loss = 1039.4693734645844;\n",
      "mode_hat tensor(1.0183, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8257, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3863, requires_grad=True)\n",
      "epoch 400 loss = 1036.214755654335;\n",
      "mode_hat tensor(1.0276, requires_grad=True)\n",
      "ltscale_hat tensor(-1.1008, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4480, requires_grad=True)\n",
      "epoch 500 loss = 1048.2735177278519;\n",
      "mode_hat tensor(1.0294, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3527, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4593, requires_grad=True)\n",
      "epoch 600 loss = 1197.7754633426666;\n",
      "mode_hat tensor(1.0384, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6063, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5677, requires_grad=True)\n",
      "epoch 700 loss = 958.1399931907654;\n",
      "mode_hat tensor(1.0193, requires_grad=True)\n",
      "ltscale_hat tensor(-1.8339, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5576, requires_grad=True)\n",
      "epoch 800 loss = 884.1477173566818;\n",
      "mode_hat tensor(1.0371, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0176, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5453, requires_grad=True)\n",
      "epoch 900 loss = 1160.8390724658966;\n",
      "mode_hat tensor(1.0315, requires_grad=True)\n",
      "ltscale_hat tensor(-2.2328, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5564, requires_grad=True)\n",
      "Final mean_losses: 1110.5487087218098\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "full_tmode:\n",
      "tensor([-8.7556, -0.4927, -0.2364,  1.1589, -1.7415, 10.4468,  2.7066,  1.6360,\n",
      "        -0.0335,  2.3776], grad_fn=<SliceBackward>) (10 elems)\n",
      "globalpsi:\n",
      "tensor([-4.1997, -3.9471], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.46891599893569946\n",
      "ltscale_hat:\n",
      "-2.2755119800567627\n",
      "mode_hat:\n",
      "1.0230517387390137\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-966.8629],\n",
      "        [-867.0729]], grad_fn=<IndexBackward>) tensor([[977.0577],\n",
      "        [886.6078]], grad_fn=<IndexBackward>) tensor([[-304.3145],\n",
      "        [-306.2899]], grad_fn=<IndexBackward>) tensor([[294.1312],\n",
      "        [287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0116],\n",
      "        [0.2516]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-966.8629],\n",
      "        [-867.0729]], grad_fn=<IndexBackward>) tensor([[977.0577],\n",
      "        [886.6078]], grad_fn=<IndexBackward>) tensor([[-304.3145],\n",
      "        [-306.2899]], grad_fn=<IndexBackward>) tensor([[294.1312],\n",
      "        [287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0116],\n",
      "        [0.2516]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-966.8629],\n",
      "        [-867.0729]], grad_fn=<IndexBackward>) tensor([[977.0577],\n",
      "        [886.6078]], grad_fn=<IndexBackward>) tensor([[-304.3145],\n",
      "        [-306.2899]], grad_fn=<IndexBackward>) tensor([[294.1312],\n",
      "        [287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0116],\n",
      "        [0.2516]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1027.5691760778427;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 6 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-968.3418],\n",
      "        [1022.3344],\n",
      "        [-868.4785]], grad_fn=<IndexBackward>) tensor([[  978.5433],\n",
      "        [-1032.6825],\n",
      "        [  888.0209]], grad_fn=<IndexBackward>) tensor([[-303.8991],\n",
      "        [ 309.2611],\n",
      "        [-305.9195]], grad_fn=<IndexBackward>) tensor([[ 293.6870],\n",
      "        [-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0107],\n",
      "        [-0.0118],\n",
      "        [ 0.1996]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-968.3418],\n",
      "        [1022.3344],\n",
      "        [-868.4785]], grad_fn=<IndexBackward>) tensor([[  978.5433],\n",
      "        [-1032.6825],\n",
      "        [  888.0209]], grad_fn=<IndexBackward>) tensor([[-303.8991],\n",
      "        [ 309.2611],\n",
      "        [-305.9195]], grad_fn=<IndexBackward>) tensor([[ 293.6870],\n",
      "        [-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0107],\n",
      "        [-0.0118],\n",
      "        [ 0.1996]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-968.3418],\n",
      "        [1022.3344],\n",
      "        [-868.4785]], grad_fn=<IndexBackward>) tensor([[  978.5433],\n",
      "        [-1032.6825],\n",
      "        [  888.0209]], grad_fn=<IndexBackward>) tensor([[-303.8991],\n",
      "        [ 309.2611],\n",
      "        [-305.9195]], grad_fn=<IndexBackward>) tensor([[ 293.6870],\n",
      "        [-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0107],\n",
      "        [-0.0118],\n",
      "        [ 0.1996]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "complaint 3 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-873.2335]], grad_fn=<IndexBackward>) tensor([[891.7161]], grad_fn=<IndexBackward>) tensor([[-305.9045]], grad_fn=<IndexBackward>) tensor([[286.1126]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.3093]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-874.4150]], grad_fn=<IndexBackward>) tensor([[892.5202]], grad_fn=<IndexBackward>) tensor([[-306.0424]], grad_fn=<IndexBackward>) tensor([[286.1126]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.8245]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-874.4150]], grad_fn=<IndexBackward>) tensor([[892.5202]], grad_fn=<IndexBackward>) tensor([[-306.0424]], grad_fn=<IndexBackward>) tensor([[286.1126]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.8245]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 0 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-874.4100]], grad_fn=<IndexBackward>) tensor([[892.9740]], grad_fn=<IndexBackward>) tensor([[-305.4952]], grad_fn=<IndexBackward>) tensor([[285.6730]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.2582]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "complaint -20\n",
      "yay -20\n",
      "complaint -40\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "complaint -60\n",
      "complaint -80\n",
      "yay -260\n",
      "complaint -100\n",
      "epoch 100 loss = 1205.972628633181;\n",
      "mode_hat tensor(0.3443, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3859, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3193, requires_grad=True)\n",
      "complaint -120\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "yay -560\n",
      "complaint -140\n",
      "epoch 200 loss = 890.863477150599;\n",
      "mode_hat tensor(0.5723, requires_grad=True)\n",
      "ltscale_hat tensor(-0.6453, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4832, requires_grad=True)\n",
      "complaint -160\n",
      "yay -620\n",
      "complaint -180\n",
      "complaint -200\n",
      "complaint -220\n",
      "yay -680\n",
      "complaint -240\n",
      "complaint -260\n",
      "yay -740\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 906.9384283224742;\n",
      "mode_hat tensor(0.7308, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0078, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6216, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 400 loss = 992.8926900227865;\n",
      "mode_hat tensor(0.8931, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3673, requires_grad=True)\n",
      "ldfraw_hat tensor(0.7600, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "complaint -280\n",
      "epoch 500 loss = 1212.1282603740692;\n",
      "mode_hat tensor(0.9838, requires_grad=True)\n",
      "ltscale_hat tensor(-1.7219, requires_grad=True)\n",
      "ldfraw_hat tensor(0.8114, requires_grad=True)\n",
      "Final mean_losses: 1073.6280764819255\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.0672, -3.9105], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.8114436268806458\n",
      "ltscale_hat:\n",
      "-1.7218799591064453\n",
      "mode_hat:\n",
      "0.983806312084198\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "epoch 0 loss = 4900.660924951236;\n",
      "mode_hat tensor(-0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 30558.624184668064;\n",
      "mode_hat tensor(0.3091, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4978, requires_grad=True)\n",
      "epoch 200 loss = 8163.9516783158;\n",
      "mode_hat tensor(0.4893, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9800, requires_grad=True)\n",
      "epoch 300 loss = 76523.1078886489;\n",
      "mode_hat tensor(0.7550, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4669, requires_grad=True)\n",
      "epoch 400 loss = 9292.706171671549;\n",
      "mode_hat tensor(0.8087, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.9472, requires_grad=True)\n",
      "epoch 500 loss = 12375.91801504294;\n",
      "mode_hat tensor(0.9601, requires_grad=True)\n",
      "ltscale_hat tensor(-2.5046, requires_grad=True)\n",
      "ldfraw_hat tensor(2.4272, requires_grad=True)\n",
      "Final mean_losses: 16779.938387010912\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "ldfraw_hat:\n",
      "2.4272196292877197\n",
      "ldfraw_sigma:\n",
      "0.7864524722099304\n",
      "ltscale_hat:\n",
      "-2.504622459411621\n",
      "ltscale_sigma:\n",
      "0.39579132199287415\n",
      "mode_hat:\n",
      "0.9600974917411804\n",
      "mode_sigma:\n",
      "0.2589780390262604\n",
      "t_part_hat:\n",
      "tensor([-0.1731,  0.0594,  0.0246,  0.2474, -0.3132,  0.3078,  0.2214,  0.1850,\n",
      "        -0.0050,  0.2410], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([1.0201, 0.8683, 1.0407, 0.9105, 0.9082, 0.8922, 0.9803, 1.0777, 1.0097,\n",
      "        1.0097], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "epoch 0 loss = 2069.3364456494646;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 1862.042096555233;\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3519, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2239, requires_grad=True)\n",
      "epoch 200 loss = 1237.0362072785697;\n",
      "mode_hat tensor(1.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.6904, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4345, requires_grad=True)\n",
      "epoch 300 loss = 1241.9938262104988;\n",
      "mode_hat tensor(1.0159, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0571, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5575, requires_grad=True)\n",
      "epoch 400 loss = 985.3348168134689;\n",
      "mode_hat tensor(1.0318, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3615, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5953, requires_grad=True)\n",
      "epoch 500 loss = 922.269272963206;\n",
      "mode_hat tensor(1.0386, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6827, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6416, requires_grad=True)\n",
      "epoch 600 loss = 2898.2399175167084;\n",
      "mode_hat tensor(1.0331, requires_grad=True)\n",
      "ltscale_hat tensor(-1.8859, requires_grad=True)\n",
      "ldfraw_hat tensor(0.7088, requires_grad=True)\n",
      "epoch 700 loss = 895.0144120852151;\n",
      "mode_hat tensor(1.0284, requires_grad=True)\n",
      "ltscale_hat tensor(-2.1176, requires_grad=True)\n",
      "ldfraw_hat tensor(0.7843, requires_grad=True)\n",
      "epoch 800 loss = 995.3754666248958;\n",
      "mode_hat tensor(1.0198, requires_grad=True)\n",
      "ltscale_hat tensor(-2.1982, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6912, requires_grad=True)\n",
      "epoch 900 loss = 917.4025261799495;\n",
      "mode_hat tensor(1.0296, requires_grad=True)\n",
      "ltscale_hat tensor(-2.3996, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6862, requires_grad=True)\n",
      "epoch 1000 loss = 1073.515604933103;\n",
      "mode_hat tensor(1.0289, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4785, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6131, requires_grad=True)\n",
      "Final mean_losses: 1338.1064031761923\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "full_tmode:\n",
      "tensor([-8.9063, -0.4252, -0.1566,  1.2872, -1.8144, 10.3430,  2.8128,  1.7019,\n",
      "        -0.0562,  2.3456], grad_fn=<SliceBackward>) (10 elems)\n",
      "globalpsi:\n",
      "tensor([-3.9983, -3.2190], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.5416245460510254\n",
      "ltscale_hat:\n",
      "-2.5224592685699463\n",
      "mode_hat:\n",
      "1.0309780836105347\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1111.0498321652412;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-757.1621]], grad_fn=<IndexBackward>) tensor([[768.6618]], grad_fn=<IndexBackward>) tensor([[-269.7281]], grad_fn=<IndexBackward>) tensor([[258.2900]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0615]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-759.9008]], grad_fn=<IndexBackward>) tensor([[771.3469]], grad_fn=<IndexBackward>) tensor([[-269.1592]], grad_fn=<IndexBackward>) tensor([[257.6593]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0537]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-762.0263]], grad_fn=<IndexBackward>) tensor([[773.6189]], grad_fn=<IndexBackward>) tensor([[-268.4806]], grad_fn=<IndexBackward>) tensor([[256.9818]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0938]], grad_fn=<IndexBackward>)\n",
      "yay -80\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-764.8963]], grad_fn=<IndexBackward>) tensor([[776.3964]], grad_fn=<IndexBackward>) tensor([[-268.0226]], grad_fn=<IndexBackward>) tensor([[256.4444]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0781]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-765.7379]], grad_fn=<IndexBackward>) tensor([[777.3843]], grad_fn=<IndexBackward>) tensor([[-267.6634]], grad_fn=<IndexBackward>) tensor([[256.1127]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0957]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1.. tensor([[-766.7546]], grad_fn=<IndexBackward>) tensor([[778.4912]], grad_fn=<IndexBackward>) tensor([[-267.3223]], grad_fn=<IndexBackward>) tensor([[255.7760]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1903]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-768.1425]], grad_fn=<IndexBackward>) tensor([[779.8497]], grad_fn=<IndexBackward>) tensor([[-267.0170]], grad_fn=<IndexBackward>) tensor([[255.4306]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1207]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-768.2784]], grad_fn=<IndexBackward>) tensor([[780.3611]], grad_fn=<IndexBackward>) tensor([[-266.5933]], grad_fn=<IndexBackward>) tensor([[255.1094]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.5987]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-772.8248]], grad_fn=<IndexBackward>) tensor([[783.8578]], grad_fn=<IndexBackward>) tensor([[-266.6729]], grad_fn=<IndexBackward>) tensor([[254.7818]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.8581]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-772.8827]], grad_fn=<IndexBackward>) tensor([[784.3181]], grad_fn=<IndexBackward>) tensor([[-266.1768]], grad_fn=<IndexBackward>) tensor([[254.4047]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.3368]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1144.7012115120888;\n",
      "mode_hat tensor(0.5022, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4259, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1471, requires_grad=True)\n",
      "yay -320\n",
      "complaint -20\n",
      "yay -380\n",
      "complaint -40\n",
      "yay -440\n",
      "complaint -60\n",
      "yay -500\n",
      "complaint -80\n",
      "yay -560\n",
      "epoch 200 loss = 1085.1144585609436;\n",
      "mode_hat tensor(0.9396, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8308, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1269, requires_grad=True)\n",
      "complaint -100\n",
      "yay -620\n",
      "yay -680\n",
      "complaint -120\n",
      "yay -740\n",
      "complaint -140\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 1087.36568069458;\n",
      "mode_hat tensor(1.0649, requires_grad=True)\n",
      "ltscale_hat tensor(-1.1974, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0261, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "complaint -160\n",
      "yay -1100\n",
      "complaint -180\n",
      "yay -1160\n",
      "complaint -200\n",
      "epoch 400 loss = 1073.402023613453;\n",
      "mode_hat tensor(1.0698, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5464, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1058, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 1002.5532005429268;\n",
      "mode_hat tensor(1.0962, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9210, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1995, requires_grad=True)\n",
      "yay -1520\n",
      "yay -1580\n",
      "yay -1640\n",
      "complaint -240\n",
      "yay -1700\n",
      "complaint -260\n",
      "yay -1760\n",
      "epoch 600 loss = 1016.9895387887955;\n",
      "mode_hat tensor(1.1201, requires_grad=True)\n",
      "ltscale_hat tensor(-2.2190, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2938, requires_grad=True)\n",
      "complaint -280\n",
      "yay -1820\n",
      "complaint -300\n",
      "yay -1880\n",
      "yay -1940\n",
      "complaint -320\n",
      "yay -2000\n",
      "complaint -340\n",
      "yay -2060\n",
      "epoch 700 loss = 1064.4795163273811;\n",
      "mode_hat tensor(1.1367, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4901, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5324, requires_grad=True)\n",
      "yay -2120\n",
      "Final mean_losses: 1088.7175880363184\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.5937, -4.5370], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.5734208226203918\n",
      "ltscale_hat:\n",
      "-2.500462532043457\n",
      "mode_hat:\n",
      "1.151228427886963\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "epoch 0 loss = 63598.76760184765;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 6409.815923035145;\n",
      "mode_hat tensor(0.3662, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4833, requires_grad=True)\n",
      "epoch 200 loss = 25959.586943030357;\n",
      "mode_hat tensor(0.5893, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0049, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9544, requires_grad=True)\n",
      "epoch 300 loss = 20776.08464396;\n",
      "mode_hat tensor(0.8216, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5013, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4343, requires_grad=True)\n",
      "epoch 400 loss = 16153.775314509869;\n",
      "mode_hat tensor(1.0074, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9963, requires_grad=True)\n",
      "ldfraw_hat tensor(1.9043, requires_grad=True)\n",
      "epoch 500 loss = 13523.322404384613;\n",
      "mode_hat tensor(0.9723, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4797, requires_grad=True)\n",
      "ldfraw_hat tensor(2.3602, requires_grad=True)\n",
      "epoch 600 loss = 18005.01995098591;\n",
      "mode_hat tensor(0.9971, requires_grad=True)\n",
      "ltscale_hat tensor(-2.9649, requires_grad=True)\n",
      "ldfraw_hat tensor(2.7735, requires_grad=True)\n",
      "epoch 700 loss = 13681.518928825855;\n",
      "mode_hat tensor(1.0547, requires_grad=True)\n",
      "ltscale_hat tensor(-3.4092, requires_grad=True)\n",
      "ldfraw_hat tensor(3.1702, requires_grad=True)\n",
      "epoch 800 loss = 11618.678466290236;\n",
      "mode_hat tensor(1.1153, requires_grad=True)\n",
      "ltscale_hat tensor(-3.8505, requires_grad=True)\n",
      "ldfraw_hat tensor(3.5028, requires_grad=True)\n",
      "epoch 900 loss = 8948.372858822346;\n",
      "mode_hat tensor(1.1380, requires_grad=True)\n",
      "ltscale_hat tensor(-4.2744, requires_grad=True)\n",
      "ldfraw_hat tensor(3.7537, requires_grad=True)\n",
      "epoch 1000 loss = 11381.651378154755;\n",
      "mode_hat tensor(1.1135, requires_grad=True)\n",
      "ltscale_hat tensor(-4.7007, requires_grad=True)\n",
      "ldfraw_hat tensor(3.9028, requires_grad=True)\n",
      "epoch 1100 loss = 5075.795046389103;\n",
      "mode_hat tensor(1.1616, requires_grad=True)\n",
      "ltscale_hat tensor(-5.1525, requires_grad=True)\n",
      "ldfraw_hat tensor(3.9077, requires_grad=True)\n",
      "epoch 1200 loss = 9005.242541909218;\n",
      "mode_hat tensor(1.1257, requires_grad=True)\n",
      "ltscale_hat tensor(-5.5971, requires_grad=True)\n",
      "ldfraw_hat tensor(3.9417, requires_grad=True)\n",
      "epoch 1300 loss = 15012.458707690239;\n",
      "mode_hat tensor(1.1365, requires_grad=True)\n",
      "ltscale_hat tensor(-6.0759, requires_grad=True)\n",
      "ldfraw_hat tensor(3.9737, requires_grad=True)\n",
      "epoch 1400 loss = 3733.5869463682175;\n",
      "mode_hat tensor(1.1426, requires_grad=True)\n",
      "ltscale_hat tensor(-6.5032, requires_grad=True)\n",
      "ldfraw_hat tensor(3.9162, requires_grad=True)\n",
      "epoch 1500 loss = 4716.8811666965485;\n",
      "mode_hat tensor(1.1909, requires_grad=True)\n",
      "ltscale_hat tensor(-6.9820, requires_grad=True)\n",
      "ldfraw_hat tensor(3.8712, requires_grad=True)\n",
      "epoch 1600 loss = 6176.358779788017;\n",
      "mode_hat tensor(1.1293, requires_grad=True)\n",
      "ltscale_hat tensor(-7.4005, requires_grad=True)\n",
      "ldfraw_hat tensor(3.8455, requires_grad=True)\n",
      "epoch 1700 loss = 2435.3630352020264;\n",
      "mode_hat tensor(1.1442, requires_grad=True)\n",
      "ltscale_hat tensor(-7.8584, requires_grad=True)\n",
      "ldfraw_hat tensor(3.7208, requires_grad=True)\n",
      "epoch 1800 loss = 3879.1615775823593;\n",
      "mode_hat tensor(1.1709, requires_grad=True)\n",
      "ltscale_hat tensor(-8.3298, requires_grad=True)\n",
      "ldfraw_hat tensor(3.5882, requires_grad=True)\n",
      "epoch 1900 loss = 4539.83142387867;\n",
      "mode_hat tensor(1.1543, requires_grad=True)\n",
      "ltscale_hat tensor(-8.7256, requires_grad=True)\n",
      "ldfraw_hat tensor(3.5470, requires_grad=True)\n",
      "epoch 2000 loss = 3329.938764333725;\n",
      "mode_hat tensor(1.1131, requires_grad=True)\n",
      "ltscale_hat tensor(-9.0971, requires_grad=True)\n",
      "ldfraw_hat tensor(3.4196, requires_grad=True)\n",
      "Final mean_losses: 5895.383590851067\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "ldfraw_hat:\n",
      "3.4195523262023926\n",
      "ldfraw_sigma:\n",
      "0.7216819524765015\n",
      "ltscale_hat:\n",
      "-9.097050666809082\n",
      "ltscale_sigma:\n",
      "3.527437210083008\n",
      "mode_hat:\n",
      "1.1130566596984863\n",
      "mode_sigma:\n",
      "0.05950501933693886\n",
      "t_part_hat:\n",
      "tensor([-2.8334, -0.5718, -0.0990,  0.9866, -1.9422,  3.1326,  2.2286,  1.4074,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        -0.0561,  2.1383], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([0.9521, 0.4758, 0.8781, 0.9984, 0.5650, 0.9104, 1.0012, 0.5246, 0.6280,\n",
      "        0.5272], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "epoch 0 loss = 3341.8650502562523;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 1562.6680627465248;\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4764, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2431, requires_grad=True)\n",
      "epoch 200 loss = 1053.3827584981918;\n",
      "mode_hat tensor(0.8788, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9049, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1915, requires_grad=True)\n",
      "epoch 300 loss = 1030.3273932933807;\n",
      "mode_hat tensor(0.8630, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2624, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0294, requires_grad=True)\n",
      "epoch 400 loss = 1150.6470833420753;\n",
      "mode_hat tensor(0.8780, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5722, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1568, requires_grad=True)\n",
      "epoch 500 loss = 1088.654547214508;\n",
      "mode_hat tensor(0.8797, requires_grad=True)\n",
      "ltscale_hat tensor(-1.8918, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.3396, requires_grad=True)\n",
      "epoch 600 loss = 1076.337134361267;\n",
      "mode_hat tensor(0.8921, requires_grad=True)\n",
      "ltscale_hat tensor(-2.2436, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.4398, requires_grad=True)\n",
      "epoch 700 loss = 1032.938985645771;\n",
      "mode_hat tensor(0.8860, requires_grad=True)\n",
      "ltscale_hat tensor(-2.5070, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5916, requires_grad=True)\n",
      "Final mean_losses: 1154.9365422330065\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "full_tmode:\n",
      "tensor([-8.6139e+00, -3.6221e-01, -3.8130e-02,  1.3834e+00, -1.6585e+00,\n",
      "         1.0591e+01,  2.9083e+00,  1.8538e+00,  8.3795e-03,  2.5083e+00],\n",
      "       grad_fn=<SliceBackward>) (10 elems)\n",
      "globalpsi:\n",
      "tensor([-4.5578, -4.4762], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.6733982563018799\n",
      "ltscale_hat:\n",
      "-2.6512293815612793\n",
      "mode_hat:\n",
      "0.8908804059028625\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1132.167441566785;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "yay -80\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-747.7946]], grad_fn=<IndexBackward>) tensor([[764.7070]], grad_fn=<IndexBackward>) tensor([[-266.6316]], grad_fn=<IndexBackward>) tensor([[249.8571]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1379]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-747.7946]], grad_fn=<IndexBackward>) tensor([[764.7070]], grad_fn=<IndexBackward>) tensor([[-266.6316]], grad_fn=<IndexBackward>) tensor([[249.8571]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1379]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-747.7946]], grad_fn=<IndexBackward>) tensor([[764.7070]], grad_fn=<IndexBackward>) tensor([[-266.6316]], grad_fn=<IndexBackward>) tensor([[249.8571]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1379]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-749.3693]], grad_fn=<IndexBackward>) tensor([[766.1761]], grad_fn=<IndexBackward>) tensor([[-266.3263]], grad_fn=<IndexBackward>) tensor([[249.4660]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0536]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-749.3693]], grad_fn=<IndexBackward>) tensor([[766.1761]], grad_fn=<IndexBackward>) tensor([[-266.3263]], grad_fn=<IndexBackward>) tensor([[249.4660]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0536]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-749.3693]], grad_fn=<IndexBackward>) tensor([[766.1761]], grad_fn=<IndexBackward>) tensor([[-266.3263]], grad_fn=<IndexBackward>) tensor([[249.4660]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0536]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-750.7706]], grad_fn=<IndexBackward>) tensor([[767.5319]], grad_fn=<IndexBackward>) tensor([[-265.8611]], grad_fn=<IndexBackward>) tensor([[248.9438]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1560]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-750.7706]], grad_fn=<IndexBackward>) tensor([[767.5319]], grad_fn=<IndexBackward>) tensor([[-265.8611]], grad_fn=<IndexBackward>) tensor([[248.9438]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1560]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-750.7706]], grad_fn=<IndexBackward>) tensor([[767.5319]], grad_fn=<IndexBackward>) tensor([[-265.8611]], grad_fn=<IndexBackward>) tensor([[248.9438]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1560]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-751.8135]], grad_fn=<IndexBackward>) tensor([[768.6473]], grad_fn=<IndexBackward>) tensor([[-265.3269]], grad_fn=<IndexBackward>) tensor([[248.3977]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0955]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "complaint -20\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "complaint -40\n",
      "epoch 100 loss = 1002.2044886946678;\n",
      "mode_hat tensor(0.4310, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5002, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2178, requires_grad=True)\n",
      "complaint -60\n",
      "yay -320\n",
      "complaint -80\n",
      "complaint -100\n",
      "yay -380\n",
      "complaint -120\n",
      "complaint -140\n",
      "complaint -160\n",
      "yay -440\n",
      "complaint -180\n",
      "complaint -200\n",
      "complaint -220\n",
      "yay -500\n",
      "complaint -240\n",
      "complaint -260\n",
      "complaint -280\n",
      "yay -560\n",
      "complaint -300\n",
      "epoch 200 loss = 1019.50348285834;\n",
      "mode_hat tensor(0.7497, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9121, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1668, requires_grad=True)\n",
      "complaint -320\n",
      "yay -620\n",
      "yay -680\n",
      "yay -740\n",
      "complaint -340\n",
      "complaint -360\n",
      "yay -800\n",
      "complaint -380\n",
      "complaint -400\n",
      "complaint -420\n",
      "yay -860\n",
      "complaint -440\n",
      "epoch 300 loss = 1083.2484758496284;\n",
      "mode_hat tensor(0.9611, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3034, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0227, requires_grad=True)\n",
      "yay -920\n",
      "complaint -460\n",
      "yay -980\n",
      "complaint -480\n",
      "complaint -500\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 1098.9403932293255;\n",
      "mode_hat tensor(1.1054, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6795, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1584, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "complaint -520\n",
      "yay -1340\n",
      "complaint -540\n",
      "complaint -560\n",
      "complaint -580\n",
      "yay -1400\n",
      "complaint -600\n",
      "complaint -620\n",
      "complaint -640\n",
      "yay -1460\n",
      "epoch 500 loss = 1062.262545287609;\n",
      "mode_hat tensor(1.1085, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0241, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2637, requires_grad=True)\n",
      "yay -1520\n",
      "complaint -660\n",
      "complaint -680\n",
      "yay -1580\n",
      "complaint -700\n",
      "complaint -720\n",
      "yay -1640\n",
      "Final mean_losses: 1160.8965105199702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.5535, -4.2508], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.3592480719089508\n",
      "ltscale_hat:\n",
      "-2.121051073074341\n",
      "mode_hat:\n",
      "1.0897966623306274\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "epoch 0 loss = 31418.25775653124;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 11213.163178265095;\n",
      "mode_hat tensor(0.3239, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4975, requires_grad=True)\n",
      "epoch 200 loss = 8965.621189236641;\n",
      "mode_hat tensor(0.4756, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9846, requires_grad=True)\n",
      "epoch 300 loss = 18509.079420775175;\n",
      "mode_hat tensor(0.7204, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4732, requires_grad=True)\n",
      "epoch 400 loss = 9306.379492719969;\n",
      "mode_hat tensor(0.7358, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.9576, requires_grad=True)\n",
      "epoch 500 loss = 8107.193083693584;\n",
      "mode_hat tensor(0.9052, requires_grad=True)\n",
      "ltscale_hat tensor(-2.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(2.4361, requires_grad=True)\n",
      "epoch 600 loss = 18748.474509576958;\n",
      "mode_hat tensor(0.9398, requires_grad=True)\n",
      "ltscale_hat tensor(-3.0009, requires_grad=True)\n",
      "ldfraw_hat tensor(2.8957, requires_grad=True)\n",
      "epoch 700 loss = 12066.64346063137;\n",
      "mode_hat tensor(0.9541, requires_grad=True)\n",
      "ltscale_hat tensor(-3.4809, requires_grad=True)\n",
      "ldfraw_hat tensor(3.3391, requires_grad=True)\n",
      "epoch 800 loss = 13548.002039452393;\n",
      "mode_hat tensor(0.9744, requires_grad=True)\n",
      "ltscale_hat tensor(-3.9619, requires_grad=True)\n",
      "ldfraw_hat tensor(3.7471, requires_grad=True)\n",
      "epoch 900 loss = 3488.1667805512743;\n",
      "mode_hat tensor(1.0773, requires_grad=True)\n",
      "ltscale_hat tensor(-4.4428, requires_grad=True)\n",
      "ldfraw_hat tensor(3.9745, requires_grad=True)\n",
      "epoch 1000 loss = 2212.154203792413;\n",
      "mode_hat tensor(1.0993, requires_grad=True)\n",
      "ltscale_hat tensor(-4.9211, requires_grad=True)\n",
      "ldfraw_hat tensor(4.0835, requires_grad=True)\n",
      "epoch 1100 loss = 14759.415320853393;\n",
      "mode_hat tensor(1.0273, requires_grad=True)\n",
      "ltscale_hat tensor(-5.3992, requires_grad=True)\n",
      "ldfraw_hat tensor(4.0530, requires_grad=True)\n",
      "epoch 1200 loss = 29137.13357780377;\n",
      "mode_hat tensor(1.0580, requires_grad=True)\n",
      "ltscale_hat tensor(-5.8766, requires_grad=True)\n",
      "ldfraw_hat tensor(4.1343, requires_grad=True)\n",
      "epoch 1300 loss = 20933.492733915646;\n",
      "mode_hat tensor(1.0298, requires_grad=True)\n",
      "ltscale_hat tensor(-6.3601, requires_grad=True)\n",
      "ldfraw_hat tensor(4.0745, requires_grad=True)\n",
      "epoch 1400 loss = 7932.906663258871;\n",
      "mode_hat tensor(1.0100, requires_grad=True)\n",
      "ltscale_hat tensor(-6.8354, requires_grad=True)\n",
      "ldfraw_hat tensor(4.0588, requires_grad=True)\n",
      "epoch 1500 loss = 17214.21116361022;\n",
      "mode_hat tensor(1.0559, requires_grad=True)\n",
      "ltscale_hat tensor(-7.3221, requires_grad=True)\n",
      "ldfraw_hat tensor(4.0894, requires_grad=True)\n",
      "epoch 1600 loss = 12396.135161797207;\n",
      "mode_hat tensor(1.0488, requires_grad=True)\n",
      "ltscale_hat tensor(-7.8039, requires_grad=True)\n",
      "ldfraw_hat tensor(4.0374, requires_grad=True)\n",
      "epoch 1700 loss = 1341.0850251416366;\n",
      "mode_hat tensor(1.0086, requires_grad=True)\n",
      "ltscale_hat tensor(-8.2614, requires_grad=True)\n",
      "ldfraw_hat tensor(3.9041, requires_grad=True)\n",
      "epoch 1800 loss = 2466.1292862494784;\n",
      "mode_hat tensor(1.0477, requires_grad=True)\n",
      "ltscale_hat tensor(-8.7231, requires_grad=True)\n",
      "ldfraw_hat tensor(3.8337, requires_grad=True)\n",
      "epoch 1900 loss = 22808.3576639692;\n",
      "mode_hat tensor(1.0283, requires_grad=True)\n",
      "ltscale_hat tensor(-9.1663, requires_grad=True)\n",
      "ldfraw_hat tensor(3.7771, requires_grad=True)\n",
      "epoch 2000 loss = 8095.79810321331;\n",
      "mode_hat tensor(0.9901, requires_grad=True)\n",
      "ltscale_hat tensor(-9.5367, requires_grad=True)\n",
      "ldfraw_hat tensor(3.6251, requires_grad=True)\n",
      "Final mean_losses: 8386.230427435154\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "ldfraw_hat:\n",
      "3.625087022781372\n",
      "ldfraw_sigma:\n",
      "0.6543295383453369\n",
      "ltscale_hat:\n",
      "-9.536681175231934\n",
      "ltscale_sigma:\n",
      "3.361572504043579\n",
      "mode_hat:\n",
      "0.990127444267273\n",
      "mode_sigma:\n",
      "0.029913613572716713\n",
      "t_part_hat:\n",
      "tensor([-3.3248, -0.4576, -0.1692,  1.4640, -1.8430,  3.0019,  2.8922,  1.6838,\n",
      "        -0.0328,  2.2436], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([0.5955, 0.3233, 0.3077, 0.2280, 0.2963, 0.7949, 0.2221, 0.2640, 0.5316,\n",
      "        0.4363], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "epoch 0 loss = 3546.592558304468;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 1500.018285234769;\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4693, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1754, requires_grad=True)\n",
      "epoch 200 loss = 1054.2645937800407;\n",
      "mode_hat tensor(0.8675, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8577, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1215, requires_grad=True)\n",
      "epoch 300 loss = 1052.960721472899;\n",
      "mode_hat tensor(0.8807, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2594, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0438, requires_grad=True)\n",
      "epoch 400 loss = 1058.127542257309;\n",
      "mode_hat tensor(0.8944, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6162, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1293, requires_grad=True)\n",
      "epoch 500 loss = 1150.2808392445247;\n",
      "mode_hat tensor(0.8977, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9195, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2957, requires_grad=True)\n",
      "epoch 600 loss = 1089.942773759365;\n",
      "mode_hat tensor(0.8967, requires_grad=True)\n",
      "ltscale_hat tensor(-2.2005, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.4361, requires_grad=True)\n",
      "epoch 700 loss = 1067.1965463558834;\n",
      "mode_hat tensor(0.9010, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4729, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5817, requires_grad=True)\n",
      "epoch 800 loss = 1156.6422211527824;\n",
      "mode_hat tensor(0.9241, requires_grad=True)\n",
      "ltscale_hat tensor(-2.6324, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5848, requires_grad=True)\n",
      "Final mean_losses: 1112.3051809507779\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "full_tmode:\n",
      "tensor([-8.8393, -0.3807, -0.0610,  1.3651, -1.7009, 10.3724,  2.6326,  1.4566,\n",
      "         0.0321,  2.4441], grad_fn=<SliceBackward>) (10 elems)\n",
      "globalpsi:\n",
      "tensor([-4.5446, -3.9595], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.5990968346595764\n",
      "ltscale_hat:\n",
      "-2.6617677211761475\n",
      "mode_hat:\n",
      "0.917339026927948\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 100 4.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1132.001669704914;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "complaint 9 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-851.9849],\n",
      "        [-755.9136]], grad_fn=<IndexBackward>) tensor([[879.8873],\n",
      "        [760.7631]], grad_fn=<IndexBackward>) tensor([[-311.2720],\n",
      "        [-270.2596]], grad_fn=<IndexBackward>) tensor([[283.4231],\n",
      "        [265.3979]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0536],\n",
      "        [-0.0122]], grad_fn=<IndexBackward>)\n",
      "yay -20\n",
      "complaint 8 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-857.2175],\n",
      "        [-760.7724]], grad_fn=<IndexBackward>) tensor([[885.2911],\n",
      "        [765.6864]], grad_fn=<IndexBackward>) tensor([[-309.9179],\n",
      "        [-268.8008]], grad_fn=<IndexBackward>) tensor([[281.7830],\n",
      "        [263.8986]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0613],\n",
      "        [ 0.0119]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-858.0817],\n",
      "        [-762.0709]], grad_fn=<IndexBackward>) tensor([[886.3374],\n",
      "        [766.9745]], grad_fn=<IndexBackward>) tensor([[-309.4155],\n",
      "        [-268.3548]], grad_fn=<IndexBackward>) tensor([[281.2792],\n",
      "        [263.4359]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.1194],\n",
      "        [-0.0153]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-859.2029],\n",
      "        [-763.3169]], grad_fn=<IndexBackward>) tensor([[887.5612],\n",
      "        [768.2279]], grad_fn=<IndexBackward>) tensor([[-308.8314],\n",
      "        [-267.8011]], grad_fn=<IndexBackward>) tensor([[280.6683],\n",
      "        [262.8728]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.1953],\n",
      "        [-0.0173]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-864.9923],\n",
      "        [-764.5903]], grad_fn=<IndexBackward>) tensor([[891.9977],\n",
      "        [769.5001]], grad_fn=<IndexBackward>) tensor([[-308.8950],\n",
      "        [-267.3300]], grad_fn=<IndexBackward>) tensor([[280.1410],\n",
      "        [262.3880]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.7487],\n",
      "        [-0.0322]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-864.1534],\n",
      "        [-765.3707]], grad_fn=<IndexBackward>) tensor([[891.7536],\n",
      "        [770.3305]], grad_fn=<IndexBackward>) tensor([[-308.6327],\n",
      "        [-267.2412]], grad_fn=<IndexBackward>) tensor([[280.0466],\n",
      "        [262.3062]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.9859],\n",
      "        [ 0.0249]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-864.4159],\n",
      "        [-766.3758]], grad_fn=<IndexBackward>) tensor([[892.2942],\n",
      "        [771.3371]], grad_fn=<IndexBackward>) tensor([[-308.4243],\n",
      "        [-267.1032]], grad_fn=<IndexBackward>) tensor([[279.8797],\n",
      "        [262.1570]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.6663],\n",
      "        [ 0.0151]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-864.6734],\n",
      "        [-767.4802]], grad_fn=<IndexBackward>) tensor([[892.8527],\n",
      "        [772.4300]], grad_fn=<IndexBackward>) tensor([[-308.1927],\n",
      "        [-266.9513]], grad_fn=<IndexBackward>) tensor([[279.6914],\n",
      "        [261.9882]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.3221],\n",
      "        [-0.0133]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-866.1058],\n",
      "        [-768.3851]], grad_fn=<IndexBackward>) tensor([[894.2368],\n",
      "        [773.4048]], grad_fn=<IndexBackward>) tensor([[-308.0858],\n",
      "        [-266.7589]], grad_fn=<IndexBackward>) tensor([[279.4890],\n",
      "        [261.8065]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.4659],\n",
      "        [ 0.0673]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-865.1445],\n",
      "        [-769.7288]], grad_fn=<IndexBackward>) tensor([[893.9858],\n",
      "        [774.6863]], grad_fn=<IndexBackward>) tensor([[-307.6497],\n",
      "        [-266.5738]], grad_fn=<IndexBackward>) tensor([[279.2440],\n",
      "        [261.5852]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.4357],\n",
      "        [-0.0311]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "#vals = MIDDF35\n",
    "guidenames = [\"amortized_laplace\",\"meanfield\",\"unamortized_laplace\"]#,\"unamortized_laplace\",\"meanfield\",]#,\"MLE\"]#,\"laplace\"\n",
    "if True:\n",
    "\n",
    "    for aniter in range(3):\n",
    "        for trueparams in [ndom_fat_params,ndom_norm_params]:#,tdom_fat_params,tdom_norm_params,]:\n",
    "            for nsamps,nparticles in [(10,1),(10,3),(50,1),(50,3),(100,1),(400,1)]:\n",
    "                    for guidename in guidenames:\n",
    "                        #\n",
    "\n",
    "                        print(aniter,nparticles,guidenames,trueparams)\n",
    "                        result = trainGuide(guidename,nparticles,trueparams,\n",
    "                                            filename=\"testresults/demoT_2.csv\",\n",
    "\n",
    "                                            subsample_N = nsamps)\n",
    "                        print(aniter,nparticles,guidenames)\n",
    "                        for line in range(10):\n",
    "                            print()\n",
    "                    \n",
    "else:\n",
    "    plt.plot([getMLE(.01 + x/100,1.,0,1.) for x in range(500)])\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "MCMC, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "/anaconda3/bin/python\n",
      "Reloading polytopize.\n",
      "addHess tested\n",
      "addHess tested\n",
      "making:\n",
      "1\n",
      "testresults/scenario_N44_mu1.0_sigma2.0_nu-1.0.csv from file\n",
      "44\n",
      "tensor([32.5364, -3.3454,  3.2929, -2.5415])\n",
      "{'hessian': tensor([[1., 0., 1.],\n",
      "        [0., 2., 0.],\n",
      "        [1., 0., 1.]]), 'logPosterior': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"running...\")\n",
    "\n",
    "import sys, os\n",
    "\n",
    "print(sys.executable)\n",
    "#print(sys.path)\n",
    "\n",
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "print(\"making:\")\n",
    "\n",
    "multisiteMod = createScenario(tdom_fat_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-78d7c46ccca3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-78d7c46ccca3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    multisiteMod.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "multisiteMod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27889007329940796\n"
     ]
    }
   ],
   "source": [
    "maxError = torch.max(errors)\n",
    "print(float(maxError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
