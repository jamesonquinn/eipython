{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro\n",
    "\n",
    "Scratch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pyro/__init__.py\n",
      "/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import pyro\n",
    "\n",
    "print(pyro.__file__)\n",
    "\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amortizable hierarchical t model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 ['meanfield', 'amortized_laplace', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 400 1.0\n",
      "epoch 0 loss = 34874.912465294205;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 25529.976437787212;\n",
      "mode_hat tensor(0.4971, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4959, requires_grad=True)\n",
      "epoch 200 loss = 19935.30695546667;\n",
      "mode_hat tensor(0.9619, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9848, requires_grad=True)\n",
      "epoch 300 loss = 16035.43184953928;\n",
      "mode_hat tensor(1.2231, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4771, requires_grad=True)\n",
      "epoch 400 loss = 12474.144851307075;\n",
      "mode_hat tensor(1.3870, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.9618, requires_grad=True)\n",
      "epoch 500 loss = 10204.774855514366;\n",
      "mode_hat tensor(1.4429, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4904, requires_grad=True)\n",
      "ldfraw_hat tensor(2.4414, requires_grad=True)\n",
      "epoch 600 loss = 7412.221295773983;\n",
      "mode_hat tensor(1.4822, requires_grad=True)\n",
      "ltscale_hat tensor(-2.9491, requires_grad=True)\n",
      "ldfraw_hat tensor(2.8817, requires_grad=True)\n",
      "epoch 700 loss = 6572.705907245478;\n",
      "mode_hat tensor(1.4371, requires_grad=True)\n",
      "ltscale_hat tensor(-3.3962, requires_grad=True)\n",
      "ldfraw_hat tensor(3.1904, requires_grad=True)\n",
      "epoch 800 loss = 4953.249788055818;\n",
      "mode_hat tensor(1.3940, requires_grad=True)\n",
      "ltscale_hat tensor(-3.7539, requires_grad=True)\n",
      "ldfraw_hat tensor(2.8892, requires_grad=True)\n",
      "epoch 900 loss = 3984.1529882450895;\n",
      "mode_hat tensor(1.2957, requires_grad=True)\n",
      "ltscale_hat tensor(-3.9257, requires_grad=True)\n",
      "ldfraw_hat tensor(2.4512, requires_grad=True)\n",
      "epoch 1000 loss = 3258.256810625394;\n",
      "mode_hat tensor(1.2359, requires_grad=True)\n",
      "ltscale_hat tensor(-4.0628, requires_grad=True)\n",
      "ldfraw_hat tensor(1.9904, requires_grad=True)\n",
      "epoch 1100 loss = 2855.3031108677387;\n",
      "mode_hat tensor(1.2249, requires_grad=True)\n",
      "ltscale_hat tensor(-4.2245, requires_grad=True)\n",
      "ldfraw_hat tensor(1.5255, requires_grad=True)\n",
      "epoch 1200 loss = 2289.5469400485354;\n",
      "mode_hat tensor(1.2504, requires_grad=True)\n",
      "ltscale_hat tensor(-4.4346, requires_grad=True)\n",
      "ldfraw_hat tensor(1.0641, requires_grad=True)\n",
      "epoch 1300 loss = 2038.114598850409;\n",
      "mode_hat tensor(1.2917, requires_grad=True)\n",
      "ltscale_hat tensor(-4.6871, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6310, requires_grad=True)\n",
      "epoch 1400 loss = 1794.9251446922617;\n",
      "mode_hat tensor(1.2979, requires_grad=True)\n",
      "ltscale_hat tensor(-4.9960, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2226, requires_grad=True)\n",
      "epoch 1500 loss = 1632.1131802996;\n",
      "mode_hat tensor(1.2684, requires_grad=True)\n",
      "ltscale_hat tensor(-5.3506, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1668, requires_grad=True)\n",
      "epoch 1600 loss = 1454.7941406965256;\n",
      "mode_hat tensor(1.2918, requires_grad=True)\n",
      "ltscale_hat tensor(-5.7725, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5108, requires_grad=True)\n",
      "epoch 1700 loss = 1335.5718767046928;\n",
      "mode_hat tensor(1.2966, requires_grad=True)\n",
      "ltscale_hat tensor(-6.1642, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.8577, requires_grad=True)\n",
      "epoch 1800 loss = 1249.1776596307755;\n",
      "mode_hat tensor(1.2932, requires_grad=True)\n",
      "ltscale_hat tensor(-6.5280, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.1155, requires_grad=True)\n",
      "epoch 1900 loss = 1193.4706340630848;\n",
      "mode_hat tensor(1.2791, requires_grad=True)\n",
      "ltscale_hat tensor(-6.9263, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.3291, requires_grad=True)\n",
      "epoch 2000 loss = 1134.972751001517;\n",
      "mode_hat tensor(1.2685, requires_grad=True)\n",
      "ltscale_hat tensor(-7.3275, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5351, requires_grad=True)\n",
      "Final mean_losses: 1160.9104047037583\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "ldfraw_hat:\n",
      "-1.5350701808929443\n",
      "ldfraw_sigma:\n",
      "0.7054116725921631\n",
      "ltscale_hat:\n",
      "-7.327511787414551\n",
      "ltscale_sigma:\n",
      "2.596762180328369\n",
      "mode_hat:\n",
      "1.2685315608978271\n",
      "mode_sigma:\n",
      "0.015509024262428284\n",
      "t_part_hat:\n",
      "tensor([-8.8290, -0.7262, -0.4315,  1.0688, -2.1168,  9.8908,  2.5813,  1.2735,\n",
      "        -0.3158,  2.0276], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([0.9982, 0.4021, 0.2384, 0.1907, 0.3228, 0.4126, 0.2873, 0.7314, 0.3499,\n",
      "        0.5407], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 3 ['meanfield', 'amortized_laplace', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['meanfield', 'amortized_laplace', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 400 1.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 34551.452286054686;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-763.0471]], grad_fn=<IndexBackward>) tensor([[765.4146]], grad_fn=<IndexBackward>) tensor([[-266.0941]], grad_fn=<IndexBackward>) tensor([[263.6610]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0655]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-763.0471]], grad_fn=<IndexBackward>) tensor([[765.4146]], grad_fn=<IndexBackward>) tensor([[-266.0941]], grad_fn=<IndexBackward>) tensor([[263.6610]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0655]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-763.0471]], grad_fn=<IndexBackward>) tensor([[765.4146]], grad_fn=<IndexBackward>) tensor([[-266.0941]], grad_fn=<IndexBackward>) tensor([[263.6610]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0655]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-764.0419]], grad_fn=<IndexBackward>) tensor([[766.4293]], grad_fn=<IndexBackward>) tensor([[-264.3480]], grad_fn=<IndexBackward>) tensor([[261.8986]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0620]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-764.0419]], grad_fn=<IndexBackward>) tensor([[766.4293]], grad_fn=<IndexBackward>) tensor([[-264.3480]], grad_fn=<IndexBackward>) tensor([[261.8986]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0620]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-764.0419]], grad_fn=<IndexBackward>) tensor([[766.4293]], grad_fn=<IndexBackward>) tensor([[-264.3480]], grad_fn=<IndexBackward>) tensor([[261.8986]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0620]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-764.9338]], grad_fn=<IndexBackward>) tensor([[767.3354]], grad_fn=<IndexBackward>) tensor([[-263.5502]], grad_fn=<IndexBackward>) tensor([[261.0931]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0555]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-764.9338]], grad_fn=<IndexBackward>) tensor([[767.3354]], grad_fn=<IndexBackward>) tensor([[-263.5502]], grad_fn=<IndexBackward>) tensor([[261.0931]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0555]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-764.9338]], grad_fn=<IndexBackward>) tensor([[767.3354]], grad_fn=<IndexBackward>) tensor([[-263.5502]], grad_fn=<IndexBackward>) tensor([[261.0931]], grad_fn=<IndexBackward>) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              2.. tensor([[-0.0555]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-764.7858]], grad_fn=<IndexBackward>) tensor([[767.2986]], grad_fn=<IndexBackward>) tensor([[-263.0905]], grad_fn=<IndexBackward>) tensor([[260.6662]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0885]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "complaint -20\n",
      "yay -80\n",
      "complaint -40\n",
      "complaint -60\n",
      "yay -140\n",
      "complaint -80\n",
      "complaint -100\n",
      "complaint -120\n",
      "yay -200\n",
      "complaint -140\n",
      "complaint -160\n",
      "complaint -180\n",
      "yay -260\n",
      "complaint -200\n",
      "epoch 100 loss = 43072.01790089408;\n",
      "mode_hat tensor(0.0952, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3391, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5050, requires_grad=True)\n",
      "complaint -220\n",
      "complaint -240\n",
      "yay -320\n",
      "complaint -260\n",
      "complaint -280\n",
      "yay -380\n",
      "complaint -300\n",
      "complaint -320\n",
      "complaint -340\n",
      "yay -440\n",
      "complaint -360\n",
      "yay -500\n",
      "complaint -380\n",
      "complaint -400\n",
      "yay -560\n",
      "complaint -420\n",
      "complaint -440\n",
      "epoch 200 loss = 31435.770855764546;\n",
      "mode_hat tensor(0.1742, requires_grad=True)\n",
      "ltscale_hat tensor(-0.7750, requires_grad=True)\n",
      "ldfraw_hat tensor(1.0037, requires_grad=True)\n",
      "complaint -460\n",
      "yay -620\n",
      "complaint -480\n",
      "complaint -500\n",
      "complaint -520\n",
      "yay -680\n",
      "complaint -540\n",
      "complaint -560\n",
      "yay -740\n",
      "complaint -580\n",
      "complaint -600\n",
      "complaint -620\n",
      "yay -800\n",
      "complaint -640\n",
      "complaint -660\n",
      "complaint -680\n",
      "yay -860\n",
      "complaint -700\n",
      "epoch 300 loss = 38590.57687781254;\n",
      "mode_hat tensor(0.3539, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0914, requires_grad=True)\n",
      "ldfraw_hat tensor(1.5008, requires_grad=True)\n",
      "complaint -720\n",
      "yay -920\n",
      "complaint -740\n",
      "complaint -760\n",
      "complaint -780\n",
      "yay -980\n",
      "complaint -800\n",
      "complaint -820\n",
      "yay -1040\n",
      "complaint -840\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 35532.44543488821;\n",
      "mode_hat tensor(0.5286, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2220, requires_grad=True)\n",
      "ldfraw_hat tensor(1.8026, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 37311.046296854816;\n",
      "mode_hat tensor(0.6418, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2268, requires_grad=True)\n",
      "ldfraw_hat tensor(1.8591, requires_grad=True)\n",
      "Final mean_losses: 38452.521188596664\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.5946, -4.5914], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "1.8590941429138184\n",
      "ltscale_hat:\n",
      "-1.2268086671829224\n",
      "mode_hat:\n",
      "0.6417721509933472\n",
      "0 3 ['meanfield', 'amortized_laplace', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['meanfield', 'amortized_laplace', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 400 1.0\n",
      "epoch 0 loss = 52137.38791052501;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 36021.43713346124;\n",
      "mode_hat tensor(0.4573, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4302, requires_grad=True)\n",
      "epoch 200 loss = 37723.26652568579;\n",
      "mode_hat tensor(0.8561, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3257, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "#vals = MIDDF35\n",
    "guidenames = [\"meanfield\",\"amortized_laplace\",\"unamortized_laplace\"]#,\"unamortized_laplace\",\"meanfield\",]#,\"MLE\"]#,\"laplace\"\n",
    "if True:\n",
    "\n",
    "    for aniter in range(4):\n",
    "        for trueparams in [ndom_fat_params,ndom_norm_params]:#,tdom_fat_params,tdom_norm_params,]:\n",
    "            for nsamps,nparticles in [(400,3),(100,3),]:#(10,1),(10,3),(50,1),(50,3),(100,1),(400,1)]:#(44,5)]:#]:#,\n",
    "                    for guidename in guidenames:\n",
    "                        #\n",
    "\n",
    "                        print(aniter,nparticles,guidenames,trueparams)\n",
    "                        result = trainGuide(guidename,nparticles,trueparams,\n",
    "                                            filename=\"testresults/demoT_2.csv\",\n",
    "\n",
    "                                            subsample_N = nsamps)\n",
    "                        print(aniter,nparticles,guidenames)\n",
    "                        for line in range(10):\n",
    "                            print()\n",
    "                    \n",
    "else:\n",
    "    plt.plot([getMLE(.01 + x/100,1.,0,1.) for x in range(500)])\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "MCMC, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "/anaconda3/bin/python\n",
      "Reloading polytopize.\n",
      "addHess tested\n",
      "addHess tested\n",
      "making:\n",
      "1\n",
      "testresults/scenario_N44_mu1.0_sigma2.0_nu-1.0.csv from file\n",
      "44\n",
      "tensor([32.5364, -3.3454,  3.2929, -2.5415])\n",
      "{'hessian': tensor([[1., 0., 1.],\n",
      "        [0., 2., 0.],\n",
      "        [1., 0., 1.]]), 'logPosterior': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"running...\")\n",
    "\n",
    "import sys, os\n",
    "\n",
    "print(sys.executable)\n",
    "#print(sys.path)\n",
    "\n",
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "print(\"making:\")\n",
    "\n",
    "multisiteMod = createScenario(tdom_fat_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-78d7c46ccca3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-78d7c46ccca3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    multisiteMod.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "multisiteMod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27889007329940796\n"
     ]
    }
   ],
   "source": [
    "maxError = torch.max(errors)\n",
    "print(float(maxError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
