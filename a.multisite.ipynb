{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro\n",
    "\n",
    "Scratch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pyro/__init__.py\n",
      "/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import pyro\n",
    "\n",
    "print(pyro.__file__)\n",
    "\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amortizable hierarchical t model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 ['amortized_laplace', 'meanfield'] {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "1\n",
      "exception: [Errno 2] No such file or directory: 'testresults/scenario_N400_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv'\n",
      "generating data {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "creating torch.Size([]) torch.Size([400]) torch.Size([]) torch.Size([400]) torch.Size([400])\n",
      "Not none - creating data.\n",
      "testresults/scenario_N400_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv created\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1135.2204928994179;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "yay -80\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-744.4706]], grad_fn=<IndexBackward>) tensor([[762.0895]], grad_fn=<IndexBackward>) tensor([[-266.3276]], grad_fn=<IndexBackward>) tensor([[248.8062]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0975]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-744.4706]], grad_fn=<IndexBackward>) tensor([[762.0895]], grad_fn=<IndexBackward>) tensor([[-266.3276]], grad_fn=<IndexBackward>) tensor([[248.8062]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0975]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-744.4706]], grad_fn=<IndexBackward>) tensor([[762.0895]], grad_fn=<IndexBackward>) tensor([[-266.3276]], grad_fn=<IndexBackward>) tensor([[248.8062]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0975]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-745.8616]], grad_fn=<IndexBackward>) tensor([[763.3664]], grad_fn=<IndexBackward>) tensor([[-266.0930]], grad_fn=<IndexBackward>) tensor([[248.4810]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1071]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-745.8616]], grad_fn=<IndexBackward>) tensor([[763.3664]], grad_fn=<IndexBackward>) tensor([[-266.0930]], grad_fn=<IndexBackward>) tensor([[248.4810]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1071]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-745.8616]], grad_fn=<IndexBackward>) tensor([[763.3664]], grad_fn=<IndexBackward>) tensor([[-266.0930]], grad_fn=<IndexBackward>) tensor([[248.4810]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1071]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-748.1794]], grad_fn=<IndexBackward>) tensor([[765.6571]], grad_fn=<IndexBackward>) tensor([[-265.6101]], grad_fn=<IndexBackward>) tensor([[247.8874]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2450]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-748.1794]], grad_fn=<IndexBackward>) tensor([[765.6571]], grad_fn=<IndexBackward>) tensor([[-265.6101]], grad_fn=<IndexBackward>) tensor([[247.8874]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2450]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-748.1794]], grad_fn=<IndexBackward>) tensor([[765.6571]], grad_fn=<IndexBackward>) tensor([[-265.6101]], grad_fn=<IndexBackward>) tensor([[247.8874]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2450]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153],\n",
      "        [200]]) \n",
      "              1.. tensor([[-748.1963],\n",
      "        [ 664.5724]], grad_fn=<IndexBackward>) tensor([[ 766.0429],\n",
      "        [-667.7022]], grad_fn=<IndexBackward>) tensor([[-265.1719],\n",
      "        [ 236.6145]], grad_fn=<IndexBackward>) tensor([[ 247.5379],\n",
      "        [-233.5001]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.2126],\n",
      "        [-0.0154]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -140\n",
      "complaint -20\n",
      "complaint -40\n",
      "yay -200\n",
      "complaint -60\n",
      "yay -260\n",
      "complaint -80\n",
      "complaint -100\n",
      "epoch 100 loss = 1137.595272620519;\n",
      "mode_hat tensor(0.3970, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4746, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2368, requires_grad=True)\n",
      "complaint -120\n",
      "yay -320\n",
      "complaint -140\n",
      "complaint -160\n",
      "complaint -180\n",
      "yay -380\n",
      "complaint -200\n",
      "complaint -220\n",
      "yay -440\n",
      "complaint -240\n",
      "complaint -260\n",
      "complaint -280\n",
      "yay -500\n",
      "complaint -300\n",
      "complaint -320\n",
      "complaint -340\n",
      "yay -560\n",
      "complaint -360\n",
      "epoch 200 loss = 1082.0065761605897;\n",
      "mode_hat tensor(0.6817, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8677, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2112, requires_grad=True)\n",
      "complaint -380\n",
      "complaint -400\n",
      "yay -620\n",
      "complaint -420\n",
      "complaint -440\n",
      "complaint -460\n",
      "yay -680\n",
      "yay -740\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 1127.8270234266918;\n",
      "mode_hat tensor(0.8076, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2632, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1237, requires_grad=True)\n",
      "complaint -480\n",
      "yay -920\n",
      "complaint -500\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "complaint -520\n",
      "complaint -540\n",
      "complaint -560\n",
      "yay -1160\n",
      "complaint -580\n",
      "complaint -600\n",
      "epoch 400 loss = 1032.1646380027137;\n",
      "mode_hat tensor(0.9700, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6483, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0670, requires_grad=True)\n",
      "complaint -620\n",
      "yay -1220\n",
      "complaint -640\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 1094.1911440292993;\n",
      "mode_hat tensor(1.0438, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9546, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2159, requires_grad=True)\n",
      "yay -1520\n",
      "complaint -660\n",
      "complaint -680\n",
      "yay -1580\n",
      "complaint -700\n",
      "complaint -720\n",
      "complaint -740\n",
      "yay -1640\n",
      "complaint -760\n",
      "complaint -780\n",
      "yay -1700\n",
      "Final mean_losses: 1170.151620989206\n",
      "complaint -800\n",
      "guidename amortized_laplace\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "gammapsi:\n",
      "tensor([-4.5310, -4.3026], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.33712953329086304\n",
      "ltscale_hat:\n",
      "-2.133474111557007\n",
      "mode_hat:\n",
      "1.0257443189620972\n",
      "0 3 ['amortized_laplace', 'meanfield']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield'] {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "epoch 0 loss = 22450.079531391464;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 50397.24194900195;\n",
      "mode_hat tensor(0.2998, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4961, requires_grad=True)\n",
      "epoch 200 loss = 28726.762388984363;\n",
      "mode_hat tensor(0.6175, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9889, requires_grad=True)\n",
      "epoch 300 loss = 9676.810987989109;\n",
      "mode_hat tensor(0.8049, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4735, requires_grad=True)\n",
      "epoch 400 loss = 26126.013014952343;\n",
      "mode_hat tensor(0.8521, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.9584, requires_grad=True)\n",
      "epoch 500 loss = 32864.28129045169;\n",
      "mode_hat tensor(0.9201, requires_grad=True)\n",
      "ltscale_hat tensor(-2.5048, requires_grad=True)\n",
      "ldfraw_hat tensor(2.4371, requires_grad=True)\n",
      "epoch 600 loss = 31282.4628833135;\n",
      "mode_hat tensor(0.9477, requires_grad=True)\n",
      "ltscale_hat tensor(-3.0006, requires_grad=True)\n",
      "ldfraw_hat tensor(2.8949, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 700 loss = 22692.42860710621;\n",
      "mode_hat tensor(0.9559, requires_grad=True)\n",
      "ltscale_hat tensor(-3.4818, requires_grad=True)\n",
      "ldfraw_hat tensor(3.3320, requires_grad=True)\n",
      "epoch 800 loss = 39173.169117202364;\n",
      "mode_hat tensor(0.9965, requires_grad=True)\n",
      "ltscale_hat tensor(-3.9587, requires_grad=True)\n",
      "ldfraw_hat tensor(3.7272, requires_grad=True)\n",
      "Final mean_losses: 20204.920087120696\n",
      "guidename meanfield\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "ldfraw_hat:\n",
      "3.727245330810547\n",
      "ldfraw_sigma:\n",
      "0.5796933770179749\n",
      "ltscale_hat:\n",
      "-3.9586799144744873\n",
      "ltscale_sigma:\n",
      "0.4400278627872467\n",
      "mode_hat:\n",
      "0.9964814186096191\n",
      "mode_sigma:\n",
      "0.11552917212247849\n",
      "t_part_hat:\n",
      "tensor([-1.1708, -0.3345,  0.0504,  1.0988, -1.1858,  1.1496,  1.2783,  1.2901,\n",
      "         0.1572,  1.1138], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([0.8768, 0.6785, 0.4044, 0.5251, 0.5298, 1.0130, 0.7741, 0.5386, 0.6346,\n",
      "        0.9693], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 3 ['amortized_laplace', 'meanfield']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield'] {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1075.946047604084;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 9 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-846.8553],\n",
      "        [-734.7325]], grad_fn=<IndexBackward>) tensor([[879.4849],\n",
      "        [749.3345]], grad_fn=<IndexBackward>) tensor([[-311.8281],\n",
      "        [-276.0099]], grad_fn=<IndexBackward>) tensor([[279.1110],\n",
      "        [261.4233]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0875],\n",
      "        [ 0.0154]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-847.7371],\n",
      "        [-735.9593]], grad_fn=<IndexBackward>) tensor([[880.5432],\n",
      "        [750.5759]], grad_fn=<IndexBackward>) tensor([[-311.4385],\n",
      "        [-275.6673]], grad_fn=<IndexBackward>) tensor([[278.6916],\n",
      "        [261.0394]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0592],\n",
      "        [-0.0112]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-848.8533]], grad_fn=<IndexBackward>) tensor([[881.7642]], grad_fn=<IndexBackward>) tensor([[-311.1102]], grad_fn=<IndexBackward>) tensor([[278.3026]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1033]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-852.8247]], grad_fn=<IndexBackward>) tensor([[885.4116]], grad_fn=<IndexBackward>) tensor([[-310.6887]], grad_fn=<IndexBackward>) tensor([[277.5851]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.5167]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-852.7804]], grad_fn=<IndexBackward>) tensor([[885.8306]], grad_fn=<IndexBackward>) tensor([[-310.1176]], grad_fn=<IndexBackward>) tensor([[277.1236]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0562]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-858.6181]], grad_fn=<IndexBackward>) tensor([[890.3203]], grad_fn=<IndexBackward>) tensor([[-310.2480]], grad_fn=<IndexBackward>) tensor([[276.6488]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.8969]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-857.4801]], grad_fn=<IndexBackward>) tensor([[889.9847]], grad_fn=<IndexBackward>) tensor([[-309.6150]], grad_fn=<IndexBackward>) tensor([[276.2430]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.8674]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-858.7153],\n",
      "        [-744.2138]], grad_fn=<IndexBackward>) tensor([[891.2909],\n",
      "        [759.0520]], grad_fn=<IndexBackward>) tensor([[-309.3015],\n",
      "        [-273.2794]], grad_fn=<IndexBackward>) tensor([[275.8653],\n",
      "        [258.4542]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.8606],\n",
      "        [ 0.0130]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-857.5893]], grad_fn=<IndexBackward>) tensor([[890.9629]], grad_fn=<IndexBackward>) tensor([[-308.6884]], grad_fn=<IndexBackward>) tensor([[275.4705]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1557]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-857.6663],\n",
      "        [-746.6614]], grad_fn=<IndexBackward>) tensor([[891.4676],\n",
      "        [761.5385]], grad_fn=<IndexBackward>) tensor([[-308.2494],\n",
      "        [-272.6554]], grad_fn=<IndexBackward>) tensor([[275.1010],\n",
      "        [257.7556]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.6529],\n",
      "        [-0.0227]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "complaint -20\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1087.844681918621;\n",
      "mode_hat tensor(0.5035, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4499, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2024, requires_grad=True)\n",
      "complaint -40\n",
      "yay -320\n",
      "complaint -60\n",
      "yay -380\n",
      "complaint -80\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -100\n",
      "yay -560\n",
      "complaint -120\n",
      "epoch 200 loss = 1044.4799829125404;\n",
      "mode_hat tensor(0.8936, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8879, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1836, requires_grad=True)\n",
      "yay -620\n",
      "yay -680\n",
      "complaint -140\n",
      "yay -740\n",
      "yay -800\n",
      "complaint -160\n",
      "yay -860\n",
      "epoch 300 loss = 1040.7800615429878;\n",
      "mode_hat tensor(1.0601, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2807, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0309, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "complaint -180\n",
      "yay -1040\n",
      "complaint -200\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 1087.9860866069794;\n",
      "mode_hat tensor(1.1458, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6662, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1186, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "complaint -220\n",
      "yay -1340\n",
      "complaint -240\n",
      "yay -1400\n",
      "complaint -260\n",
      "yay -1460\n",
      "epoch 500 loss = 1068.7884035110474;\n",
      "mode_hat tensor(1.1154, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9865, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2974, requires_grad=True)\n",
      "Final mean_losses: 1081.9741676561396\n",
      "file exists: testresults/fit_amortized_laplace_0_N400_S50_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv\n",
      "guidename amortized_laplace\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "gammapsi:\n",
      "tensor([-4.5810, -4.4923], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.2974480986595154\n",
      "ltscale_hat:\n",
      "-1.9865387678146362\n",
      "mode_hat:\n",
      "1.1153576374053955\n",
      "0 1 ['amortized_laplace', 'meanfield']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield'] {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "epoch 0 loss = 23835.85165309906;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 54330.549160182476;\n",
      "mode_hat tensor(0.2531, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5048, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4864, requires_grad=True)\n",
      "epoch 200 loss = 12401.686730861664;\n",
      "mode_hat tensor(0.4818, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0048, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9671, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 300 loss = 33921.58119571209;\n",
      "mode_hat tensor(0.6666, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5023, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4317, requires_grad=True)\n",
      "epoch 400 loss = 107447.16083872318;\n",
      "mode_hat tensor(0.8517, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9964, requires_grad=True)\n",
      "ldfraw_hat tensor(1.8886, requires_grad=True)\n",
      "epoch 500 loss = 21940.903351068497;\n",
      "mode_hat tensor(0.9326, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4834, requires_grad=True)\n",
      "ldfraw_hat tensor(2.3172, requires_grad=True)\n",
      "Final mean_losses: 29608.160448305054\n",
      "file exists: testresults/fit_meanfield_0_N400_S50_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv\n",
      "guidename meanfield\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "ldfraw_hat:\n",
      "2.3172106742858887\n",
      "ldfraw_sigma:\n",
      "0.9920160174369812\n",
      "ltscale_hat:\n",
      "-2.483428716659546\n",
      "ltscale_sigma:\n",
      "0.9187871813774109\n",
      "mode_hat:\n",
      "0.932604193687439\n",
      "mode_sigma:\n",
      "0.4432777166366577\n",
      "t_part_hat:\n",
      "tensor([-0.8441, -0.1050,  0.0903,  0.4957, -0.5075,  0.7193,  0.7258,  0.5772,\n",
      "         0.0715,  0.6983], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([1.0179, 0.7599, 0.8545, 1.0878, 0.7736, 0.9675, 1.0093, 1.0891, 0.7166,\n",
      "        0.7689], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 1 ['amortized_laplace', 'meanfield']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield'] {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-970.9653],\n",
      "        [ 949.6343],\n",
      "        [-859.0482]], grad_fn=<IndexBackward>) tensor([[ 979.8196],\n",
      "        [-983.6089],\n",
      "        [ 881.1290]], grad_fn=<IndexBackward>) tensor([[-303.0065],\n",
      "        [ 333.5544],\n",
      "        [-309.0152]], grad_fn=<IndexBackward>) tensor([[ 294.1312],\n",
      "        [-299.6739],\n",
      "        [ 287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0210],\n",
      "        [-0.0941],\n",
      "        [ 0.0721]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-970.9653],\n",
      "        [ 949.6343],\n",
      "        [-859.0482]], grad_fn=<IndexBackward>) tensor([[ 979.8196],\n",
      "        [-983.6089],\n",
      "        [ 881.1290]], grad_fn=<IndexBackward>) tensor([[-303.0065],\n",
      "        [ 333.5544],\n",
      "        [-309.0152]], grad_fn=<IndexBackward>) tensor([[ 294.1312],\n",
      "        [-299.6739],\n",
      "        [ 287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0210],\n",
      "        [-0.0941],\n",
      "        [ 0.0721]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-970.9653],\n",
      "        [ 949.6343],\n",
      "        [-859.0482]], grad_fn=<IndexBackward>) tensor([[ 979.8196],\n",
      "        [-983.6089],\n",
      "        [ 881.1290]], grad_fn=<IndexBackward>) tensor([[-303.0065],\n",
      "        [ 333.5544],\n",
      "        [-309.0152]], grad_fn=<IndexBackward>) tensor([[ 294.1312],\n",
      "        [-299.6739],\n",
      "        [ 287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0210],\n",
      "        [-0.0941],\n",
      "        [ 0.0721]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1012.816136399905;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 6 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 947.6814],\n",
      "        [-860.2469]], grad_fn=<IndexBackward>) tensor([[-981.7775],\n",
      "        [ 882.4008]], grad_fn=<IndexBackward>) tensor([[ 332.7938],\n",
      "        [-308.6288]], grad_fn=<IndexBackward>) tensor([[-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2271],\n",
      "        [ 0.1017]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 947.6814],\n",
      "        [-860.2469]], grad_fn=<IndexBackward>) tensor([[-981.7775],\n",
      "        [ 882.4008]], grad_fn=<IndexBackward>) tensor([[ 332.7938],\n",
      "        [-308.6288]], grad_fn=<IndexBackward>) tensor([[-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2271],\n",
      "        [ 0.1017]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 947.6814],\n",
      "        [-860.2469]], grad_fn=<IndexBackward>) tensor([[-981.7775],\n",
      "        [ 882.4008]], grad_fn=<IndexBackward>) tensor([[ 332.7938],\n",
      "        [-308.6288]], grad_fn=<IndexBackward>) tensor([[-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2271],\n",
      "        [ 0.1017]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "complaint 3 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-972.6442],\n",
      "        [ 947.7338],\n",
      "        [-860.7593]], grad_fn=<IndexBackward>) tensor([[ 981.5540],\n",
      "        [-981.7012],\n",
      "        [ 882.8566]], grad_fn=<IndexBackward>) tensor([[-301.3921],\n",
      "        [ 331.6607],\n",
      "        [-307.5595]], grad_fn=<IndexBackward>) tensor([[ 292.4709],\n",
      "        [-297.6182],\n",
      "        [ 285.3909]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0114],\n",
      "        [ 0.0750],\n",
      "        [-0.0714]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-972.6442],\n",
      "        [ 947.7338],\n",
      "        [-860.7593]], grad_fn=<IndexBackward>) tensor([[ 981.5540],\n",
      "        [-981.7012],\n",
      "        [ 882.8566]], grad_fn=<IndexBackward>) tensor([[-301.3921],\n",
      "        [ 331.6607],\n",
      "        [-307.5595]], grad_fn=<IndexBackward>) tensor([[ 292.4709],\n",
      "        [-297.6182],\n",
      "        [ 285.3909]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0114],\n",
      "        [ 0.0750],\n",
      "        [-0.0714]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-972.6442],\n",
      "        [ 947.7338],\n",
      "        [-860.7593]], grad_fn=<IndexBackward>) tensor([[ 981.5540],\n",
      "        [-981.7012],\n",
      "        [ 882.8566]], grad_fn=<IndexBackward>) tensor([[-301.3921],\n",
      "        [ 331.6607],\n",
      "        [-307.5595]], grad_fn=<IndexBackward>) tensor([[ 292.4709],\n",
      "        [-297.6182],\n",
      "        [ 285.3909]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0114],\n",
      "        [ 0.0750],\n",
      "        [-0.0714]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 946.1571],\n",
      "        [-861.0986]], grad_fn=<IndexBackward>) tensor([[-980.3403],\n",
      "        [ 883.3439]], grad_fn=<IndexBackward>) tensor([[ 330.9109],\n",
      "        [-307.0380]], grad_fn=<IndexBackward>) tensor([[-296.9036],\n",
      "        [ 284.8669]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1759],\n",
      "        [ 0.0742]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -20\n",
      "complaint -20\n",
      "complaint -40\n",
      "complaint -60\n",
      "yay -80\n",
      "yay -140\n",
      "complaint -80\n",
      "yay -200\n",
      "complaint -100\n",
      "complaint -120\n",
      "yay -260\n",
      "complaint -140\n",
      "epoch 100 loss = 1046.4973004261653;\n",
      "mode_hat tensor(0.3343, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3437, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2388, requires_grad=True)\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -160\n",
      "yay -560\n",
      "complaint -180\n",
      "epoch 200 loss = 1020.8878565629325;\n",
      "mode_hat tensor(0.6050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5933, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3157, requires_grad=True)\n",
      "complaint -200\n",
      "complaint -220\n",
      "yay -620\n",
      "complaint -240\n",
      "complaint -260\n",
      "complaint -280\n",
      "yay -680\n",
      "complaint -300\n",
      "complaint -320\n",
      "complaint -340\n",
      "yay -740\n",
      "complaint -360\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 958.4965374072392;\n",
      "mode_hat tensor(0.7788, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9499, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5433, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 955.4245822032294;\n",
      "mode_hat tensor(0.9159, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2362, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldfraw_hat tensor(0.5937, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 1646.7501264214516;\n",
      "mode_hat tensor(1.0138, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4861, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6323, requires_grad=True)\n",
      "Final mean_losses: 1213.0122562798583\n",
      "guidename amortized_laplace\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "gammapsi:\n",
      "tensor([-3.9399, -3.8382], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.6322805881500244\n",
      "ltscale_hat:\n",
      "-1.4861204624176025\n",
      "mode_hat:\n",
      "1.0137560367584229\n",
      "0 3 ['amortized_laplace', 'meanfield']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield'] {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "epoch 0 loss = 8550.63738489151;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 21850.472622573376;\n",
      "mode_hat tensor(0.2547, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4971, requires_grad=True)\n",
      "epoch 200 loss = 12068.059099316597;\n",
      "mode_hat tensor(0.4035, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9835, requires_grad=True)\n",
      "epoch 300 loss = 18367.578557431698;\n",
      "mode_hat tensor(0.6058, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4707, requires_grad=True)\n",
      "epoch 400 loss = 3031.909502406915;\n",
      "mode_hat tensor(0.7223, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(1.9558, requires_grad=True)\n",
      "epoch 500 loss = 21067.11321012179;\n",
      "mode_hat tensor(0.8743, requires_grad=True)\n",
      "ltscale_hat tensor(-2.5049, requires_grad=True)\n",
      "ldfraw_hat tensor(2.4304, requires_grad=True)\n",
      "Final mean_losses: 17416.216045239955\n",
      "guidename meanfield\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "ldfraw_hat:\n",
      "2.4304444789886475\n",
      "ldfraw_sigma:\n",
      "0.8106411099433899\n",
      "ltscale_hat:\n",
      "-2.5049333572387695\n",
      "ltscale_sigma:\n",
      "0.35438254475593567\n",
      "mode_hat:\n",
      "0.8742986917495728\n",
      "mode_sigma:\n",
      "0.282319575548172\n",
      "t_part_hat:\n",
      "tensor([-0.2164,  0.0810,  0.1497,  0.1641, -0.1828,  0.2327,  0.2396,  0.2919,\n",
      "         0.1948,  0.2414], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([0.9248, 0.9247, 0.8550, 0.9062, 0.9027, 1.0305, 1.0533, 0.8973, 0.8137,\n",
      "        0.8495], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 3 ['amortized_laplace', 'meanfield']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield'] {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1081.604932665825;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-767.3847]], grad_fn=<IndexBackward>) tensor([[773.9207]], grad_fn=<IndexBackward>) tensor([[-265.0167]], grad_fn=<IndexBackward>) tensor([[258.4247]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0561]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-769.7353]], grad_fn=<IndexBackward>) tensor([[776.2953]], grad_fn=<IndexBackward>) tensor([[-264.3648]], grad_fn=<IndexBackward>) tensor([[257.7444]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0604]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-769.8675]], grad_fn=<IndexBackward>) tensor([[776.7910]], grad_fn=<IndexBackward>) tensor([[-263.9439]], grad_fn=<IndexBackward>) tensor([[257.4306]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.4102]], grad_fn=<IndexBackward>)\n",
      "yay -80\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-770.5219]], grad_fn=<IndexBackward>) tensor([[777.6425]], grad_fn=<IndexBackward>) tensor([[-263.9212]], grad_fn=<IndexBackward>) tensor([[257.4595]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.6589]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-776.6080]], grad_fn=<IndexBackward>) tensor([[782.1487]], grad_fn=<IndexBackward>) tensor([[-264.4530]], grad_fn=<IndexBackward>) tensor([[257.4247]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.4875]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-775.6580]], grad_fn=<IndexBackward>) tensor([[781.7682]], grad_fn=<IndexBackward>) tensor([[-264.1135]], grad_fn=<IndexBackward>) tensor([[257.2668]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.7364]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-776.8838]], grad_fn=<IndexBackward>) tensor([[782.8847]], grad_fn=<IndexBackward>) tensor([[-263.9701]], grad_fn=<IndexBackward>) tensor([[257.0691]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.9001]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153],\n",
      "        [200]]) \n",
      "              1.. tensor([[-776.6225],\n",
      "        [ 674.4162]], grad_fn=<IndexBackward>) tensor([[ 782.8552],\n",
      "        [-676.5274]], grad_fn=<IndexBackward>) tensor([[-263.5902],\n",
      "        [ 245.8916]], grad_fn=<IndexBackward>) tensor([[ 256.7554],\n",
      "        [-243.7688]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.6021],\n",
      "        [ 0.0116]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-776.4799]], grad_fn=<IndexBackward>) tensor([[782.9628]], grad_fn=<IndexBackward>) tensor([[-263.2278]], grad_fn=<IndexBackward>) tensor([[256.4636]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2813]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-775.3286]], grad_fn=<IndexBackward>) tensor([[782.4374]], grad_fn=<IndexBackward>) tensor([[-262.7473]], grad_fn=<IndexBackward>) tensor([[256.1816]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.5432]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1158.46069252491;\n",
      "mode_hat tensor(0.3446, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3208, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2269, requires_grad=True)\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "complaint -20\n",
      "yay -500\n",
      "complaint -40\n",
      "yay -560\n",
      "epoch 200 loss = 890.0678281784058;\n",
      "mode_hat tensor(0.6406, requires_grad=True)\n",
      "ltscale_hat tensor(-0.6697, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4213, requires_grad=True)\n",
      "complaint -60\n",
      "yay -620\n",
      "yay -680\n",
      "yay -740\n",
      "yay -800\n",
      "complaint -80\n",
      "yay -860\n",
      "epoch 300 loss = 1112.3317114114761;\n",
      "mode_hat tensor(0.8549, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0254, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5547, requires_grad=True)\n",
      "complaint -100\n",
      "yay -920\n",
      "complaint -120\n",
      "yay -980\n",
      "complaint -140\n",
      "yay -1040\n",
      "yay -1100\n",
      "complaint -160\n",
      "yay -1160\n",
      "complaint -180\n",
      "epoch 400 loss = 891.6801894903183;\n",
      "mode_hat tensor(1.0060, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3092, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6620, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -200\n",
      "yay -1280\n",
      "complaint -220\n",
      "yay -1340\n",
      "complaint -240\n",
      "yay -1400\n",
      "complaint -260\n",
      "yay -1460\n",
      "epoch 500 loss = 1050.0438885688782;\n",
      "mode_hat tensor(1.0676, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5546, requires_grad=True)\n",
      "ldfraw_hat tensor(0.7384, requires_grad=True)\n",
      "Final mean_losses: 1122.877824307751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists: testresults/fit_amortized_laplace_0_N400_S10_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv\n",
      "guidename amortized_laplace\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "gammapsi:\n",
      "tensor([-4.4607, -4.3359], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.7383781671524048\n",
      "ltscale_hat:\n",
      "-1.554552435874939\n",
      "mode_hat:\n",
      "1.0676418542861938\n",
      "0 1 ['amortized_laplace', 'meanfield']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield'] {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "epoch 0 loss = 21245.505483150482;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 24019.770265340805;\n",
      "mode_hat tensor(0.2787, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5049, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4885, requires_grad=True)\n",
      "epoch 200 loss = 20129.166100800037;\n",
      "mode_hat tensor(0.4814, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0043, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9643, requires_grad=True)\n",
      "epoch 300 loss = 4998.956131517887;\n",
      "mode_hat tensor(0.6682, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5015, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4324, requires_grad=True)\n",
      "epoch 400 loss = 36366.62750971317;\n",
      "mode_hat tensor(0.7632, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9985, requires_grad=True)\n",
      "ldfraw_hat tensor(1.8918, requires_grad=True)\n",
      "epoch 500 loss = 22105.393146157265;\n",
      "mode_hat tensor(0.8682, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4854, requires_grad=True)\n",
      "ldfraw_hat tensor(2.3396, requires_grad=True)\n",
      "Final mean_losses: 36108.3655125998\n",
      "file exists: testresults/fit_meanfield_0_N400_S10_mu1.0_sigma-2.3025851249694824_nu2.5+exp-0.6931471824645996.csv\n",
      "guidename meanfield\n",
      "trueparams {'modal_effect': tensor(1.), 'df': tensor(-0.6931), 't_scale': tensor(-2.3026)}\n",
      "ldfraw_hat:\n",
      "2.339611530303955\n",
      "ldfraw_sigma:\n",
      "0.9626396298408508\n",
      "ltscale_hat:\n",
      "-2.4853594303131104\n",
      "ltscale_sigma:\n",
      "0.9370231628417969\n",
      "mode_hat:\n",
      "0.868190348148346\n",
      "mode_sigma:\n",
      "0.48875412344932556\n",
      "t_part_hat:\n",
      "tensor([-0.2139, -0.0029,  0.0814,  0.2656, -0.1638,  0.1736,  0.2208,  0.2330,\n",
      "         0.0496,  0.1819], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([0.9336, 0.9313, 1.0090, 0.9362, 0.9403, 0.9942, 0.9619, 0.9881, 0.8766,\n",
      "        1.0433], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 1 ['amortized_laplace', 'meanfield']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "#vals = MIDDF35\n",
    "guidenames = [\"unamortized_laplace\"]#,\"amortized_laplace\",\"meanfield\",\"unamortized_laplace\",\"meanfield\",]#,\"MLE\"]#,\"laplace\"\n",
    "if True:\n",
    "\n",
    "  for nsamps in [50,10]:#50]:\n",
    "    for aniter in range(1):\n",
    "        for nparticles in [3,1]:#,5]:\n",
    "            for trueparams in [ndom_fat_params,]:#ndom_norm_params,tdom_fat_params,tdom_norm_params,]:\n",
    "                for guidename in guidenames:\n",
    "                    #\n",
    "\n",
    "                    print(aniter,nparticles,guidenames,trueparams)\n",
    "                    result = trainGuide(guidename,nparticles,trueparams,\n",
    "                                        filename=\"testresults/demoT_2.csv\",\n",
    "                                      \n",
    "                                        subsample_N = nsamps)\n",
    "                    print(aniter,nparticles,guidenames)\n",
    "                    for line in range(10):\n",
    "                        print()\n",
    "                    \n",
    "else:\n",
    "    plt.plot([getMLE(.01 + x/100,1.,0,1.) for x in range(500)])\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "MCMC, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "/anaconda3/bin/python\n",
      "Reloading polytopize.\n",
      "addHess tested\n",
      "addHess tested\n",
      "making:\n",
      "1\n",
      "testresults/scenario_N44_mu1.0_sigma2.0_nu-1.0.csv from file\n",
      "44\n",
      "tensor([32.5364, -3.3454,  3.2929, -2.5415])\n",
      "{'hessian': tensor([[1., 0., 1.],\n",
      "        [0., 2., 0.],\n",
      "        [1., 0., 1.]]), 'logPosterior': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"running...\")\n",
    "\n",
    "import sys, os\n",
    "\n",
    "print(sys.executable)\n",
    "#print(sys.path)\n",
    "\n",
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "print(\"making:\")\n",
    "\n",
    "multisiteMod = createScenario(tdom_fat_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-78d7c46ccca3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-78d7c46ccca3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    multisiteMod.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "multisiteMod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27889007329940796\n"
     ]
    }
   ],
   "source": [
    "maxError = torch.max(errors)\n",
    "print(float(maxError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
