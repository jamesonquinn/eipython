{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro\n",
    "\n",
    "Scratch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pyro/__init__.py\n",
      "/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import pyro\n",
    "\n",
    "print(pyro.__file__)\n",
    "\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amortizable hierarchical t model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1114.9764868021011;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "yay -80\n",
      "yay -140\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-771.5305]], grad_fn=<IndexBackward>) tensor([[789.4745]], grad_fn=<IndexBackward>) tensor([[-276.8895]], grad_fn=<IndexBackward>) tensor([[259.0411]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0957]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-776.3513]], grad_fn=<IndexBackward>) tensor([[794.2509]], grad_fn=<IndexBackward>) tensor([[-276.3059]], grad_fn=<IndexBackward>) tensor([[258.2705]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1358]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-777.1024]], grad_fn=<IndexBackward>) tensor([[795.0465]], grad_fn=<IndexBackward>) tensor([[-276.0822]], grad_fn=<IndexBackward>) tensor([[258.0190]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1191]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-778.0443]], grad_fn=<IndexBackward>) tensor([[796.0026]], grad_fn=<IndexBackward>) tensor([[-275.8445]], grad_fn=<IndexBackward>) tensor([[257.7404]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1458]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-779.5526]], grad_fn=<IndexBackward>) tensor([[797.3688]], grad_fn=<IndexBackward>) tensor([[-275.7402]], grad_fn=<IndexBackward>) tensor([[257.5491]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.3748]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-784.7009]], grad_fn=<IndexBackward>) tensor([[801.2920]], grad_fn=<IndexBackward>) tensor([[-275.5828]], grad_fn=<IndexBackward>) tensor([[256.8805]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-2.1112]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[ 40],\n",
      "        [153]]) \n",
      "              1.. tensor([[ 906.0317],\n",
      "        [-783.5305]], grad_fn=<IndexBackward>) tensor([[-925.7383],\n",
      "        [ 800.7679]], grad_fn=<IndexBackward>) tensor([[ 288.9338],\n",
      "        [-275.0769]], grad_fn=<IndexBackward>) tensor([[-269.2373],\n",
      "        [ 256.5736]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0102],\n",
      "        [-1.2658]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-783.4427]], grad_fn=<IndexBackward>) tensor([[800.9654]], grad_fn=<IndexBackward>) tensor([[-274.7126]], grad_fn=<IndexBackward>) tensor([[256.2733]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.9167]], grad_fn=<IndexBackward>)\n",
      "yay -200\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-783.4524]], grad_fn=<IndexBackward>) tensor([[801.2668]], grad_fn=<IndexBackward>) tensor([[-274.3789]], grad_fn=<IndexBackward>) tensor([[256.0011]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.5634]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-784.3168]], grad_fn=<IndexBackward>) tensor([[802.0229]], grad_fn=<IndexBackward>) tensor([[-274.1561]], grad_fn=<IndexBackward>) tensor([[255.6972]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.7529]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -260\n",
      "epoch 100 loss = 1146.9118095636368;\n",
      "mode_hat tensor(0.4247, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3410, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2819, requires_grad=True)\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -20\n",
      "yay -560\n",
      "epoch 200 loss = 902.3052217960358;\n",
      "mode_hat tensor(0.7667, requires_grad=True)\n",
      "ltscale_hat tensor(-0.6244, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4233, requires_grad=True)\n",
      "complaint -40\n",
      "yay -620\n",
      "yay -680\n",
      "yay -740\n",
      "yay -800\n",
      "yay -860\n",
      "complaint -60\n",
      "epoch 300 loss = 1029.0199601650238;\n",
      "mode_hat tensor(0.9636, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9097, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4931, requires_grad=True)\n",
      "yay -920\n",
      "complaint -80\n",
      "yay -980\n",
      "complaint -100\n",
      "yay -1040\n",
      "complaint -120\n",
      "yay -1100\n",
      "complaint -140\n",
      "yay -1160\n",
      "epoch 400 loss = 911.5129956007004;\n",
      "mode_hat tensor(1.0653, requires_grad=True)\n",
      "ltscale_hat tensor(-1.1931, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4677, requires_grad=True)\n",
      "complaint -160\n",
      "yay -1220\n",
      "yay -1280\n",
      "complaint -180\n",
      "yay -1340\n",
      "yay -1400\n",
      "complaint -200\n",
      "yay -1460\n",
      "epoch 500 loss = 1082.9528213739395;\n",
      "mode_hat tensor(1.0653, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5150, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6006, requires_grad=True)\n",
      "Final mean_losses: 1126.488160642747\n",
      "file exists: testresults/fit_amortized_laplace_0_parts1_N400_S10_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.4171, -4.2824], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.6005856394767761\n",
      "ltscale_hat:\n",
      "-1.5150495767593384\n",
      "mode_hat:\n",
      "1.0652893781661987\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "epoch 0 loss = 37581.71555542946;\n",
      "mode_hat tensor(-0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 8995.700284957886;\n",
      "mode_hat tensor(0.2358, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5040, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4884, requires_grad=True)\n",
      "epoch 200 loss = 28029.957255125046;\n",
      "mode_hat tensor(0.4271, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0040, requires_grad=True)\n",
      "ldfraw_hat tensor(0.9680, requires_grad=True)\n",
      "epoch 300 loss = 10403.773786783218;\n",
      "mode_hat tensor(0.5949, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4993, requires_grad=True)\n",
      "ldfraw_hat tensor(1.4347, requires_grad=True)\n",
      "epoch 400 loss = 6871.094324290752;\n",
      "mode_hat tensor(0.7665, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9904, requires_grad=True)\n",
      "ldfraw_hat tensor(1.9170, requires_grad=True)\n",
      "epoch 500 loss = 30553.035027861595;\n",
      "mode_hat tensor(0.8413, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4782, requires_grad=True)\n",
      "ldfraw_hat tensor(2.3658, requires_grad=True)\n",
      "epoch 600 loss = 16239.887854099274;\n",
      "mode_hat tensor(0.7939, requires_grad=True)\n",
      "ltscale_hat tensor(-2.9496, requires_grad=True)\n",
      "ldfraw_hat tensor(2.7681, requires_grad=True)\n",
      "Final mean_losses: 22385.347178032338\n",
      "file exists: testresults/fit_meanfield_0_parts1_N400_S10_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename meanfield\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "ldfraw_hat:\n",
      "2.9899110794067383\n",
      "ldfraw_sigma:\n",
      "0.8052737712860107\n",
      "ltscale_hat:\n",
      "-3.2288105487823486\n",
      "ltscale_sigma:\n",
      "0.8304917216300964\n",
      "mode_hat:\n",
      "0.8919063210487366\n",
      "mode_sigma:\n",
      "0.44034138321876526\n",
      "t_part_hat:\n",
      "tensor([-0.2679, -0.0297,  0.0994,  0.2053, -0.2138,  0.2332,  0.2859,  0.2633,\n",
      "         0.0520,  0.2362], grad_fn=<SliceBackward>) (10 elems)\n",
      "t_part_sigma:\n",
      "tensor([0.9562, 0.9287, 1.0097, 0.8918, 1.0888, 1.0035, 1.0283, 0.9693, 0.9522,\n",
      "        0.9910], grad_fn=<SliceBackward>) (10 elems)\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "epoch 0 loss = 2072.3795664310455;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "epoch 100 loss = 1703.8440679311752;\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3304, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2016, requires_grad=True)\n",
      "epoch 200 loss = 1163.0583981275558;\n",
      "mode_hat tensor(1.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5834, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3087, requires_grad=True)\n",
      "epoch 300 loss = 1039.4693734645844;\n",
      "mode_hat tensor(1.0183, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8257, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3863, requires_grad=True)\n",
      "epoch 400 loss = 1036.214755654335;\n",
      "mode_hat tensor(1.0276, requires_grad=True)\n",
      "ltscale_hat tensor(-1.1008, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4480, requires_grad=True)\n",
      "epoch 500 loss = 1048.2735177278519;\n",
      "mode_hat tensor(1.0294, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3527, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4593, requires_grad=True)\n",
      "epoch 600 loss = 1197.7754633426666;\n",
      "mode_hat tensor(1.0384, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6063, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5677, requires_grad=True)\n",
      "epoch 700 loss = 958.1399931907654;\n",
      "mode_hat tensor(1.0193, requires_grad=True)\n",
      "ltscale_hat tensor(-1.8339, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5576, requires_grad=True)\n",
      "epoch 800 loss = 884.1477173566818;\n",
      "mode_hat tensor(1.0371, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0176, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5453, requires_grad=True)\n",
      "epoch 900 loss = 1160.8390724658966;\n",
      "mode_hat tensor(1.0315, requires_grad=True)\n",
      "ltscale_hat tensor(-2.2328, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5564, requires_grad=True)\n",
      "Final mean_losses: 1110.5487087218098\n",
      "file exists: testresults/fit_unamortized_laplace_0_parts1_N400_S10_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename unamortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "full_tmode:\n",
      "tensor([-8.7556, -0.4927, -0.2364,  1.1589, -1.7415, 10.4468,  2.7066,  1.6360,\n",
      "        -0.0335,  2.3776], grad_fn=<SliceBackward>) (10 elems)\n",
      "globalpsi:\n",
      "tensor([-4.1997, -3.9471], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.46891599893569946\n",
      "ltscale_hat:\n",
      "-2.2755119800567627\n",
      "mode_hat:\n",
      "1.0230517387390137\n",
      "0 1 ['amortized_laplace', 'meanfield', 'unamortized_laplace']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['amortized_laplace', 'meanfield', 'unamortized_laplace'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename amortized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-966.8629],\n",
      "        [-867.0729]], grad_fn=<IndexBackward>) tensor([[977.0577],\n",
      "        [886.6078]], grad_fn=<IndexBackward>) tensor([[-304.3145],\n",
      "        [-306.2899]], grad_fn=<IndexBackward>) tensor([[294.1312],\n",
      "        [287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0116],\n",
      "        [0.2516]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-966.8629],\n",
      "        [-867.0729]], grad_fn=<IndexBackward>) tensor([[977.0577],\n",
      "        [886.6078]], grad_fn=<IndexBackward>) tensor([[-304.3145],\n",
      "        [-306.2899]], grad_fn=<IndexBackward>) tensor([[294.1312],\n",
      "        [287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0116],\n",
      "        [0.2516]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-966.8629],\n",
      "        [-867.0729]], grad_fn=<IndexBackward>) tensor([[977.0577],\n",
      "        [886.6078]], grad_fn=<IndexBackward>) tensor([[-304.3145],\n",
      "        [-306.2899]], grad_fn=<IndexBackward>) tensor([[294.1312],\n",
      "        [287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0116],\n",
      "        [0.2516]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1027.5691760778427;\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 6 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-968.3418],\n",
      "        [1022.3344],\n",
      "        [-868.4785]], grad_fn=<IndexBackward>) tensor([[  978.5433],\n",
      "        [-1032.6825],\n",
      "        [  888.0209]], grad_fn=<IndexBackward>) tensor([[-303.8991],\n",
      "        [ 309.2611],\n",
      "        [-305.9195]], grad_fn=<IndexBackward>) tensor([[ 293.6870],\n",
      "        [-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0107],\n",
      "        [-0.0118],\n",
      "        [ 0.1996]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-968.3418],\n",
      "        [1022.3344],\n",
      "        [-868.4785]], grad_fn=<IndexBackward>) tensor([[  978.5433],\n",
      "        [-1032.6825],\n",
      "        [  888.0209]], grad_fn=<IndexBackward>) tensor([[-303.8991],\n",
      "        [ 309.2611],\n",
      "        [-305.9195]], grad_fn=<IndexBackward>) tensor([[ 293.6870],\n",
      "        [-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0107],\n",
      "        [-0.0118],\n",
      "        [ 0.1996]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-968.3418],\n",
      "        [1022.3344],\n",
      "        [-868.4785]], grad_fn=<IndexBackward>) tensor([[  978.5433],\n",
      "        [-1032.6825],\n",
      "        [  888.0209]], grad_fn=<IndexBackward>) tensor([[-303.8991],\n",
      "        [ 309.2611],\n",
      "        [-305.9195]], grad_fn=<IndexBackward>) tensor([[ 293.6870],\n",
      "        [-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0107],\n",
      "        [-0.0118],\n",
      "        [ 0.1996]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "complaint 3 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-873.2335]], grad_fn=<IndexBackward>) tensor([[891.7161]], grad_fn=<IndexBackward>) tensor([[-305.9045]], grad_fn=<IndexBackward>) tensor([[286.1126]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.3093]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-874.4150]], grad_fn=<IndexBackward>) tensor([[892.5202]], grad_fn=<IndexBackward>) tensor([[-306.0424]], grad_fn=<IndexBackward>) tensor([[286.1126]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.8245]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-874.4150]], grad_fn=<IndexBackward>) tensor([[892.5202]], grad_fn=<IndexBackward>) tensor([[-306.0424]], grad_fn=<IndexBackward>) tensor([[286.1126]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.8245]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 0 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-874.4100]], grad_fn=<IndexBackward>) tensor([[892.9740]], grad_fn=<IndexBackward>) tensor([[-305.4952]], grad_fn=<IndexBackward>) tensor([[285.6730]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.2582]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "complaint -20\n",
      "yay -20\n",
      "complaint -40\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "complaint -60\n",
      "complaint -80\n",
      "yay -260\n",
      "complaint -100\n",
      "epoch 100 loss = 1205.972628633181;\n",
      "mode_hat tensor(0.3443, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3859, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3193, requires_grad=True)\n",
      "complaint -120\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "yay -560\n",
      "complaint -140\n",
      "epoch 200 loss = 890.863477150599;\n",
      "mode_hat tensor(0.5723, requires_grad=True)\n",
      "ltscale_hat tensor(-0.6453, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4832, requires_grad=True)\n",
      "complaint -160\n",
      "yay -620\n",
      "complaint -180\n",
      "complaint -200\n",
      "complaint -220\n",
      "yay -680\n",
      "complaint -240\n",
      "complaint -260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay -740\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 906.9384283224742;\n",
      "mode_hat tensor(0.7308, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0078, requires_grad=True)\n",
      "ldfraw_hat tensor(0.6216, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "#vals = MIDDF35\n",
    "guidenames = [\"amortized_laplace\",\"meanfield\",\"unamortized_laplace\"]#,\"unamortized_laplace\",\"meanfield\",]#,\"MLE\"]#,\"laplace\"\n",
    "if True:\n",
    "\n",
    "    for aniter in range(4):\n",
    "        for trueparams in [ndom_fat_params]:#,ndom_norm_params]:#,tdom_fat_params,tdom_norm_params,]:\n",
    "            for nsamps,nparticles in [(10,1),(10,3),(50,1),(50,3)]:#,(100,1),(400,1)]:\n",
    "                    for guidename in guidenames:\n",
    "                        #\n",
    "\n",
    "                        print(aniter,nparticles,guidenames,trueparams)\n",
    "                        result = trainGuide(guidename,nparticles,trueparams,\n",
    "                                            filename=\"testresults/demoT_2.csv\",\n",
    "\n",
    "                                            subsample_N = nsamps)\n",
    "                        print(aniter,nparticles,guidenames)\n",
    "                        for line in range(10):\n",
    "                            print()\n",
    "                    \n",
    "else:\n",
    "    plt.plot([getMLE(.01 + x/100,1.,0,1.) for x in range(500)])\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "MCMC, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "/anaconda3/bin/python\n",
      "Reloading polytopize.\n",
      "addHess tested\n",
      "addHess tested\n",
      "making:\n",
      "1\n",
      "testresults/scenario_N44_mu1.0_sigma2.0_nu-1.0.csv from file\n",
      "44\n",
      "tensor([32.5364, -3.3454,  3.2929, -2.5415])\n",
      "{'hessian': tensor([[1., 0., 1.],\n",
      "        [0., 2., 0.],\n",
      "        [1., 0., 1.]]), 'logPosterior': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"running...\")\n",
    "\n",
    "import sys, os\n",
    "\n",
    "print(sys.executable)\n",
    "#print(sys.path)\n",
    "\n",
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "print(\"making:\")\n",
    "\n",
    "multisiteMod = createScenario(tdom_fat_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-78d7c46ccca3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-78d7c46ccca3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    multisiteMod.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "multisiteMod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27889007329940796\n"
     ]
    }
   ],
   "source": [
    "maxError = torch.max(errors)\n",
    "print(float(maxError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
