{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro\n",
    "\n",
    "This is a jupyter notebook for testing / coding. So far, each code block is a separate test; unlike an ordinary notebook, they are not meant to run sequentially.\n",
    "\n",
    "Let's do MCMC:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, running \"ei\". This started out as a copy of Fritz's code but it's evolved into a working version of ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base:Yes, I will run. line 5 1 :\n",
      "Reloading cmult...\n",
      "Reloading polytopize.\n",
      "Reloading polytopize.\n",
      "base:Yes, I will run. line 5 2 :\n",
      "Reloading polytopize.\n",
      "eiresults/scenario_SIG0.1_0_N527.csv from file\n",
      "svi.step(... line 41 1 : 0 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 1 : 5.27 True\n",
      "types? line 100 1 : [torch.float64, torch.float64]\n",
      "types? line 100 2 : [torch.float64, torch.float64]\n",
      "types? line 100 3 : [torch.float64, torch.float64]\n",
      "sds: tensor(0.6401, grad_fn=<StdBackward0>) tensor(0.6272, grad_fn=<AddBackward0>) tensor(0.6167, grad_fn=<StdBackward0>)\n",
      "model:end line 41 1 :\n",
      "lp:  line 41 1 : tensor(-8853177.3207, grad_fn=<AddBackward0>) tensor(-2105.1289, grad_fn=<AddBackward0>) 774\n",
      "ps2\n",
      "guide:end line 41 1 :\n",
      "model:end line 41 2 :\n",
      " ecstar = tensor([-0.0050, -0.0050], grad_fn=<SliceBackward>)\n",
      "epoch 0 loss = 8.88E+07, mean_loss=8.88E+07;\n",
      " logitstar = tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0150, -0.0150,  0.0300]], grad_fn=<AddBackward0>)\n",
      "svi.step(... line 41 2 : 1 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 2 : 5.27 True\n",
      "types? line 100 4 : [torch.float64, torch.float64]\n",
      "types? line 100 5 : [torch.float64, torch.float64]\n",
      "types? line 100 6 : [torch.float64, torch.float64]\n",
      "model:end line 41 3 :\n",
      "lp:  line 41 2 : tensor(-8074097.0612, grad_fn=<AddBackward0>) tensor(-1690.4484, grad_fn=<AddBackward0>) 774\n",
      "guide:end line 41 2 :\n",
      "model:end line 41 4 :\n",
      "svi.step(... line 41 3 : 2 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 3 : 5.27 True\n",
      "types? line 100 7 : [torch.float64, torch.float64]\n",
      "types? line 100 8 : [torch.float64, torch.float64]\n",
      "types? line 100 9 : [torch.float64, torch.float64]\n",
      "model:end line 41 5 :\n",
      "lp:  line 41 3 : tensor(-8652248.5828, grad_fn=<AddBackward0>) tensor(-1654.9189, grad_fn=<AddBackward0>) 774\n",
      "guide:end line 41 3 :\n",
      "model:end line 41 6 :\n",
      "svi.step(... line 41 4 : 3 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 4 : 5.27 True\n",
      "types? line 100 10 : [torch.float64, torch.float64]\n",
      "model:end line 41 7 :\n",
      "lp:  line 41 4 : tensor(-8640683.8448, grad_fn=<AddBackward0>) tensor(-1772.1308, grad_fn=<AddBackward0>) 774\n",
      "guide:end line 41 4 :\n",
      "model:end line 41 8 :\n",
      "svi.step(... line 41 5 : 4 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 5 : 5.27 True\n",
      "model:end line 41 9 :\n",
      "lp:  line 41 5 : tensor(-7780390.8629, grad_fn=<AddBackward0>) tensor(-1893.5630, grad_fn=<AddBackward0>) 774\n",
      "guide:end line 41 5 :\n",
      "model:end line 41 10 :\n",
      "svi.step(... line 41 6 : 5 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 6 : 5.27 True\n",
      "types? line 100 16\n",
      "lp:  line 41 6 : tensor(-8143061.3895, grad_fn=<AddBackward0>) tensor(-1812.4727, grad_fn=<AddBackward0>) 774\n",
      "guide:end line 41 6 :\n",
      "svi.step(... line 41 7 : 6 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 7 : 5.27 True\n",
      "lp:  line 41 7 : tensor(-9031586.8662, grad_fn=<AddBackward0>) tensor(-1861.0047, grad_fn=<AddBackward0>) 774\n",
      "guide:end line 41 7 :\n",
      "svi.step(... line 41 8 : 7 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 8 : 5.27 True\n",
      "lp:  line 41 8 : tensor(-8229427.5706, grad_fn=<AddBackward0>) tensor(-1929.6380, grad_fn=<AddBackward0>) 774\n",
      "guide:end line 41 8 :\n",
      "model:end line 41 16\n",
      "svi.step(... line 41 9 : 8 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 9 : 5.27 True\n",
      "lp:  line 41 9 : tensor(-8320704.5296, grad_fn=<AddBackward0>) tensor(-1816.1856, grad_fn=<AddBackward0>) 774\n",
      "guide:end line 41 9 :\n",
      "svi.step(... line 41 10 : 9 5.27 torch.Size([100, 9])\n",
      "guide:begin line 41 10 : 5.27 True\n",
      "lp:  line 41 10 : tensor(-8378633.5482, grad_fn=<AddBackward0>) tensor(-1807.4717, grad_fn=<AddBackward0>) 774\n",
      "guide:end line 41 10 :\n",
      "types? line 100 32\n",
      "sds: tensor(0.6112, grad_fn=<StdBackward0>) tensor(0.5961, grad_fn=<AddBackward0>) tensor(0.5847, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.0550, -0.0550], grad_fn=<SliceBackward>)\n",
      "epoch 10 loss = 9.17E+07, mean_loss=8.83E+07;\n",
      " logitstar = tensor([[-0.0529, -0.0529,  0.1058],\n",
      "        [-0.0781, -0.0370,  0.1151],\n",
      "        [-0.0340, -0.0750,  0.1090]], grad_fn=<AddBackward0>)\n",
      "svi.step(... line 41 16\n",
      "guide:begin line 41 16\n",
      "lp:  line 41 16\n",
      "guide:end line 41 16\n",
      "model:end line 41 32\n",
      "sds: tensor(0.5300, grad_fn=<StdBackward0>) tensor(0.5155, grad_fn=<AddBackward0>) tensor(0.4998, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.1050, -0.1050], grad_fn=<SliceBackward>)\n",
      "epoch 20 loss = 8.42E+07, mean_loss=8.77E+07;\n",
      " logitstar = tensor([[-0.1319, -0.1319,  0.2638],\n",
      "        [-0.1364, -0.0537,  0.1901],\n",
      "        [-0.0467, -0.1295,  0.1761]], grad_fn=<AddBackward0>)\n",
      "types? line 100 64\n",
      "sds: tensor(0.4401, grad_fn=<StdBackward0>) tensor(0.4227, grad_fn=<AddBackward0>) tensor(0.4102, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.1550, -0.1550], grad_fn=<SliceBackward>)\n",
      "epoch 30 loss = 8.22E+07, mean_loss=8.72E+07;\n",
      " logitstar = tensor([[-0.2256, -0.2256,  0.4512],\n",
      "        [-0.1543, -0.0586,  0.2129],\n",
      "        [-0.0851, -0.1808,  0.2659]], grad_fn=<AddBackward0>)\n",
      "svi.step(... line 41 32\n",
      "guide:begin line 41 32\n",
      "lp:  line 41 32\n",
      "guide:end line 41 32\n",
      "model:end line 41 64\n",
      "sds: tensor(0.3741, grad_fn=<StdBackward0>) tensor(0.3517, grad_fn=<AddBackward0>) tensor(0.3404, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.2040, -0.2050], grad_fn=<SliceBackward>)\n",
      "epoch 40 loss = 8.06E+07, mean_loss=8.67E+07;\n",
      " logitstar = tensor([[-0.3225, -0.3235,  0.6460],\n",
      "        [-0.1655, -0.0603,  0.2258],\n",
      "        [-0.1240, -0.2312,  0.3552]], grad_fn=<AddBackward0>)\n",
      "types? line 100 128\n",
      "sds: tensor(0.3287, grad_fn=<StdBackward0>) tensor(0.3043, grad_fn=<AddBackward0>) tensor(0.2912, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.2064, -0.2550], grad_fn=<SliceBackward>)\n",
      "epoch 50 loss = 7.96E+07, mean_loss=8.65E+07;\n",
      " logitstar = tensor([[-0.3391, -0.4228,  0.7619],\n",
      "        [-0.1225, -0.0638,  0.1863],\n",
      "        [-0.1576, -0.2785,  0.4361]], grad_fn=<AddBackward0>)\n",
      "sds: tensor(0.2732, grad_fn=<StdBackward0>) tensor(0.2520, grad_fn=<AddBackward0>) tensor(0.2380, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.1746, -0.3050], grad_fn=<SliceBackward>)\n",
      "epoch 60 loss = 8.65E+07, mean_loss=8.63E+07;\n",
      " logitstar = tensor([[-0.2795, -0.5225,  0.8021],\n",
      "        [-0.0423, -0.0813,  0.1236],\n",
      "        [-0.2019, -0.3112,  0.5131]], grad_fn=<AddBackward0>)\n",
      "svi.step(... line 41 64\n",
      "guide:begin line 41 64\n",
      "lp:  line 41 64\n",
      "guide:end line 41 64\n",
      "model:end line 41 128\n",
      "sds: tensor(0.2269, grad_fn=<StdBackward0>) tensor(0.1936, grad_fn=<AddBackward0>) tensor(0.1869, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.1309, -0.3550], grad_fn=<SliceBackward>)\n",
      "epoch 70 loss = 8.43E+07, mean_loss=8.61E+07;\n",
      " logitstar = tensor([[-0.2285, -0.6225,  0.8510],\n",
      "        [ 0.0508, -0.0943,  0.0434],\n",
      "        [-0.2151, -0.3483,  0.5633]], grad_fn=<AddBackward0>)\n",
      "sds: tensor(0.1972, grad_fn=<StdBackward0>) tensor(0.1530, grad_fn=<AddBackward0>) tensor(0.1476, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.0868, -0.4050], grad_fn=<SliceBackward>)\n",
      "epoch 80 loss = 7.43E+07, mean_loss=8.57E+07;\n",
      " logitstar = tensor([[-0.2060, -0.7224,  0.9285],\n",
      "        [ 0.1447, -0.0988, -0.0459],\n",
      "        [-0.1992, -0.3938,  0.5930]], grad_fn=<AddBackward0>)\n",
      "types? line 100 256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9e9a48bbdf86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msigma_nu\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.02\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#.02,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m#%prun result = trainGuide(nsamps=nsamps,subsample_n=subn)#inits = inits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtrainGuide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnsamps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnsamps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubsample_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnsteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnsteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msigmanu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigma_nu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#inits = inits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\eipython\\eipython\\ei_multisample.py\u001b[0m in \u001b[0;36mtrainGuide\u001b[1;34m(subsample_n, filebase, nsteps, sigmanu, dummydata, nsamps, dversion, inits, num_y_samps)\u001b[0m\n\u001b[0;32m   1398\u001b[0m         \u001b[0mddp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"svi.step(...\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindeps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1399\u001b[0m         loss = svi.step(subset,scale,True,do_print=(i % 10 == 0),nsamps=nsamps,\n\u001b[1;32m-> 1400\u001b[1;33m                 inits=inits)\n\u001b[0m\u001b[0;32m   1401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m             \u001b[0mmean_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyro\\infer\\svi.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;31m# get loss and compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyro\\infer\\trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrainable_params\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msurrogate_loss_particle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'requires_grad'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0msurrogate_loss_particle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0msurrogate_loss_particle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mwarn_if_nan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import ei_multisample #import *\n",
    "reload(ei_multisample)\n",
    "from ei_multisample import *\n",
    "import cProfile as profile\n",
    "\n",
    "#ec,erc = legible_values(3,3)\n",
    "#print(ec,\"\\n\",erc,\"\\n\",ec+erc)\n",
    "inits = good_inits(0.)\n",
    "#del inits[\"ercstar_raw\"]\n",
    "nsteps = 2000\n",
    "for (nsamps,subn) in [(10,100)]:#[(2,60),(5,60),(2,30),(20,30),(40,5)]:\n",
    "    \n",
    "    for i in range(5):\n",
    "        for sigma_nu in [.1, .3, .02]: #.02, \n",
    "            #%prun result = trainGuide(nsamps=nsamps,subsample_n=subn)#inits = inits)\n",
    "            trainGuide(nsamps=nsamps,subsample_n=subn,nsteps=nsteps,sigmanu=sigma_nu,dversion=i)#inits = inits)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base:Yes, I will run. line 5 3 :\n",
      "Reloading polytopize.\n",
      "tensor([[ 0.0500, -0.9100,  1.2200],\n",
      "        [ 0.2100,  0.0100, -0.9500],\n",
      "        [ 0.3400, -0.5400,  0.5800]])\n",
      "0.15\n",
      "0.15\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import ei_multisample #import *\n",
    "reload(ei_multisample)\n",
    "from ei_multisample import *\n",
    "import cProfile as profile\n",
    "\n",
    "inits = dict() #good_inits()\n",
    "#del inits[\"ercstar_raw\"]\n",
    "#%prun result = trainGuide(inits = inits)\n",
    "\n",
    "NCparams = EIData.load(\"NC_Data/NC_2016_statewide_alpha_and_beta.csv\")\n",
    "print(NCparams.alpha + NCparams.beta)\n",
    "#print(\"components\")\n",
    "#print(NCparams.alpha)\n",
    "#\n",
    "#print(NCparams.beta)\n",
    "print(SIM_SIGMA_NU)\n",
    "SIM_SIGMA_NU = 0.001\n",
    "print(ei_multisample.SIM_SIGMA_NU)\n",
    "ei_multisample.SIM_SIGMA_NU = .0001\n",
    "print(ei_multisample.SIM_SIGMA_NU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing hessian transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hess tensor([[6., 0., 0., 0.],\n",
      "        [0., 6., 0., 0.],\n",
      "        [0., 0., 6., 0.],\n",
      "        [0., 0., 0., 6.]], grad_fn=<CopySlices>)\n",
      "d  tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "Σd  tensor(12., grad_fn=<SumBackward0>)\n",
      "dΣdΣd  (tensor([[6., 6.],\n",
      "        [6., 6.]]),)\n",
      "(tensor([[72., 72.],\n",
      "        [72., 72.]]),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ts = torch.tensor\n",
    "os = torch.ones\n",
    "zs = torch.zeros\n",
    "from importlib import reload\n",
    "import myhessian\n",
    "reload(myhessian)\n",
    "\n",
    "t1 = os(2,2,requires_grad=True)\n",
    "r = torch.sum(t1 * t1 * t1)\n",
    "\n",
    "h = myhessian.hessian(r,t1)\n",
    "print(\"hess\",h)\n",
    "r2 = torch.sum(h * h)\n",
    "[r3] = torch.autograd.grad(r,t1,create_graph=True,retain_graph=True)\n",
    "print(\"d \",r3)\n",
    "print(\"Σd \",torch.sum(r3))\n",
    "[r4] = torch.autograd.grad(torch.sum(r3),t1,create_graph=True,retain_graph=True)\n",
    "print(\"dΣdΣd \",torch.autograd.grad(torch.sum(r4),t1,create_graph=True,retain_graph=True))\n",
    "print(torch.autograd.grad(r2,t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I will run.\n",
      "Yes, I will run.\n",
      "ge fail\n",
      "loc tensor([[ 0.3147],\n",
      "        [ 2.4440],\n",
      "        [-4.0535],\n",
      "        [ 2.2007]])\n",
      "polytopedLoc tensor([[ 2.1063e-01,  2.1136e-02],\n",
      "        [ 2.8514e-01, -1.8626e-09],\n",
      "        [ 3.4959e-01,  9.9957e-02],\n",
      "        [ 3.5462e-01,  1.1892e-02],\n",
      "        [ 2.5462e+00,  3.2228e-01]])\n",
      "ge fail\n",
      "loc tensor([[-5.8855],\n",
      "        [-5.2805],\n",
      "        [ 5.4654],\n",
      "        [ 0.1889]])\n",
      "polytopedLoc tensor([[ 2.8885e-01,  1.1871e+00],\n",
      "        [ 8.5629e-01,  2.0505e+00],\n",
      "        [ 3.9984e-01, -1.4901e-08],\n",
      "        [ 9.5542e-01,  1.5538e+00],\n",
      "        [ 1.0063e+00,  9.9522e-01]])\n",
      "Reloading cmult...\n",
      "callable? <bound method TorchDistributionMixin.__call__ of Multinomial()>\n",
      "callable? <bound method TorchDistributionMixin.__call__ of TorchCMult()>\n",
      "Sampling multinomial: tensor([1., 2.])\n",
      "Sampling cm2: tensor([0., 3.])\n",
      "tensor(5.6022, grad_fn=<NegBackward>) tensor([[112.2500]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import hessian\n",
    "\n",
    "from importlib import reload\n",
    "import polytopize #import *\n",
    "reload(polytopize)\n",
    "from polytopize import *\n",
    "\n",
    "import tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test rank1torch (to get yhat from pi,n,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimize_Q (50 tests): \n",
      "R=3, C=5, tolerance=0.001\n",
      "==================================================\n",
      "Oh no! In test 3, Q has some negative entries:\n",
      "\t trueQ[2][4]=0.00010659269901225343, \n",
      "\t     Q[2][4]=-0.00021605131041724235\n",
      "Oh no! In test 5, Q has some negative entries:\n",
      "\t trueQ[1][4]=0.00011974151857430115, \n",
      "\t     Q[1][4]=-1.1631345842033625e-06\n",
      "Oh no! In test 8, Q has some negative entries:\n",
      "\t trueQ[0][1]=2.882161788875237e-05, \n",
      "\t     Q[0][1]=-0.0004783869662787765\n",
      "Oh no! In test 15, Q has some negative entries:\n",
      "\t trueQ[2][3]=0.0007846675580367446, \n",
      "\t     Q[2][3]=-6.166117964312434e-05\n",
      "Oh no! In test 28, Q has some negative entries:\n",
      "\t trueQ[0][4]=8.13114020274952e-05, \n",
      "\t     Q[0][4]=-0.00018321917741559446\n",
      "Oh no! In test 40, Q has some negative entries:\n",
      "\t trueQ[2][3]=0.00032634526723995805, \n",
      "\t     Q[2][3]=-0.000617634505033493\n",
      "Oh no! In test 47, Q has some negative entries:\n",
      "\t trueQ[2][1]=0.00017936740186996758, \n",
      "\t     Q[2][1]=-0.00041433278238400817\n",
      "Oh no! In test 48, Q has some negative entries:\n",
      "\t trueQ[0][0]=4.524858377408236e-05, \n",
      "\t     Q[0][0]=-0.0006647921400144696\n",
      "\n",
      "Cumulative results for the 50 tests \n",
      "(R=3, C=5, tolerance=0.001):\n",
      "-------------------------------------------\n",
      "Worst error in entry of Q: 0.0019194334745407104\n",
      "\n",
      "To get within tolerance, it took us:\n",
      "002 iterations: ***** 5.0 times\n",
      "003 iterations: ********** 10.0 times\n",
      "004 iterations: *********** 11.0 times\n",
      "005 iterations: ****** 6.0 times\n",
      "006 iterations: **** 4.0 times\n",
      "007 iterations: **** 4.0 times\n",
      "008 iterations: ****** 6.0 times\n",
      "010 iterations: *** 3.0 times\n",
      "013 iterations: * 1.0 times\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import rank1torch #import *\n",
    "reload(rank1torch)\n",
    "from rank1torch import *\n",
    "\n",
    "test_solver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Most SVI problems in pyro are coded as a model, a generic guide (such as: multivariate Gaussian in all parameters), and specific observations/data (passed as arguments to svi.step). For EI, that's going to be different; the observations are going to be built into the guide function, leaving nothing to include in the \"data\" argument to svi.step.\n",
    "\n",
    "That means there is a lot of work for the guide to do. As usual, it must establish reasonable distributional families for the posterior of each of the hyperparameters. But for the latent parameters, the job of the guide is to take a \"relative strength\" number for each race/candidate/precinct combo, and turn that into a number of votes for each combo, such that those numbers obey all the constraints set by observations. This means that for each precinct (considered separately), the latent guide must:\n",
    "\n",
    "-Find the \"center point\" where candidate preference is independent of race.\n",
    "\n",
    "-Find the \"basis vectors\" (actually, there are more than enough of them to form a basis) which determine the directions to move in the space.\n",
    "\n",
    "-For any given set of \"relative strengths\" which is a distance $d$ in a direction $\\theta$, find the first constraint violated when moving in that direction, and the distance $r$ between the origin and that constraint.\n",
    "\n",
    "-Project the \"relative strengths\" onto the numbers of votes, by moving $r(1-e^{-d})$ in direction $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a = zs(2,2,2,2)\n",
    "a[0,1,1,1] = 2\n",
    "print(a[1,1])\n",
    "print(a[0,1])\n",
    "print(torch.max(a))\n",
    "print(torch.distributions.exponential.Exponential(ts([1])).sample(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
