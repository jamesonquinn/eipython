{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro\n",
    "\n",
    "This is a jupyter notebook for testing / coding. So far, each code block is a separate test; unlike an ordinary notebook, they are not meant to run sequentially.\n",
    "\n",
    "Let's do MCMC:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, running \"ei\". This started out as a copy of Fritz's code but it's evolved into a working version of ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base:Yes, I will run. line 5 1 :\n",
      "Reloading cmult...\n",
      "Reloading polytopize.\n",
      "Reloading polytopize.\n",
      "base:Yes, I will run. line 5 2 :\n",
      "Reloading polytopize.\n",
      "eiresults/scenario_SIG0.02_0_N2774.csv from file\n",
      "svi.step(... line 41 1 : 0 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 1 : 55.48 True\n",
      "types? line 100 1 : [torch.float64, torch.float64]\n",
      "types? line 100 2 : [torch.float64, torch.float64]\n",
      "types? line 100 3 : [torch.float64, torch.float64]\n",
      "sds: tensor(0.6021, grad_fn=<StdBackward0>) tensor(0.5917, grad_fn=<AddBackward0>) tensor(0.5836, grad_fn=<StdBackward0>)\n",
      "model:end line 41 1 :\n",
      "lp:  line 41 1 : tensor(-52959875.2979, grad_fn=<AddBackward0>) tensor(1.2434, grad_fn=<AddBackward0>) 764\n",
      "ps2\n",
      "guide:end line 41 1 :\n",
      "model:end line 41 2 :\n",
      " ecstar = tensor([-0.0050, -0.0050], grad_fn=<SliceBackward>)\n",
      "epoch 0 loss = 7.58E+08, mean_loss=7.58E+08;\n",
      " logitstar = tensor([[-0.0100, -0.0100,  0.0200],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0050, -0.0050,  0.0100]], grad_fn=<AddBackward0>)\n",
      "svi.step(... line 41 2 : 1 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 2 : 55.48 True\n",
      "types? line 100 4 : [torch.float64, torch.float64]\n",
      "types? line 100 5 : [torch.float64, torch.float64]\n",
      "types? line 100 6 : [torch.float64, torch.float64]\n",
      "model:end line 41 3 :\n",
      "lp:  line 41 2 : tensor(-41233622.8443, grad_fn=<AddBackward0>) tensor(1.8587, grad_fn=<AddBackward0>) 764\n",
      "guide:end line 41 2 :\n",
      "model:end line 41 4 :\n",
      "svi.step(... line 41 3 : 2 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 3 : 55.48 True\n",
      "types? line 100 7 : [torch.float64, torch.float64]\n",
      "types? line 100 8 : [torch.float64, torch.float64]\n",
      "types? line 100 9 : [torch.float64, torch.float64]\n",
      "model:end line 41 5 :\n",
      "lp:  line 41 3 : tensor(-45927594.9040, grad_fn=<AddBackward0>) tensor(3.1775, grad_fn=<AddBackward0>) 764\n",
      "guide:end line 41 3 :\n",
      "model:end line 41 6 :\n",
      "svi.step(... line 41 4 : 3 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 4 : 55.48 True\n",
      "types? line 100 10 : [torch.float64, torch.float64]\n",
      "model:end line 41 7 :\n",
      "lp:  line 41 4 : tensor(-43615798.1036, grad_fn=<AddBackward0>) tensor(2.8389, grad_fn=<AddBackward0>) 764\n",
      "guide:end line 41 4 :\n",
      "model:end line 41 8 :\n",
      "svi.step(... line 41 5 : 4 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 5 : 55.48 True\n",
      "model:end line 41 9 :\n",
      "lp:  line 41 5 : tensor(-45611346.1860, grad_fn=<AddBackward0>) tensor(3.2259, grad_fn=<AddBackward0>) 764\n",
      "guide:end line 41 5 :\n",
      "model:end line 41 10 :\n",
      "svi.step(... line 41 6 : 5 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 6 : 55.48 True\n",
      "types? line 100 16\n",
      "lp:  line 41 6 : tensor(-43938229.0938, grad_fn=<AddBackward0>) tensor(4.2193, grad_fn=<AddBackward0>) 764\n",
      "guide:end line 41 6 :\n",
      "svi.step(... line 41 7 : 6 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 7 : 55.48 True\n",
      "lp:  line 41 7 : tensor(-42520040.2841, grad_fn=<AddBackward0>) tensor(5.2883, grad_fn=<AddBackward0>) 764\n",
      "guide:end line 41 7 :\n",
      "svi.step(... line 41 8 : 7 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 8 : 55.48 True\n",
      "lp:  line 41 8 : tensor(-40797613.3449, grad_fn=<AddBackward0>) tensor(5.0136, grad_fn=<AddBackward0>) 764\n",
      "guide:end line 41 8 :\n",
      "model:end line 41 16\n",
      "svi.step(... line 41 9 : 8 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 9 : 55.48 True\n",
      "lp:  line 41 9 : tensor(-46085549.2541, grad_fn=<AddBackward0>) tensor(6.1417, grad_fn=<AddBackward0>) 764\n",
      "guide:end line 41 9 :\n",
      "svi.step(... line 41 10 : 9 55.48 torch.Size([50, 9])\n",
      "guide:begin line 41 10 : 55.48 True\n",
      "lp:  line 41 10 : tensor(-37730329.7165, grad_fn=<AddBackward0>) tensor(5.9127, grad_fn=<AddBackward0>) 764\n",
      "guide:end line 41 10 :\n",
      "types? line 100 32\n",
      "sds: tensor(0.5040, grad_fn=<StdBackward0>) tensor(0.4939, grad_fn=<AddBackward0>) tensor(0.4829, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.0550, -0.0550], grad_fn=<SliceBackward>)\n",
      "epoch 10 loss = 6.69E+08, mean_loss=7.43E+08;\n",
      " logitstar = tensor([[-0.1100, -0.0956,  0.2056],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0550, -0.0694,  0.1244]], grad_fn=<AddBackward0>)\n",
      "svi.step(... line 41 16\n",
      "guide:begin line 41 16\n",
      "lp:  line 41 16\n",
      "guide:end line 41 16\n",
      "model:end line 41 32\n",
      "sds: tensor(0.4444, grad_fn=<StdBackward0>) tensor(0.4311, grad_fn=<AddBackward0>) tensor(0.4167, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.1050, -0.1050], grad_fn=<SliceBackward>)\n",
      "epoch 20 loss = 6.38E+08, mean_loss=7.29E+08;\n",
      " logitstar = tensor([[-0.2100, -0.1925,  0.4025],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.1050, -0.1225,  0.2275]], grad_fn=<AddBackward0>)\n",
      "types? line 100 64\n",
      "sds: tensor(0.3742, grad_fn=<StdBackward0>) tensor(0.3584, grad_fn=<AddBackward0>) tensor(0.3439, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.1550, -0.1550], grad_fn=<SliceBackward>)\n",
      "epoch 30 loss = 6.20E+08, mean_loss=7.12E+08;\n",
      " logitstar = tensor([[-0.3100, -0.2915,  0.6015],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.1550, -0.1735,  0.3285]], grad_fn=<AddBackward0>)\n",
      "svi.step(... line 41 32\n",
      "guide:begin line 41 32\n",
      "lp:  line 41 32\n",
      "guide:end line 41 32\n",
      "model:end line 41 64\n",
      "sds: tensor(0.3248, grad_fn=<StdBackward0>) tensor(0.3068, grad_fn=<AddBackward0>) tensor(0.2933, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.1916, -0.2050], grad_fn=<SliceBackward>)\n",
      "epoch 40 loss = 6.00E+08, mean_loss=6.94E+08;\n",
      " logitstar = tensor([[-0.3956, -0.3912,  0.7868],\n",
      "        [ 0.0134,  0.0000, -0.0134],\n",
      "        [-0.1927, -0.2238,  0.4165]], grad_fn=<AddBackward0>)\n",
      "types? line 100 128\n",
      "sds: tensor(0.2810, grad_fn=<StdBackward0>) tensor(0.2521, grad_fn=<AddBackward0>) tensor(0.2425, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.1758, -0.2550], grad_fn=<SliceBackward>)\n",
      "epoch 50 loss = 5.42E+08, mean_loss=6.78E+08;\n",
      " logitstar = tensor([[-0.3932, -0.4911,  0.8843],\n",
      "        [ 0.0792,  0.0000, -0.0792],\n",
      "        [-0.2134, -0.2739,  0.4873]], grad_fn=<AddBackward0>)\n",
      "sds: tensor(0.2390, grad_fn=<StdBackward0>) tensor(0.2168, grad_fn=<AddBackward0>) tensor(0.2039, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.1376, -0.3050], grad_fn=<SliceBackward>)\n",
      "epoch 60 loss = 4.73E+08, mean_loss=6.60E+08;\n",
      " logitstar = tensor([[-0.3291, -0.5911,  0.9202],\n",
      "        [ 0.1674,  0.0000, -0.1674],\n",
      "        [-0.2511, -0.3239,  0.5750]], grad_fn=<AddBackward0>)\n",
      "svi.step(... line 41 64\n",
      "guide:begin line 41 64\n",
      "lp:  line 41 64\n",
      "guide:end line 41 64\n",
      "model:end line 41 128\n",
      "sds: tensor(0.1875, grad_fn=<StdBackward0>) tensor(0.1506, grad_fn=<AddBackward0>) tensor(0.1406, grad_fn=<StdBackward0>)\n",
      "ps2\n",
      " ecstar = tensor([-0.0917, -0.3475], grad_fn=<SliceBackward>)\n",
      "epoch 70 loss = 5.87E+08, mean_loss=6.44E+08;\n",
      " logitstar = tensor([[-0.2445, -0.6835,  0.9280],\n",
      "        [ 0.2633,  0.0075, -0.2708],\n",
      "        [-0.2939, -0.3664,  0.6603]], grad_fn=<AddBackward0>)\n",
      "ecstar_raw.grad line 41 1 : \n",
      "    Size 0: [3, 2]; NANny\n",
      "       0:XXX  1:XX\n",
      "    Size 1: [3, 2]; NANny\n",
      "       0:XXX  1:XX\n",
      "2s line 41 1 : \n",
      "    Size 0: [50, 3, 3]; NANny\n",
      "       0:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  1:XXX  2:XXX\n",
      "    Size 1: [50, 3, 3]; NANny\n",
      "       0:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  1:XXX  2:XXX\n",
      "    Size 2: [50, 3, 3]; NANny\n",
      "       0:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  1:XXX  2:XXX\n",
      "    Size 3: [50, 2, 2]; NANny\n",
      "       0:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  1:XX  2:XX\n",
      "    Size 4: [50, 2, 2]; NANny\n",
      "       0:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  1:XX  2:XX\n",
      "    Size 5: [50, 3, 3]; NANny\n",
      "       0:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  1:XXX  2:XXX\n",
      "    Size 6: [50, 3, 3]; NANny\n",
      "       0:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  1:XXX  2:XXX\n",
      "    Size 7: [50, 3, 3]; NANny\n",
      "       0:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  1:XXX  2:XXX\n",
      "    Size 8: [50, 3, 3]; NANny\n",
      "       0:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX  1:XXX  2:XXX\n",
      "    Size 9: [6]; noNAN\n",
      "    Size 10: [50, 2, 2]; noNAN\n",
      "    Size 11: [50, 3, 3]; noNAN\n",
      "    Size 12: [50, 3, 3]; noNAN\n",
      "scoper line 41 1 : \n",
      "    Size 0: [50]; noNAN\n",
      "    Size 1: [50, 3]; noNAN\n",
      "    Size 2: [50, 3]; noNAN\n",
      "    Size 3: [50, 9]; noNAN\n",
      "    Size 4: [2]; noNAN\n",
      "    Size 5: [2, 2]; noNAN\n",
      "> c:\\users\\jameson\\dropbox\\eipython\\eipython\\ei_multisample.py(1044)fix_ecerc_grad()\n",
      "-> ec_then_erc_star.grad = ec_then_erc_star.grad + ecerc2r.grad * REATTACH_GRAD_PORTION\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import ei_multisample #import *\n",
    "reload(ei_multisample)\n",
    "from ei_multisample import *\n",
    "import cProfile as profile\n",
    "\n",
    "#ec,erc = legible_values(3,3)\n",
    "#print(ec,\"\\n\",erc,\"\\n\",ec+erc)\n",
    "inits = good_inits(0.)\n",
    "#del inits[\"ercstar_raw\"]\n",
    "nsteps = 2000\n",
    "for (nsamps,subn) in [(10,50)]:#[(2,60),(5,60),(2,30),(20,30),(40,5)]:\n",
    "    for sigma_nu in [.02, .1, .3]:\n",
    "        for i in range(5):\n",
    "            #%prun result = trainGuide(nsamps=nsamps,subsample_n=subn)#inits = inits)\n",
    "            trainGuide(nsamps=nsamps,subsample_n=subn,nsteps=nsteps,sigmanu=sigma_nu,dversion=i)#inits = inits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base:Yes, I will run. line 5 3 :\n",
      "Reloading polytopize.\n",
      "tensor([[ 0.0500, -0.9100,  1.2200],\n",
      "        [ 0.2100,  0.0100, -0.9500],\n",
      "        [ 0.3400, -0.5400,  0.5800]])\n",
      "0.15\n",
      "0.15\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import ei_multisample #import *\n",
    "reload(ei_multisample)\n",
    "from ei_multisample import *\n",
    "import cProfile as profile\n",
    "\n",
    "inits = dict() #good_inits()\n",
    "#del inits[\"ercstar_raw\"]\n",
    "#%prun result = trainGuide(inits = inits)\n",
    "\n",
    "NCparams = EIData.load(\"NC_Data/NC_2016_statewide_alpha_and_beta.csv\")\n",
    "print(NCparams.alpha + NCparams.beta)\n",
    "#print(\"components\")\n",
    "#print(NCparams.alpha)\n",
    "#\n",
    "#print(NCparams.beta)\n",
    "print(SIM_SIGMA_NU)\n",
    "SIM_SIGMA_NU = 0.001\n",
    "print(ei_multisample.SIM_SIGMA_NU)\n",
    "ei_multisample.SIM_SIGMA_NU = .0001\n",
    "print(ei_multisample.SIM_SIGMA_NU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing hessian transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hess tensor([[6., 0., 0., 0.],\n",
      "        [0., 6., 0., 0.],\n",
      "        [0., 0., 6., 0.],\n",
      "        [0., 0., 0., 6.]], grad_fn=<CopySlices>)\n",
      "d  tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "Σd  tensor(12., grad_fn=<SumBackward0>)\n",
      "dΣdΣd  (tensor([[6., 6.],\n",
      "        [6., 6.]]),)\n",
      "(tensor([[72., 72.],\n",
      "        [72., 72.]]),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ts = torch.tensor\n",
    "os = torch.ones\n",
    "zs = torch.zeros\n",
    "from importlib import reload\n",
    "import myhessian\n",
    "reload(myhessian)\n",
    "\n",
    "t1 = os(2,2,requires_grad=True)\n",
    "r = torch.sum(t1 * t1 * t1)\n",
    "\n",
    "h = myhessian.hessian(r,t1)\n",
    "print(\"hess\",h)\n",
    "r2 = torch.sum(h * h)\n",
    "[r3] = torch.autograd.grad(r,t1,create_graph=True,retain_graph=True)\n",
    "print(\"d \",r3)\n",
    "print(\"Σd \",torch.sum(r3))\n",
    "[r4] = torch.autograd.grad(torch.sum(r3),t1,create_graph=True,retain_graph=True)\n",
    "print(\"dΣdΣd \",torch.autograd.grad(torch.sum(r4),t1,create_graph=True,retain_graph=True))\n",
    "print(torch.autograd.grad(r2,t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I will run.\n",
      "Yes, I will run.\n",
      "ge fail\n",
      "loc tensor([[ 0.3147],\n",
      "        [ 2.4440],\n",
      "        [-4.0535],\n",
      "        [ 2.2007]])\n",
      "polytopedLoc tensor([[ 2.1063e-01,  2.1136e-02],\n",
      "        [ 2.8514e-01, -1.8626e-09],\n",
      "        [ 3.4959e-01,  9.9957e-02],\n",
      "        [ 3.5462e-01,  1.1892e-02],\n",
      "        [ 2.5462e+00,  3.2228e-01]])\n",
      "ge fail\n",
      "loc tensor([[-5.8855],\n",
      "        [-5.2805],\n",
      "        [ 5.4654],\n",
      "        [ 0.1889]])\n",
      "polytopedLoc tensor([[ 2.8885e-01,  1.1871e+00],\n",
      "        [ 8.5629e-01,  2.0505e+00],\n",
      "        [ 3.9984e-01, -1.4901e-08],\n",
      "        [ 9.5542e-01,  1.5538e+00],\n",
      "        [ 1.0063e+00,  9.9522e-01]])\n",
      "Reloading cmult...\n",
      "callable? <bound method TorchDistributionMixin.__call__ of Multinomial()>\n",
      "callable? <bound method TorchDistributionMixin.__call__ of TorchCMult()>\n",
      "Sampling multinomial: tensor([1., 2.])\n",
      "Sampling cm2: tensor([0., 3.])\n",
      "tensor(5.6022, grad_fn=<NegBackward>) tensor([[112.2500]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import hessian\n",
    "\n",
    "from importlib import reload\n",
    "import polytopize #import *\n",
    "reload(polytopize)\n",
    "from polytopize import *\n",
    "\n",
    "import tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test rank1torch (to get yhat from pi,n,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimize_Q (50 tests): \n",
      "R=3, C=5, tolerance=0.001\n",
      "==================================================\n",
      "Oh no! In test 3, Q has some negative entries:\n",
      "\t trueQ[2][4]=0.00010659269901225343, \n",
      "\t     Q[2][4]=-0.00021605131041724235\n",
      "Oh no! In test 5, Q has some negative entries:\n",
      "\t trueQ[1][4]=0.00011974151857430115, \n",
      "\t     Q[1][4]=-1.1631345842033625e-06\n",
      "Oh no! In test 8, Q has some negative entries:\n",
      "\t trueQ[0][1]=2.882161788875237e-05, \n",
      "\t     Q[0][1]=-0.0004783869662787765\n",
      "Oh no! In test 15, Q has some negative entries:\n",
      "\t trueQ[2][3]=0.0007846675580367446, \n",
      "\t     Q[2][3]=-6.166117964312434e-05\n",
      "Oh no! In test 28, Q has some negative entries:\n",
      "\t trueQ[0][4]=8.13114020274952e-05, \n",
      "\t     Q[0][4]=-0.00018321917741559446\n",
      "Oh no! In test 40, Q has some negative entries:\n",
      "\t trueQ[2][3]=0.00032634526723995805, \n",
      "\t     Q[2][3]=-0.000617634505033493\n",
      "Oh no! In test 47, Q has some negative entries:\n",
      "\t trueQ[2][1]=0.00017936740186996758, \n",
      "\t     Q[2][1]=-0.00041433278238400817\n",
      "Oh no! In test 48, Q has some negative entries:\n",
      "\t trueQ[0][0]=4.524858377408236e-05, \n",
      "\t     Q[0][0]=-0.0006647921400144696\n",
      "\n",
      "Cumulative results for the 50 tests \n",
      "(R=3, C=5, tolerance=0.001):\n",
      "-------------------------------------------\n",
      "Worst error in entry of Q: 0.0019194334745407104\n",
      "\n",
      "To get within tolerance, it took us:\n",
      "002 iterations: ***** 5.0 times\n",
      "003 iterations: ********** 10.0 times\n",
      "004 iterations: *********** 11.0 times\n",
      "005 iterations: ****** 6.0 times\n",
      "006 iterations: **** 4.0 times\n",
      "007 iterations: **** 4.0 times\n",
      "008 iterations: ****** 6.0 times\n",
      "010 iterations: *** 3.0 times\n",
      "013 iterations: * 1.0 times\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import rank1torch #import *\n",
    "reload(rank1torch)\n",
    "from rank1torch import *\n",
    "\n",
    "test_solver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Most SVI problems in pyro are coded as a model, a generic guide (such as: multivariate Gaussian in all parameters), and specific observations/data (passed as arguments to svi.step). For EI, that's going to be different; the observations are going to be built into the guide function, leaving nothing to include in the \"data\" argument to svi.step.\n",
    "\n",
    "That means there is a lot of work for the guide to do. As usual, it must establish reasonable distributional families for the posterior of each of the hyperparameters. But for the latent parameters, the job of the guide is to take a \"relative strength\" number for each race/candidate/precinct combo, and turn that into a number of votes for each combo, such that those numbers obey all the constraints set by observations. This means that for each precinct (considered separately), the latent guide must:\n",
    "\n",
    "-Find the \"center point\" where candidate preference is independent of race.\n",
    "\n",
    "-Find the \"basis vectors\" (actually, there are more than enough of them to form a basis) which determine the directions to move in the space.\n",
    "\n",
    "-For any given set of \"relative strengths\" which is a distance $d$ in a direction $\\theta$, find the first constraint violated when moving in that direction, and the distance $r$ between the origin and that constraint.\n",
    "\n",
    "-Project the \"relative strengths\" onto the numbers of votes, by moving $r(1-e^{-d})$ in direction $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a = zs(2,2,2,2)\n",
    "a[0,1,1,1] = 2\n",
    "print(a[1,1])\n",
    "print(a[0,1])\n",
    "print(torch.max(a))\n",
    "print(torch.distributions.exponential.Exponential(ts([1])).sample(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
