{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro\n",
    "\n",
    "Scratch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pyro/__init__.py\n",
      "/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import pyro\n",
    "\n",
    "print(pyro.__file__)\n",
    "\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amortizable hierarchical t model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1114.9764868021011; mean_loss= 1114.9764868021011; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "yay -80\n",
      "yay -140\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-758.1817]], grad_fn=<IndexBackward>) tensor([[775.8912]], grad_fn=<IndexBackward>) tensor([[-273.3251]], grad_fn=<IndexBackward>) tensor([[255.5488]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0668]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-760.3786]], grad_fn=<IndexBackward>) tensor([[778.2142]], grad_fn=<IndexBackward>) tensor([[-272.3632]], grad_fn=<IndexBackward>) tensor([[254.4714]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0562]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-760.9210]], grad_fn=<IndexBackward>) tensor([[778.8583]], grad_fn=<IndexBackward>) tensor([[-272.1857]], grad_fn=<IndexBackward>) tensor([[254.3024]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0540]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-762.7485]], grad_fn=<IndexBackward>) tensor([[780.7405]], grad_fn=<IndexBackward>) tensor([[-271.8352]], grad_fn=<IndexBackward>) tensor([[253.9326]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0893]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-763.9128]], grad_fn=<IndexBackward>) tensor([[781.8844]], grad_fn=<IndexBackward>) tensor([[-271.8665]], grad_fn=<IndexBackward>) tensor([[253.9484]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0535]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-766.1877]], grad_fn=<IndexBackward>) tensor([[784.0071]], grad_fn=<IndexBackward>) tensor([[-271.8053]], grad_fn=<IndexBackward>) tensor([[253.8084]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1776]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-767.0217]], grad_fn=<IndexBackward>) tensor([[785.1108]], grad_fn=<IndexBackward>) tensor([[-271.7240]], grad_fn=<IndexBackward>) tensor([[253.8167]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1818]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-768.6386]], grad_fn=<IndexBackward>) tensor([[786.4227]], grad_fn=<IndexBackward>) tensor([[-271.8431]], grad_fn=<IndexBackward>) tensor([[253.8213]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2377]], grad_fn=<IndexBackward>)\n",
      "yay -200\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-774.6683]], grad_fn=<IndexBackward>) tensor([[791.0793]], grad_fn=<IndexBackward>) tensor([[-272.1786]], grad_fn=<IndexBackward>) tensor([[253.6181]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-2.1495]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-769.4078]], grad_fn=<IndexBackward>) tensor([[787.8139]], grad_fn=<IndexBackward>) tensor([[-271.3256]], grad_fn=<IndexBackward>) tensor([[253.4716]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.5521]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -260\n",
      "epoch 100 loss = 975.9642126560211; mean_loss= 1091.5882627338115; ()\n",
      "mode_hat tensor(0.3623, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3023, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2255, requires_grad=True)\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "complaint -20\n",
      "yay -500\n",
      "complaint -40\n",
      "yay -560\n",
      "epoch 200 loss = 1036.641165792942; mean_loss= 1126.1951536712381; ()\n",
      "mode_hat tensor(0.6725, requires_grad=True)\n",
      "ltscale_hat tensor(-0.6750, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3662, requires_grad=True)\n",
      "complaint -60\n",
      "yay -620\n",
      "yay -680\n",
      "complaint -80\n",
      "yay -740\n",
      "complaint -100\n",
      "yay -800\n",
      "complaint -120\n",
      "yay -860\n",
      "complaint -140\n",
      "epoch 300 loss = 979.7822771072388; mean_loss= 1166.7980994195404; ()\n",
      "mode_hat tensor(0.8739, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0026, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4447, requires_grad=True)\n",
      "yay -920\n",
      "complaint -160\n",
      "yay -980\n",
      "complaint -180\n",
      "yay -1040\n",
      "complaint -200\n",
      "yay -1100\n",
      "complaint -220\n",
      "yay -1160\n",
      "epoch 400 loss = 974.7091377973557; mean_loss= 1096.193905829913; ()\n",
      "mode_hat tensor(0.9838, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2699, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4490, requires_grad=True)\n",
      "complaint -240\n",
      "yay -1220\n",
      "complaint -260\n",
      "yay -1280\n",
      "complaint -280\n",
      "yay -1340\n",
      "complaint -300\n",
      "yay -1400\n",
      "complaint -320\n",
      "yay -1460\n",
      "epoch 500 loss = 1046.4857411384583; mean_loss= 1070.3099062611518; ()\n",
      "mode_hat tensor(1.0894, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5619, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5104, requires_grad=True)\n",
      "complaint -340\n",
      "yay -1520\n",
      "complaint -360\n",
      "yay -1580\n",
      "yay -1640\n",
      "yay -1700\n",
      "yay -1760\n",
      "epoch 600 loss = 1349.4986680746078; mean_loss= 1170.8886882018849; ()\n",
      "mode_hat tensor(1.1310, requires_grad=True)\n",
      "ltscale_hat tensor(-1.8173, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4163, requires_grad=True)\n",
      "Final mean_losses: 1170.8886882018849\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.3565, -4.2812], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.416309118270874\n",
      "ltscale_hat:\n",
      "-1.8173495531082153\n",
      "mode_hat:\n",
      "1.1310222148895264\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-923.8293],\n",
      "        [-916.1729]], grad_fn=<IndexBackward>) tensor([[947.8468],\n",
      "        [919.7704]], grad_fn=<IndexBackward>) tensor([[-317.7807],\n",
      "        [-290.5880]], grad_fn=<IndexBackward>) tensor([[294.1312],\n",
      "        [287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.3680],\n",
      "        [0.0161]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 950.8970474004745; mean_loss= 950.8970474004745; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-926.1448],\n",
      "        [-917.6505]], grad_fn=<IndexBackward>) tensor([[949.9050],\n",
      "        [921.2313]], grad_fn=<IndexBackward>) tensor([[-317.5041],\n",
      "        [-290.1745]], grad_fn=<IndexBackward>) tensor([[293.6870],\n",
      "        [286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0568],\n",
      "        [-0.0171]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "complaint 7 assert approx_eq: tensor([[32]]) \n",
      "              1.. tensor([[-927.7808]], grad_fn=<IndexBackward>) tensor([[951.4990]], grad_fn=<IndexBackward>) tensor([[-317.1552]], grad_fn=<IndexBackward>) tensor([[293.2486]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1884]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 6 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-929.0811],\n",
      "        [-920.3700]], grad_fn=<IndexBackward>) tensor([[952.8638],\n",
      "        [923.9966]], grad_fn=<IndexBackward>) tensor([[-316.7628],\n",
      "        [-289.3293]], grad_fn=<IndexBackward>) tensor([[292.8058],\n",
      "        [285.7240]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1742],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 0.0213]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-930.1831],\n",
      "        [-921.7521]], grad_fn=<IndexBackward>) tensor([[954.0871],\n",
      "        [925.3885]], grad_fn=<IndexBackward>) tensor([[-316.3589],\n",
      "        [-288.9171]], grad_fn=<IndexBackward>) tensor([[292.3716],\n",
      "        [285.3037]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0833],\n",
      "        [ 0.0231]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-931.1666],\n",
      "        [-923.1570]], grad_fn=<IndexBackward>) tensor([[955.2330],\n",
      "        [926.7994]], grad_fn=<IndexBackward>) tensor([[-315.9493],\n",
      "        [-288.5144]], grad_fn=<IndexBackward>) tensor([[291.9459],\n",
      "        [284.8919]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0630],\n",
      "        [0.0198]], grad_fn=<IndexBackward>)\n",
      "yay -20\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-765.1313]], grad_fn=<IndexBackward>) tensor([[790.5780]], grad_fn=<IndexBackward>) tensor([[-278.4476]], grad_fn=<IndexBackward>) tensor([[253.0516]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0507]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-765.4813]], grad_fn=<IndexBackward>) tensor([[790.9799]], grad_fn=<IndexBackward>) tensor([[-278.1940]], grad_fn=<IndexBackward>) tensor([[252.8074]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1120]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-766.5524]], grad_fn=<IndexBackward>) tensor([[791.9317]], grad_fn=<IndexBackward>) tensor([[-278.0105]], grad_fn=<IndexBackward>) tensor([[252.5535]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0777]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-767.2944]], grad_fn=<IndexBackward>) tensor([[792.6993]], grad_fn=<IndexBackward>) tensor([[-277.8041]], grad_fn=<IndexBackward>) tensor([[252.3206]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0786]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -260\n",
      "epoch 100 loss = 1007.0630823373795; mean_loss= 1054.2992322434995; ()\n",
      "mode_hat tensor(0.4453, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3259, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2230, requires_grad=True)\n",
      "yay -320\n",
      "yay -380\n",
      "complaint -20\n",
      "yay -440\n",
      "complaint -40\n",
      "yay -500\n",
      "complaint -60\n",
      "yay -560\n",
      "epoch 200 loss = 888.9862604141235; mean_loss= 1052.1201617582178; ()\n",
      "mode_hat tensor(0.7156, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5781, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3549, requires_grad=True)\n",
      "yay -620\n",
      "yay -680\n",
      "yay -740\n",
      "complaint -80\n",
      "yay -800\n",
      "complaint -100\n",
      "yay -860\n",
      "epoch 300 loss = 927.8487397432327; mean_loss= 1056.1834983222361; ()\n",
      "mode_hat tensor(0.8555, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8244, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4572, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 765.8508833646774; mean_loss= 1031.955187204826; ()\n",
      "mode_hat tensor(0.9649, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0700, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5290, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 941.5309363603592; mean_loss= 1032.0720808484211; ()\n",
      "mode_hat tensor(1.0403, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2897, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5750, requires_grad=True)\n",
      "Final mean_losses: 1032.0720808484211\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-5.2643, -5.3252], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.5749931931495667\n",
      "ltscale_hat:\n",
      "-1.2896575927734375\n",
      "mode_hat:\n",
      "1.0402940511703491\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-978.4188],\n",
      "        [ 949.6343],\n",
      "        [-876.5812]], grad_fn=<IndexBackward>) tensor([[ 984.8276],\n",
      "        [-983.6089],\n",
      "        [ 893.0777]], grad_fn=<IndexBackward>) tensor([[-300.5529],\n",
      "        [ 333.5544],\n",
      "        [-303.1477]], grad_fn=<IndexBackward>) tensor([[ 294.1312],\n",
      "        [-299.6739],\n",
      "        [ 287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0129],\n",
      "        [-0.0941],\n",
      "        [ 0.3554]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1086.118458211422; mean_loss= 1086.118458211422; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 8 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 947.6814],\n",
      "        [-877.7819]], grad_fn=<IndexBackward>) tensor([[-981.7775],\n",
      "        [ 894.3514]], grad_fn=<IndexBackward>) tensor([[ 332.7938],\n",
      "        [-302.7436]], grad_fn=<IndexBackward>) tensor([[-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2271],\n",
      "        [ 0.4026]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "complaint 7 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 947.0312],\n",
      "        [-877.7903]], grad_fn=<IndexBackward>) tensor([[-980.8462],\n",
      "        [ 894.8155]], grad_fn=<IndexBackward>) tensor([[ 332.1803],\n",
      "        [-302.1970]], grad_fn=<IndexBackward>) tensor([[-298.1714],\n",
      "        [ 286.1416]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1938],\n",
      "        [0.9698]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 6 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 945.0560],\n",
      "        [-879.0037]], grad_fn=<IndexBackward>) tensor([[-979.0004],\n",
      "        [ 896.0988]], grad_fn=<IndexBackward>) tensor([[ 332.2005],\n",
      "        [-302.5591]], grad_fn=<IndexBackward>) tensor([[-298.2377],\n",
      "        [ 286.4926]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0184],\n",
      "        [1.0287]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 943.9661],\n",
      "        [-884.6609]], grad_fn=<IndexBackward>) tensor([[-977.7664],\n",
      "        [ 900.4003]], grad_fn=<IndexBackward>) tensor([[ 332.0674],\n",
      "        [-303.1780]], grad_fn=<IndexBackward>) tensor([[-298.0360],\n",
      "        [ 286.5864]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.2311],\n",
      "        [-0.8523]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 940.7533],\n",
      "        [-879.2053]], grad_fn=<IndexBackward>) tensor([[-975.0666],\n",
      "        [ 897.1534]], grad_fn=<IndexBackward>) tensor([[ 331.5202],\n",
      "        [-302.3642]], grad_fn=<IndexBackward>) tensor([[-297.6645],\n",
      "        [ 286.5166]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.4576],\n",
      "        [ 2.1005]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 941.0115],\n",
      "        [-885.5696]], grad_fn=<IndexBackward>) tensor([[-974.7649],\n",
      "        [ 901.9388]], grad_fn=<IndexBackward>) tensor([[ 331.2193],\n",
      "        [-302.7476]], grad_fn=<IndexBackward>) tensor([[-297.1330],\n",
      "        [ 286.2923]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.3329],\n",
      "        [-0.0860]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 937.4042],\n",
      "        [-887.1752]], grad_fn=<IndexBackward>) tensor([[-971.7932],\n",
      "        [ 903.4904]], grad_fn=<IndexBackward>) tensor([[ 330.3641],\n",
      "        [-302.4889]], grad_fn=<IndexBackward>) tensor([[-296.5015],\n",
      "        [ 285.9707]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.5264],\n",
      "        [-0.2030]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [145]]) \n",
      "              1.. tensor([[-990.1341],\n",
      "        [ 935.6375],\n",
      "        [-887.3495]], grad_fn=<IndexBackward>) tensor([[ 996.6469],\n",
      "        [-970.0935],\n",
      "        [ 904.0704]], grad_fn=<IndexBackward>) tensor([[-299.2257],\n",
      "        [ 329.7215],\n",
      "        [-302.0611]], grad_fn=<IndexBackward>) tensor([[ 292.7020],\n",
      "        [-295.8618],\n",
      "        [ 285.6404]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0108],\n",
      "        [-0.5963],\n",
      "        [ 0.3001]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 935.6901],\n",
      "        [-889.2817]], grad_fn=<IndexBackward>) tensor([[-969.6515],\n",
      "        [ 905.8447]], grad_fn=<IndexBackward>) tensor([[ 329.2761],\n",
      "        [-301.8151]], grad_fn=<IndexBackward>) tensor([[-295.2036],\n",
      "        [ 285.2911]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1111],\n",
      "        [0.0391]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -20\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "complaint -20\n",
      "yay -260\n",
      "epoch 100 loss = 1039.568216264248; mean_loss= 1102.5865931128958; ()\n",
      "mode_hat tensor(0.4860, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4538, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2559, requires_grad=True)\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -40\n",
      "yay -560\n",
      "complaint -60\n",
      "epoch 200 loss = 1115.4729320406914; mean_loss= 1091.2545653846564; ()\n",
      "mode_hat tensor(0.8861, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8819, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2113, requires_grad=True)\n",
      "yay -620\n",
      "complaint -80\n",
      "yay -680\n",
      "complaint -100\n",
      "yay -740\n",
      "complaint -120\n",
      "yay -800\n",
      "complaint -140\n",
      "yay -860\n",
      "epoch 300 loss = 1079.1846544146538; mean_loss= 1229.2088738499044; ()\n",
      "mode_hat tensor(1.0783, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2795, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0948, requires_grad=True)\n",
      "yay -920\n",
      "complaint -160\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "complaint -180\n",
      "yay -1160\n",
      "epoch 400 loss = 1027.6411447525024; mean_loss= 1095.9766453888428; ()\n",
      "mode_hat tensor(1.1340, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6615, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0843, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "complaint -200\n",
      "yay -1340\n",
      "complaint -220\n",
      "yay -1400\n",
      "complaint -240\n",
      "yay -1460\n",
      "complaint -260\n",
      "epoch 500 loss = 1030.1620709300041; mean_loss= 1229.2111164027651; ()\n",
      "mode_hat tensor(1.1189, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9628, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2448, requires_grad=True)\n",
      "Final mean_losses: 1229.2111164027651\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.5658, -4.4176], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.24477627873420715\n",
      "ltscale_hat:\n",
      "-1.9628170728683472\n",
      "mode_hat:\n",
      "1.1189336776733398\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "complaint 9 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 949.6343],\n",
      "        [-896.4940]], grad_fn=<IndexBackward>) tensor([[-983.6089],\n",
      "        [ 906.5521]], grad_fn=<IndexBackward>) tensor([[ 333.5544],\n",
      "        [-297.0401]], grad_fn=<IndexBackward>) tensor([[-299.6739],\n",
      "        [ 287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0941],\n",
      "        [ 0.0245]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1170.1600197553635; mean_loss= 1170.1600197553635; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 8 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 947.6814],\n",
      "        [-898.0045]], grad_fn=<IndexBackward>) tensor([[-981.7775],\n",
      "        [ 908.0354]], grad_fn=<IndexBackward>) tensor([[ 332.7938],\n",
      "        [-296.6508]], grad_fn=<IndexBackward>) tensor([[-298.9248],\n",
      "        [ 286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2271],\n",
      "        [-0.0432]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "complaint 7 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 945.6884],\n",
      "        [-899.2194]], grad_fn=<IndexBackward>) tensor([[-979.9188],\n",
      "        [ 909.3200]], grad_fn=<IndexBackward>) tensor([[ 331.9974],\n",
      "        [-296.2000]], grad_fn=<IndexBackward>) tensor([[-298.1476],\n",
      "        [ 286.1187]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.3806],\n",
      "        [ 0.0193]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 6 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 943.0180],\n",
      "        [-902.0522]], grad_fn=<IndexBackward>) tensor([[-977.1116],\n",
      "        [ 912.1624]], grad_fn=<IndexBackward>) tensor([[ 330.5630],\n",
      "        [-295.3440]], grad_fn=<IndexBackward>) tensor([[-296.5992],\n",
      "        [ 285.2048]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1298],\n",
      "        [-0.0290]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 941.0815],\n",
      "        [-903.3084]], grad_fn=<IndexBackward>) tensor([[-975.2934],\n",
      "        [ 913.4763]], grad_fn=<IndexBackward>) tensor([[ 329.8114],\n",
      "        [-294.9301]], grad_fn=<IndexBackward>) tensor([[-295.8592],\n",
      "        [ 284.7789]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2596],\n",
      "        [ 0.0167]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[40]]) \n",
      "              1.. tensor([[938.9522]], grad_fn=<IndexBackward>) tensor([[-972.8628]], grad_fn=<IndexBackward>) tensor([[328.3869]], grad_fn=<IndexBackward>) tensor([[-294.2658]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.2104]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[ 936.6667],\n",
      "        [-907.4871]], grad_fn=<IndexBackward>) tensor([[-970.8047],\n",
      "        [ 917.6972]], grad_fn=<IndexBackward>) tensor([[ 327.3369],\n",
      "        [-293.3783]], grad_fn=<IndexBackward>) tensor([[-293.2898],\n",
      "        [ 283.1572]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0909],\n",
      "        [-0.0110]], grad_fn=<IndexBackward>)\n",
      "yay -20\n",
      "complaint 2 assert approx_eq: tensor([[40]]) \n",
      "              1.. tensor([[930.6854]], grad_fn=<IndexBackward>) tensor([[-964.7610]], grad_fn=<IndexBackward>) tensor([[323.8831]], grad_fn=<IndexBackward>) tensor([[-289.7422]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0654]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[ 40],\n",
      "        [200]]) \n",
      "              1.. tensor([[927.1803],\n",
      "        [689.7828]], grad_fn=<IndexBackward>) tensor([[-961.3864],\n",
      "        [-692.0556]], grad_fn=<IndexBackward>) tensor([[322.2913],\n",
      "        [254.9220]], grad_fn=<IndexBackward>) tensor([[-288.1523],\n",
      "        [-252.6386]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0671],\n",
      "        [ 0.0106]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-759.2098]], grad_fn=<IndexBackward>) tensor([[769.6309]], grad_fn=<IndexBackward>) tensor([[-268.2488]], grad_fn=<IndexBackward>) tensor([[257.7656]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0621]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "complaint -20\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1116.0400911569595; mean_loss= 1122.4468689452124; ()\n",
      "mode_hat tensor(0.5032, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4687, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2259, requires_grad=True)\n",
      "yay -320\n",
      "complaint -40\n",
      "yay -380\n",
      "complaint -60\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -80\n",
      "yay -560\n",
      "epoch 200 loss = 1142.159649014473; mean_loss= 1097.3227037760842; ()\n",
      "mode_hat tensor(0.9339, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8980, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1820, requires_grad=True)\n",
      "yay -620\n",
      "complaint -100\n",
      "yay -680\n",
      "complaint -120\n",
      "yay -740\n",
      "yay -800\n",
      "complaint -140\n",
      "yay -860\n",
      "epoch 300 loss = 1114.8107061386108; mean_loss= 1086.5536067784137; ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode_hat tensor(1.0988, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3113, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0175, requires_grad=True)\n",
      "complaint -160\n",
      "yay -920\n",
      "complaint -180\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 1095.3367644548416; mean_loss= 1081.2989643651654; ()\n",
      "mode_hat tensor(1.1144, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6775, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1493, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 1175.3569810390472; mean_loss= 1078.736491454871; ()\n",
      "mode_hat tensor(1.1217, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0399, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.3504, requires_grad=True)\n",
      "complaint -200\n",
      "yay -1520\n",
      "complaint -220\n",
      "yay -1580\n",
      "complaint -240\n",
      "yay -1640\n",
      "yay -1700\n",
      "yay -1760\n",
      "epoch 600 loss = 1050.5284695625305; mean_loss= 1076.453908027597; ()\n",
      "mode_hat tensor(1.1400, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4252, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5185, requires_grad=True)\n",
      "yay -1820\n",
      "complaint -260\n",
      "yay -1880\n",
      "complaint -280\n",
      "yay -1940\n",
      "complaint -300\n",
      "yay -2000\n",
      "complaint -320\n",
      "yay -2060\n",
      "complaint -340\n",
      "epoch 700 loss = 968.8748158216476; mean_loss= 1075.314982810999; ()\n",
      "mode_hat tensor(1.1625, requires_grad=True)\n",
      "ltscale_hat tensor(-2.7904, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.6556, requires_grad=True)\n",
      "yay -2120\n",
      "complaint -360\n",
      "yay -2180\n",
      "complaint -380\n",
      "yay -2240\n",
      "yay -2300\n",
      "yay -2360\n",
      "complaint -400\n",
      "epoch 800 loss = 1083.4078863859177; mean_loss= 1075.394789017922; ()\n",
      "mode_hat tensor(1.1426, requires_grad=True)\n",
      "ltscale_hat tensor(-3.1861, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.8055, requires_grad=True)\n",
      "yay -2420\n",
      "complaint -420\n",
      "yay -2480\n",
      "complaint -440\n",
      "yay -2540\n",
      "complaint -460\n",
      "yay -2600\n",
      "complaint -480\n",
      "yay -2660\n",
      "complaint -500\n",
      "epoch 900 loss = 973.6448063850403; mean_loss= 1073.6616072183685; ()\n",
      "mode_hat tensor(1.1032, requires_grad=True)\n",
      "ltscale_hat tensor(-3.6060, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.9373, requires_grad=True)\n",
      "yay -2720\n",
      "complaint -520\n",
      "yay -2780\n",
      "complaint -540\n",
      "yay -2840\n",
      "complaint -560\n",
      "yay -2900\n",
      "yay -2960\n",
      "complaint -580\n",
      "epoch 1000 loss = 1094.057658314705; mean_loss= 1073.129266903168; ()\n",
      "mode_hat tensor(1.1163, requires_grad=True)\n",
      "ltscale_hat tensor(-4.0448, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.0263, requires_grad=True)\n",
      "yay -3020\n",
      "complaint -600\n",
      "yay -3080\n",
      "yay -3140\n",
      "yay -3200\n",
      "complaint -620\n",
      "yay -3260\n",
      "epoch 1100 loss = 1087.064866900444; mean_loss= 1073.5004625122874; ()\n",
      "mode_hat tensor(1.1316, requires_grad=True)\n",
      "ltscale_hat tensor(-4.4934, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.1245, requires_grad=True)\n",
      "yay -3320\n",
      "yay -3380\n",
      "complaint -640\n",
      "yay -3440\n",
      "complaint -660\n",
      "yay -3500\n",
      "complaint -680\n",
      "yay -3560\n",
      "complaint -700\n",
      "epoch 1200 loss = 1129.8919206857681; mean_loss= 1072.4021324601288; ()\n",
      "mode_hat tensor(1.1359, requires_grad=True)\n",
      "ltscale_hat tensor(-4.9765, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2016, requires_grad=True)\n",
      "yay -3620\n",
      "complaint -720\n",
      "yay -3680\n",
      "yay -3740\n",
      "complaint -740\n",
      "yay -3800\n",
      "complaint -760\n",
      "yay -3860\n",
      "epoch 1300 loss = 1095.2343722581863; mean_loss= 1070.4244836910068; ()\n",
      "mode_hat tensor(1.1281, requires_grad=True)\n",
      "ltscale_hat tensor(-5.4628, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2539, requires_grad=True)\n",
      "yay -3920\n",
      "complaint -780\n",
      "yay -3980\n",
      "complaint -800\n",
      "yay -4040\n",
      "Final mean_losses: 1071.671819890418\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6266, -6.4450], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-1.2680373191833496\n",
      "ltscale_hat:\n",
      "-5.69033145904541\n",
      "mode_hat:\n",
      "1.1223994493484497\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 100 4.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1115.5527009367943; mean_loss= 1115.5527009367943; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "complaint 9 assert approx_eq: tensor([[32]]) \n",
      "              1.. tensor([[-945.5435]], grad_fn=<IndexBackward>) tensor([[963.6050]], grad_fn=<IndexBackward>) tensor([[-311.3103]], grad_fn=<IndexBackward>) tensor([[293.1966]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0522]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [153]]) \n",
      "              1.. tensor([[-957.8794],\n",
      "        [-765.9611]], grad_fn=<IndexBackward>) tensor([[976.3407],\n",
      "        [768.7454]], grad_fn=<IndexBackward>) tensor([[-306.9981],\n",
      "        [-266.5628]], grad_fn=<IndexBackward>) tensor([[288.5586],\n",
      "        [263.7250]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0219],\n",
      "        [-0.0536]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-768.1996]], grad_fn=<IndexBackward>) tensor([[771.0811]], grad_fn=<IndexBackward>) tensor([[-265.7104]], grad_fn=<IndexBackward>) tensor([[262.8890]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0601]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-771.9614]], grad_fn=<IndexBackward>) tensor([[774.8586]], grad_fn=<IndexBackward>) tensor([[-264.4487]], grad_fn=<IndexBackward>) tensor([[261.6068]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0554]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-773.5064]], grad_fn=<IndexBackward>) tensor([[776.3135]], grad_fn=<IndexBackward>) tensor([[-264.0108]], grad_fn=<IndexBackward>) tensor([[261.1297]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0739]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-774.7733]], grad_fn=<IndexBackward>) tensor([[777.5827]], grad_fn=<IndexBackward>) tensor([[-263.5789]], grad_fn=<IndexBackward>) tensor([[260.6903]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0792]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[ 32],\n",
      "        [153]]) \n",
      "              1.. tensor([[-969.1281],\n",
      "        [-774.6675]], grad_fn=<IndexBackward>) tensor([[987.8868],\n",
      "        [777.9337]], grad_fn=<IndexBackward>) tensor([[-303.3389],\n",
      "        [-262.9388]], grad_fn=<IndexBackward>) tensor([[284.5938],\n",
      "        [260.1985]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0136],\n",
      "        [0.5259]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-779.1125]], grad_fn=<IndexBackward>) tensor([[781.3301]], grad_fn=<IndexBackward>) tensor([[-262.8836]], grad_fn=<IndexBackward>) tensor([[259.7776]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.8885]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[ 32],\n",
      "        [153]]) \n",
      "              1.. tensor([[-971.9604],\n",
      "        [-780.1600]], grad_fn=<IndexBackward>) tensor([[990.7918],\n",
      "        [782.4540]], grad_fn=<IndexBackward>) tensor([[-302.4326],\n",
      "        [-262.4126]], grad_fn=<IndexBackward>) tensor([[283.6121],\n",
      "        [259.3250]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0109],\n",
      "        [-0.7936]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-779.1215]], grad_fn=<IndexBackward>) tensor([[782.1827]], grad_fn=<IndexBackward>) tensor([[-261.7783]], grad_fn=<IndexBackward>) tensor([[258.9420]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.2248]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1078.8034546375275; mean_loss= 1120.093848321979; ()\n",
      "mode_hat tensor(0.5026, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4960, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2588, requires_grad=True)\n",
      "yay -320\n",
      "complaint -20\n",
      "yay -380\n",
      "complaint -40\n",
      "yay -440\n",
      "complaint -60\n",
      "yay -500\n",
      "yay -560\n",
      "complaint -80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 loss = 1038.8694776892662; mean_loss= 1100.0994978172098; ()\n",
      "mode_hat tensor(0.9311, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9564, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1616, requires_grad=True)\n",
      "yay -620\n",
      "complaint -100\n",
      "yay -680\n",
      "yay -740\n",
      "complaint -120\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 1068.1525381207466; mean_loss= 1092.1148171107927; ()\n",
      "mode_hat tensor(1.1126, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3858, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0627, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "complaint -140\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n",
      "complaint -160\n",
      "epoch 400 loss = 1143.6267414093018; mean_loss= 1090.0518272547934; ()\n",
      "mode_hat tensor(1.1522, requires_grad=True)\n",
      "ltscale_hat tensor(-1.8056, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2937, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -180\n",
      "yay -1280\n",
      "yay -1340\n",
      "complaint -200\n",
      "yay -1400\n",
      "complaint -220\n",
      "yay -1460\n",
      "epoch 500 loss = 1113.942144036293; mean_loss= 1088.1050357629263; ()\n",
      "mode_hat tensor(1.1187, requires_grad=True)\n",
      "ltscale_hat tensor(-2.1193, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.4890, requires_grad=True)\n",
      "complaint -240\n",
      "yay -1520\n",
      "yay -1580\n",
      "yay -1640\n",
      "complaint -260\n",
      "yay -1700\n",
      "complaint -280\n",
      "yay -1760\n",
      "epoch 600 loss = 1125.966460287571; mean_loss= 1091.2305682936783; ()\n",
      "mode_hat tensor(1.1206, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4676, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.6269, requires_grad=True)\n",
      "complaint -300\n",
      "yay -1820\n",
      "complaint -320\n",
      "yay -1880\n",
      "complaint -340\n",
      "yay -1940\n",
      "yay -2000\n",
      "complaint -360\n",
      "yay -2060\n",
      "epoch 700 loss = 1120.854071676731; mean_loss= 1090.7140914771712; ()\n",
      "mode_hat tensor(1.1164, requires_grad=True)\n",
      "ltscale_hat tensor(-2.7439, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.7963, requires_grad=True)\n",
      "complaint -380\n",
      "yay -2120\n",
      "yay -2180\n",
      "complaint -400\n",
      "yay -2240\n",
      "complaint -420\n",
      "yay -2300\n",
      "complaint -440\n",
      "Final mean_losses: 1106.8801492301818\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6245, -4.6818], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.8907121419906616\n",
      "ltscale_hat:\n",
      "-2.9511263370513916\n",
      "mode_hat:\n",
      "1.1108951568603516\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 100 4.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1123.2769557237625; mean_loss= 1123.2769557237625; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 9 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-845.2202],\n",
      "        [-759.3675]], grad_fn=<IndexBackward>) tensor([[877.4585],\n",
      "        [765.1595]], grad_fn=<IndexBackward>) tensor([[-312.8545],\n",
      "        [-268.5270]], grad_fn=<IndexBackward>) tensor([[280.5583],\n",
      "        [262.7608]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0579],\n",
      "        [ 0.0259]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-846.1781],\n",
      "        [-760.7252]], grad_fn=<IndexBackward>) tensor([[878.5686],\n",
      "        [766.4880]], grad_fn=<IndexBackward>) tensor([[-312.3589],\n",
      "        [-268.0709]], grad_fn=<IndexBackward>) tensor([[280.0326],\n",
      "        [262.2774]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0641],\n",
      "        [-0.0306]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-847.4324]], grad_fn=<IndexBackward>) tensor([[879.8844]], grad_fn=<IndexBackward>) tensor([[-311.9424]], grad_fn=<IndexBackward>) tensor([[279.5465]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0561]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-849.7671],\n",
      "        [-764.4169]], grad_fn=<IndexBackward>) tensor([[882.3969],\n",
      "        [770.2198]], grad_fn=<IndexBackward>) tensor([[-311.2184],\n",
      "        [-266.8852]], grad_fn=<IndexBackward>) tensor([[278.6961],\n",
      "        [261.0527]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.1075],\n",
      "        [-0.0295]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-852.0929],\n",
      "        [-766.7606]], grad_fn=<IndexBackward>) tensor([[884.9050],\n",
      "        [772.6312]], grad_fn=<IndexBackward>) tensor([[-310.4588],\n",
      "        [-266.0897]], grad_fn=<IndexBackward>) tensor([[277.8155],\n",
      "        [260.2457]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1688],\n",
      "        [0.0265]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-853.4743],\n",
      "        [-767.7150]], grad_fn=<IndexBackward>) tensor([[886.3111],\n",
      "        [773.6913]], grad_fn=<IndexBackward>) tensor([[-310.1280],\n",
      "        [-265.6868]], grad_fn=<IndexBackward>) tensor([[277.3965],\n",
      "        [259.8620]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1053],\n",
      "        [0.1516]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-858.1853],\n",
      "        [-768.8729]], grad_fn=<IndexBackward>) tensor([[890.0211],\n",
      "        [774.8888]], grad_fn=<IndexBackward>) tensor([[-310.1750],\n",
      "        [-265.2869]], grad_fn=<IndexBackward>) tensor([[276.9561],\n",
      "        [259.4583]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.3831],\n",
      "        [ 0.1873]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-859.3753],\n",
      "        [-770.5665]], grad_fn=<IndexBackward>) tensor([[891.2956],\n",
      "        [776.4465]], grad_fn=<IndexBackward>) tensor([[-309.8347],\n",
      "        [-264.9825]], grad_fn=<IndexBackward>) tensor([[276.5516],\n",
      "        [259.0882]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.3628],\n",
      "        [-0.0143]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-859.4369],\n",
      "        [-771.9643]], grad_fn=<IndexBackward>) tensor([[891.7902],\n",
      "        [777.8058]], grad_fn=<IndexBackward>) tensor([[-309.3873],\n",
      "        [-264.6688]], grad_fn=<IndexBackward>) tensor([[276.1736],\n",
      "        [258.7429]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.8605],\n",
      "        [-0.0844]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-858.3101],\n",
      "        [-772.0512]], grad_fn=<IndexBackward>) tensor([[891.4620],\n",
      "        [778.2850]], grad_fn=<IndexBackward>) tensor([[-308.7823],\n",
      "        [-264.1919]], grad_fn=<IndexBackward>) tensor([[275.7813],\n",
      "        [258.3843]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1510],\n",
      "        [0.4262]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1109.610090315342; mean_loss= 1126.689676179859; ()\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5030, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2582, requires_grad=True)\n",
      "complaint -20\n",
      "yay -320\n",
      "yay -380\n",
      "complaint -40\n",
      "yay -440\n",
      "complaint -60\n",
      "yay -500\n",
      "complaint -80\n",
      "yay -560\n",
      "epoch 200 loss = 1097.2369048595428; mean_loss= 1107.5468165024633; ()\n",
      "mode_hat tensor(0.9634, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9791, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1626, requires_grad=True)\n",
      "complaint -100\n",
      "yay -620\n",
      "yay -680\n",
      "complaint -120\n",
      "yay -740\n",
      "yay -800\n",
      "complaint -140\n",
      "yay -860\n",
      "epoch 300 loss = 1177.4178119301796; mean_loss= 1099.7432608209208; ()\n",
      "mode_hat tensor(1.1111, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4303, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0962, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "complaint -160\n",
      "yay -1100\n",
      "complaint -180\n",
      "yay -1160\n",
      "epoch 400 loss = 1086.877897620201; mean_loss= 1093.8221586692364; ()\n",
      "mode_hat tensor(1.1186, requires_grad=True)\n",
      "ltscale_hat tensor(-1.8548, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldfraw_hat tensor(-0.3428, requires_grad=True)\n",
      "complaint -200\n",
      "yay -1220\n",
      "complaint -220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 1068.9547646045685; mean_loss= 1090.9346453509359; ()\n",
      "mode_hat tensor(1.1234, requires_grad=True)\n",
      "ltscale_hat tensor(-2.2800, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5670, requires_grad=True)\n",
      "yay -1520\n",
      "yay -1580\n",
      "yay -1640\n",
      "complaint -240\n",
      "yay -1700\n",
      "complaint -260\n",
      "yay -1760\n",
      "epoch 600 loss = 1105.5391873121262; mean_loss= 1090.0435182596862; ()\n",
      "mode_hat tensor(1.1221, requires_grad=True)\n",
      "ltscale_hat tensor(-2.7161, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.7432, requires_grad=True)\n",
      "complaint -280\n",
      "yay -1820\n",
      "complaint -300\n",
      "yay -1880\n",
      "yay -1940\n",
      "complaint -320\n",
      "yay -2000\n",
      "complaint -340\n",
      "yay -2060\n",
      "epoch 700 loss = 1067.8420711755753; mean_loss= 1087.9753042979764; ()\n",
      "mode_hat tensor(1.1288, requires_grad=True)\n",
      "ltscale_hat tensor(-3.1647, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.9281, requires_grad=True)\n",
      "yay -2120\n",
      "yay -2180\n",
      "yay -2240\n",
      "yay -2300\n",
      "yay -2360\n",
      "epoch 800 loss = 1065.5951558947563; mean_loss= 1086.7396698481305; ()\n",
      "mode_hat tensor(1.1285, requires_grad=True)\n",
      "ltscale_hat tensor(-3.6336, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.0739, requires_grad=True)\n",
      "yay -2420\n",
      "yay -2480\n",
      "yay -2540\n",
      "yay -2600\n",
      "yay -2660\n",
      "epoch 900 loss = 1053.291099011898; mean_loss= 1085.5798868545903; ()\n",
      "mode_hat tensor(1.1255, requires_grad=True)\n",
      "ltscale_hat tensor(-4.0873, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.1782, requires_grad=True)\n",
      "yay -2720\n",
      "yay -2780\n",
      "complaint -360\n",
      "yay -2840\n",
      "complaint -380\n",
      "yay -2900\n",
      "complaint -400\n",
      "yay -2960\n",
      "complaint -420\n",
      "epoch 1000 loss = 1081.8704847693443; mean_loss= 1085.3314719311109; ()\n",
      "mode_hat tensor(1.1139, requires_grad=True)\n",
      "ltscale_hat tensor(-4.5725, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2550, requires_grad=True)\n",
      "yay -3020\n",
      "complaint -440\n",
      "yay -3080\n",
      "complaint -460\n",
      "yay -3140\n",
      "complaint -480\n",
      "yay -3200\n",
      "complaint -500\n",
      "yay -3260\n",
      "epoch 1100 loss = 1136.3708948493004; mean_loss= 1085.9857577361693; ()\n",
      "mode_hat tensor(1.1219, requires_grad=True)\n",
      "ltscale_hat tensor(-5.0614, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2783, requires_grad=True)\n",
      "complaint -520\n",
      "yay -3320\n",
      "complaint -540\n",
      "yay -3380\n",
      "complaint -560\n",
      "yay -3440\n",
      "complaint -580\n",
      "yay -3500\n",
      "yay -3560\n",
      "complaint -600\n",
      "epoch 1200 loss = 1031.8779370188713; mean_loss= 1083.071528566265; ()\n",
      "mode_hat tensor(1.1441, requires_grad=True)\n",
      "ltscale_hat tensor(-5.5439, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.3114, requires_grad=True)\n",
      "yay -3620\n",
      "complaint -620\n",
      "yay -3680\n",
      "complaint -640\n",
      "yay -3740\n",
      "complaint -660\n",
      "yay -3800\n",
      "complaint -680\n",
      "yay -3860\n",
      "epoch 1300 loss = 1056.0633410215378; mean_loss= 1082.7079681828948; ()\n",
      "mode_hat tensor(1.1341, requires_grad=True)\n",
      "ltscale_hat tensor(-6.0317, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.3758, requires_grad=True)\n",
      "complaint -700\n",
      "yay -3920\n",
      "yay -3980\n",
      "complaint -720\n",
      "yay -4040\n",
      "complaint -740\n",
      "yay -4100\n",
      "complaint -760\n",
      "yay -4160\n",
      "epoch 1400 loss = 1113.3569788336754; mean_loss= 1083.5297334451254; ()\n",
      "mode_hat tensor(1.1410, requires_grad=True)\n",
      "ltscale_hat tensor(-6.3896, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4048, requires_grad=True)\n",
      "yay -4220\n",
      "complaint -780\n",
      "yay -4280\n",
      "complaint -800\n",
      "yay -4340\n",
      "yay -4400\n",
      "complaint -820\n",
      "yay -4460\n",
      "complaint -840\n",
      "epoch 1500 loss = 1175.3621363043785; mean_loss= 1084.5048130850723; ()\n",
      "mode_hat tensor(1.1203, requires_grad=True)\n",
      "ltscale_hat tensor(-6.8680, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4605, requires_grad=True)\n",
      "yay -4520\n",
      "complaint -860\n",
      "yay -4580\n",
      "complaint -880\n",
      "yay -4640\n",
      "complaint -900\n",
      "yay -4700\n",
      "complaint -920\n",
      "yay -4760\n",
      "epoch 1600 loss = 1082.6204783916473; mean_loss= 1081.9259112434725; ()\n",
      "mode_hat tensor(1.1136, requires_grad=True)\n",
      "ltscale_hat tensor(-7.3341, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5087, requires_grad=True)\n",
      "yay -4820\n",
      "complaint -940\n",
      "yay -4880\n",
      "yay -4940\n",
      "yay -5000\n",
      "yay -5060\n",
      "complaint -960\n",
      "epoch 1700 loss = 994.2666854858398; mean_loss= 1079.7812782378421; ()\n",
      "mode_hat tensor(1.1246, requires_grad=True)\n",
      "ltscale_hat tensor(-7.7740, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5463, requires_grad=True)\n",
      "yay -5120\n",
      "yay -5180\n",
      "yay -5240\n",
      "yay -5300\n",
      "yay -5360\n",
      "epoch 1800 loss = 1065.6500234603882; mean_loss= 1080.8324582005991; ()\n",
      "mode_hat tensor(1.1356, requires_grad=True)\n",
      "ltscale_hat tensor(-8.2536, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5816, requires_grad=True)\n",
      "yay -5420\n",
      "Final mean_losses: 1081.7334474157333\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6253, -6.4485], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-1.5894287824630737\n",
      "ltscale_hat:\n",
      "-8.351415634155273\n",
      "mode_hat:\n",
      "1.120762825012207\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 399 1.0025062656641603\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-974.3140],\n",
      "        [-877.1096],\n",
      "        [-759.9992]], grad_fn=<IndexBackward>) tensor([[982.0711],\n",
      "        [893.4365],\n",
      "        [760.1707]], grad_fn=<IndexBackward>) tensor([[-301.9076],\n",
      "        [-302.5295],\n",
      "        [-268.8676]], grad_fn=<IndexBackward>) tensor([[294.1312],\n",
      "        [287.0066],\n",
      "        [268.6786]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0193],\n",
      "        [ 0.8040],\n",
      "        [-0.0174]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1044.9734942018986; mean_loss= 1044.9734942018986; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 8 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-878.3118],\n",
      "        [-761.1728]], grad_fn=<IndexBackward>) tensor([[894.7113],\n",
      "        [761.3699]], grad_fn=<IndexBackward>) tensor([[-302.1243],\n",
      "        [-268.4661]], grad_fn=<IndexBackward>) tensor([[286.5767],\n",
      "        [268.2856]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.8519],\n",
      "        [0.0166]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "complaint 7 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-883.0518]], grad_fn=<IndexBackward>) tensor([[898.3877]], grad_fn=<IndexBackward>) tensor([[-302.0463]], grad_fn=<IndexBackward>) tensor([[286.0731]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.6373]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 6 assert approx_eq: tensor([[ 32],\n",
      "        [145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-981.5402],\n",
      "        [-885.3319],\n",
      "        [-766.1968]], grad_fn=<IndexBackward>) tensor([[989.3901],\n",
      "        [901.3165],\n",
      "        [766.3911]], grad_fn=<IndexBackward>) tensor([[-299.5720],\n",
      "        [-300.5505],\n",
      "        [-266.7155]], grad_fn=<IndexBackward>) tensor([[291.7117],\n",
      "        [284.6634],\n",
      "        [266.5315]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0103],\n",
      "        [ 0.0974],\n",
      "        [ 0.0103]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-888.4052],\n",
      "        [-768.8506]], grad_fn=<IndexBackward>) tensor([[904.3253],\n",
      "        [768.9989]], grad_fn=<IndexBackward>) tensor([[-299.8092],\n",
      "        [-265.9439]], grad_fn=<IndexBackward>) tensor([[283.8013],\n",
      "        [265.7428]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0878],\n",
      "        [-0.0528]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-889.6718],\n",
      "        [-770.0261]], grad_fn=<IndexBackward>) tensor([[905.6471],\n",
      "        [770.2026]], grad_fn=<IndexBackward>) tensor([[-299.4261],\n",
      "        [-265.5570]], grad_fn=<IndexBackward>) tensor([[283.3881],\n",
      "        [265.3651]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0626],\n",
      "        [-0.0154]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-890.7069],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-771.1169]], grad_fn=<IndexBackward>) tensor([[906.8123],\n",
      "        [771.3503]], grad_fn=<IndexBackward>) tensor([[-298.9989],\n",
      "        [-265.1441]], grad_fn=<IndexBackward>) tensor([[282.9576],\n",
      "        [264.9712]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0641],\n",
      "        [0.0605]], grad_fn=<IndexBackward>)\n",
      "yay -20\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-775.4036]], grad_fn=<IndexBackward>) tensor([[775.4724]], grad_fn=<IndexBackward>) tensor([[-264.0587]], grad_fn=<IndexBackward>) tensor([[263.8278]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1622]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-777.2195]], grad_fn=<IndexBackward>) tensor([[777.5278]], grad_fn=<IndexBackward>) tensor([[-263.2519]], grad_fn=<IndexBackward>) tensor([[263.1011]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1576]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-898.8129],\n",
      "        [-778.6296]], grad_fn=<IndexBackward>) tensor([[915.0997],\n",
      "        [778.8912]], grad_fn=<IndexBackward>) tensor([[-296.8022],\n",
      "        [-262.9185]], grad_fn=<IndexBackward>) tensor([[280.5288],\n",
      "        [262.7513]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0134],\n",
      "        [0.0943]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1029.0966348350048; mean_loss= 1032.8636838888017; ()\n",
      "mode_hat tensor(0.4782, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4648, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2849, requires_grad=True)\n",
      "yay -320\n",
      "complaint -20\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -40\n",
      "yay -560\n",
      "complaint -60\n",
      "epoch 200 loss = 1002.3328350484371; mean_loss= 1012.8056256487478; ()\n",
      "mode_hat tensor(0.9482, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9421, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0865, requires_grad=True)\n",
      "yay -620\n",
      "complaint -80\n",
      "yay -680\n",
      "complaint -100\n",
      "yay -740\n",
      "yay -800\n",
      "complaint -120\n",
      "yay -860\n",
      "epoch 300 loss = 1003.4990381896496; mean_loss= 1003.222548706078; ()\n",
      "mode_hat tensor(1.1427, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3540, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1361, requires_grad=True)\n",
      "yay -920\n",
      "complaint -140\n",
      "yay -980\n",
      "complaint -160\n",
      "yay -1040\n",
      "complaint -180\n",
      "yay -1100\n",
      "complaint -200\n",
      "yay -1160\n",
      "epoch 400 loss = 996.3802643716335; mean_loss= 999.6593909905484; ()\n",
      "mode_hat tensor(1.1639, requires_grad=True)\n",
      "ltscale_hat tensor(-1.7280, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.3256, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "complaint -220\n",
      "yay -1460\n",
      "epoch 500 loss = 998.3253571987152; mean_loss= 997.3585412911983; ()\n",
      "mode_hat tensor(1.1426, requires_grad=True)\n",
      "ltscale_hat tensor(-2.1450, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.6409, requires_grad=True)\n",
      "yay -1520\n",
      "complaint -240\n",
      "yay -1580\n",
      "complaint -260\n",
      "yay -1640\n",
      "yay -1700\n",
      "complaint -280\n",
      "yay -1760\n",
      "complaint -300\n",
      "epoch 600 loss = 994.7053697407246; mean_loss= 995.7476494391383; ()\n",
      "mode_hat tensor(1.1378, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4792, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.8269, requires_grad=True)\n",
      "yay -1820\n",
      "complaint -320\n",
      "yay -1880\n",
      "complaint -340\n",
      "yay -1940\n",
      "complaint -360\n",
      "yay -2000\n",
      "complaint -380\n",
      "yay -2060\n",
      "complaint -400\n",
      "epoch 700 loss = 995.8026705086231; mean_loss= 994.761545613039; ()\n",
      "mode_hat tensor(1.1174, requires_grad=True)\n",
      "ltscale_hat tensor(-2.7453, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.8981, requires_grad=True)\n",
      "yay -2120\n",
      "complaint -420\n",
      "yay -2180\n",
      "complaint -440\n",
      "yay -2240\n",
      "yay -2300\n",
      "yay -2360\n",
      "complaint -460\n",
      "epoch 800 loss = 990.4689525067806; mean_loss= 994.4615518590572; ()\n",
      "mode_hat tensor(1.1238, requires_grad=True)\n",
      "ltscale_hat tensor(-3.0025, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.0356, requires_grad=True)\n",
      "yay -2420\n",
      "complaint -480\n",
      "yay -2480\n",
      "complaint -500\n",
      "yay -2540\n",
      "complaint -520\n",
      "yay -2600\n",
      "yay -2660\n",
      "complaint -540\n",
      "epoch 900 loss = 991.3922256827354; mean_loss= 994.1832583124233; ()\n",
      "mode_hat tensor(1.1429, requires_grad=True)\n",
      "ltscale_hat tensor(-3.2254, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2279, requires_grad=True)\n",
      "yay -2720\n",
      "complaint -560\n",
      "yay -2780\n",
      "complaint -580\n",
      "yay -2840\n",
      "complaint -600\n",
      "yay -2900\n",
      "complaint -620\n",
      "yay -2960\n",
      "complaint -640\n",
      "epoch 1000 loss = 992.6394044458866; mean_loss= 992.9297716845676; ()\n",
      "mode_hat tensor(1.1427, requires_grad=True)\n",
      "ltscale_hat tensor(-3.5730, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.3214, requires_grad=True)\n",
      "yay -3020\n",
      "yay -3080\n",
      "complaint -660\n",
      "yay -3140\n",
      "complaint -680\n",
      "yay -3200\n",
      "yay -3260\n",
      "epoch 1100 loss = 992.630359351635; mean_loss= 993.4780275178797; ()\n",
      "mode_hat tensor(1.1523, requires_grad=True)\n",
      "ltscale_hat tensor(-3.7701, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4303, requires_grad=True)\n",
      "yay -3320\n",
      "complaint -700\n",
      "yay -3380\n",
      "Final mean_losses: 995.986481958312\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6052, -4.6052], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-1.4916037321090698\n",
      "ltscale_hat:\n",
      "-3.8204824924468994\n",
      "mode_hat:\n",
      "1.1724658012390137\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 399 1.0025062656641603\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1052.768418341875; mean_loss= 1052.768418341875; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-767.4430]], grad_fn=<IndexBackward>) tensor([[770.5748]], grad_fn=<IndexBackward>) tensor([[-266.9893]], grad_fn=<IndexBackward>) tensor([[263.9139]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0564]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-770.3437]], grad_fn=<IndexBackward>) tensor([[773.3556]], grad_fn=<IndexBackward>) tensor([[-266.3785]], grad_fn=<IndexBackward>) tensor([[263.2410]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1256]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-771.1912]], grad_fn=<IndexBackward>) tensor([[774.3431]], grad_fn=<IndexBackward>) tensor([[-266.0089]], grad_fn=<IndexBackward>) tensor([[262.9100]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0530]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-773.9304]], grad_fn=<IndexBackward>) tensor([[777.0186]], grad_fn=<IndexBackward>) tensor([[-265.4041]], grad_fn=<IndexBackward>) tensor([[262.2628]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0530]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-774.8071]], grad_fn=<IndexBackward>) tensor([[778.0271]], grad_fn=<IndexBackward>) tensor([[-265.0512]], grad_fn=<IndexBackward>) tensor([[261.9452]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1140]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-775.8734]], grad_fn=<IndexBackward>) tensor([[779.1630]], grad_fn=<IndexBackward>) tensor([[-264.7224]], grad_fn=<IndexBackward>) tensor([[261.6300]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1972]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-777.3339]], grad_fn=<IndexBackward>) tensor([[780.5632]], grad_fn=<IndexBackward>) tensor([[-264.4417]], grad_fn=<IndexBackward>) tensor([[261.3183]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1060]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1.. tensor([[-777.5010]], grad_fn=<IndexBackward>) tensor([[781.0977]], grad_fn=<IndexBackward>) tensor([[-264.0170]], grad_fn=<IndexBackward>) tensor([[261.0092]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.5889]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-782.1295]], grad_fn=<IndexBackward>) tensor([[784.6190]], grad_fn=<IndexBackward>) tensor([[-264.0988]], grad_fn=<IndexBackward>) tensor([[260.7031]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.9061]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-782.3223]], grad_fn=<IndexBackward>) tensor([[785.1725]], grad_fn=<IndexBackward>) tensor([[-263.6932]], grad_fn=<IndexBackward>) tensor([[260.4102]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.4328]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1020.7107657194138; mean_loss= 1032.9267622247387; ()\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3012, requires_grad=True)\n",
      "yay -320\n",
      "complaint -20\n",
      "yay -380\n",
      "yay -440\n",
      "complaint -40\n",
      "yay -500\n",
      "complaint -60\n",
      "yay -560\n",
      "epoch 200 loss = 1004.2963710427284; mean_loss= 1011.2952243355572; ()\n",
      "mode_hat tensor(1.0028, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0046, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0839, requires_grad=True)\n",
      "complaint -80\n",
      "yay -620\n",
      "yay -680\n",
      "complaint -100\n",
      "yay -740\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 999.2275390923023; mean_loss= 1001.702372689209; ()\n",
      "mode_hat tensor(1.1296, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4925, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5591, requires_grad=True)\n",
      "complaint -120\n",
      "yay -920\n",
      "yay -980\n",
      "complaint -140\n",
      "yay -1040\n",
      "complaint -160\n",
      "yay -1100\n",
      "complaint -180\n",
      "yay -1160\n",
      "epoch 400 loss = 996.60504296422; mean_loss= 998.0584692539067; ()\n",
      "mode_hat tensor(1.1318, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9785, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.9114, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "complaint -200\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 994.4891086518764; mean_loss= 995.6299657964403; ()\n",
      "mode_hat tensor(1.1297, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4699, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2875, requires_grad=True)\n",
      "yay -1520\n",
      "complaint -220\n",
      "yay -1580\n",
      "complaint -240\n",
      "yay -1640\n",
      "yay -1700\n",
      "complaint -260\n",
      "yay -1760\n",
      "complaint -280\n",
      "epoch 600 loss = 994.1978534758091; mean_loss= 994.5887454114111; ()\n",
      "mode_hat tensor(1.1291, requires_grad=True)\n",
      "ltscale_hat tensor(-2.9617, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.3380, requires_grad=True)\n",
      "yay -1820\n",
      "complaint -300\n",
      "yay -1880\n",
      "yay -1940\n",
      "yay -2000\n",
      "complaint -320\n",
      "yay -2060\n",
      "epoch 700 loss = 993.5155774354935; mean_loss= 993.3790101522408; ()\n",
      "mode_hat tensor(1.1301, requires_grad=True)\n",
      "ltscale_hat tensor(-3.4580, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5152, requires_grad=True)\n",
      "yay -2120\n",
      "complaint -340\n",
      "yay -2180\n",
      "complaint -360\n",
      "yay -2240\n",
      "complaint -380\n",
      "yay -2300\n",
      "complaint -400\n",
      "yay -2360\n",
      "complaint -420\n",
      "epoch 800 loss = 989.2067228257656; mean_loss= 992.5553100137726; ()\n",
      "mode_hat tensor(1.1322, requires_grad=True)\n",
      "ltscale_hat tensor(-3.9550, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5991, requires_grad=True)\n",
      "yay -2420\n",
      "complaint -440\n",
      "yay -2480\n",
      "complaint -460\n",
      "yay -2540\n",
      "complaint -480\n",
      "yay -2600\n",
      "complaint -500\n",
      "yay -2660\n",
      "complaint -520\n",
      "epoch 900 loss = 990.1038851439953; mean_loss= 991.7676822367263; ()\n",
      "mode_hat tensor(1.1308, requires_grad=True)\n",
      "ltscale_hat tensor(-4.4529, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5912, requires_grad=True)\n",
      "yay -2720\n",
      "complaint -540\n",
      "yay -2780\n",
      "complaint -560\n",
      "yay -2840\n",
      "complaint -580\n",
      "yay -2900\n",
      "complaint -600\n",
      "yay -2960\n",
      "complaint -620\n",
      "epoch 1000 loss = 991.2671104073524; mean_loss= 991.0751831393608; ()\n",
      "mode_hat tensor(1.1310, requires_grad=True)\n",
      "ltscale_hat tensor(-4.9513, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5733, requires_grad=True)\n",
      "yay -3020\n",
      "complaint -640\n",
      "yay -3080\n",
      "complaint -660\n",
      "yay -3140\n",
      "complaint -680\n",
      "yay -3200\n",
      "complaint -700\n",
      "yay -3260\n",
      "complaint -720\n",
      "epoch 1100 loss = 988.0034666657448; mean_loss= 990.1177525185001; ()\n",
      "mode_hat tensor(1.1284, requires_grad=True)\n",
      "ltscale_hat tensor(-5.4502, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5993, requires_grad=True)\n",
      "yay -3320\n",
      "complaint -740\n",
      "yay -3380\n",
      "complaint -760\n",
      "yay -3440\n",
      "complaint -780\n",
      "yay -3500\n",
      "complaint -800\n",
      "yay -3560\n",
      "complaint -820\n",
      "epoch 1200 loss = 988.6486939787865; mean_loss= 989.6495783518942; ()\n",
      "mode_hat tensor(1.1313, requires_grad=True)\n",
      "ltscale_hat tensor(-5.9490, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6146, requires_grad=True)\n",
      "yay -3620\n",
      "complaint -840\n",
      "yay -3680\n",
      "complaint -860\n",
      "yay -3740\n",
      "complaint -880\n",
      "yay -3800\n",
      "complaint -900\n",
      "yay -3860\n",
      "epoch 1300 loss = 989.4171867966652; mean_loss= 988.9596414243705; ()\n",
      "mode_hat tensor(1.1334, requires_grad=True)\n",
      "ltscale_hat tensor(-6.4477, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6047, requires_grad=True)\n",
      "yay -3920\n",
      "complaint -920\n",
      "yay -3980\n",
      "complaint -940\n",
      "yay -4040\n",
      "complaint -960\n",
      "yay -4100\n",
      "complaint -980\n",
      "yay -4160\n",
      "complaint -1000\n",
      "epoch 1400 loss = 986.7144064605236; mean_loss= 988.4489190801723; ()\n",
      "mode_hat tensor(1.1360, requires_grad=True)\n",
      "ltscale_hat tensor(-6.9463, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6155, requires_grad=True)\n",
      "yay -4220\n",
      "complaint -1020\n",
      "yay -4280\n",
      "complaint -1040\n",
      "yay -4340\n",
      "complaint -1060\n",
      "yay -4400\n",
      "complaint -1080\n",
      "yay -4460\n",
      "complaint -1100\n",
      "epoch 1500 loss = 988.2895239889622; mean_loss= 987.9730020037464; ()\n",
      "mode_hat tensor(1.1380, requires_grad=True)\n",
      "ltscale_hat tensor(-7.4447, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6143, requires_grad=True)\n",
      "yay -4520\n",
      "complaint -1120\n",
      "yay -4580\n",
      "complaint -1140\n",
      "yay -4640\n",
      "complaint -1160\n",
      "yay -4700\n",
      "complaint -1180\n",
      "yay -4760\n",
      "epoch 1600 loss = 985.6938277482986; mean_loss= 987.1958941682418; ()\n",
      "mode_hat tensor(1.1308, requires_grad=True)\n",
      "ltscale_hat tensor(-7.9430, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6195, requires_grad=True)\n",
      "complaint -1200\n",
      "yay -4820\n",
      "complaint -1220\n",
      "yay -4880\n",
      "complaint -1240\n",
      "yay -4940\n",
      "complaint -1260\n",
      "yay -5000\n",
      "complaint -1280\n",
      "yay -5060\n",
      "epoch 1700 loss = 987.3049658238888; mean_loss= 986.7559397334361; ()\n",
      "mode_hat tensor(1.1347, requires_grad=True)\n",
      "ltscale_hat tensor(-8.4415, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6366, requires_grad=True)\n",
      "complaint -1300\n",
      "yay -5120\n",
      "complaint -1320\n",
      "yay -5180\n",
      "complaint -1340\n",
      "yay -5240\n",
      "complaint -1360\n",
      "yay -5300\n",
      "complaint -1380\n",
      "yay -5360\n",
      "epoch 1800 loss = 986.1584701836109; mean_loss= 986.3387511535725; ()\n",
      "mode_hat tensor(1.1298, requires_grad=True)\n",
      "ltscale_hat tensor(-8.9402, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.7504, requires_grad=True)\n",
      "complaint -1400\n",
      "yay -5420\n",
      "complaint -1420\n",
      "yay -5480\n",
      "complaint -1440\n",
      "yay -5540\n",
      "complaint -1460\n",
      "yay -5600\n",
      "complaint -1480\n",
      "yay -5660\n",
      "epoch 1900 loss = 980.7862365841866; mean_loss= 985.6228393936558; ()\n",
      "mode_hat tensor(1.1320, requires_grad=True)\n",
      "ltscale_hat tensor(-9.4391, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.7317, requires_grad=True)\n",
      "yay -5720\n",
      "complaint -1500\n",
      "yay -5780\n",
      "complaint -1520\n",
      "yay -5840\n",
      "complaint -1540\n",
      "yay -5900\n",
      "complaint -1560\n",
      "yay -5960\n",
      "complaint -1580\n",
      "epoch 2000 loss = 985.5440281629562; mean_loss= 985.502604745348; ()\n",
      "mode_hat tensor(1.1272, requires_grad=True)\n",
      "ltscale_hat tensor(-9.9383, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.7604, requires_grad=True)\n",
      "Final mean_losses: 985.502604745348\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6052, -4.6052], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-1.760446548461914\n",
      "ltscale_hat:\n",
      "-9.93831729888916\n",
      "mode_hat:\n",
      "1.1271693706512451\n",
      "0 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 100 4.0\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-895.4459],\n",
      "        [1024.9220],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-877.5406]], grad_fn=<IndexBackward>) tensor([[  928.3318],\n",
      "        [-1034.9326],\n",
      "        [  893.7292]], grad_fn=<IndexBackward>) tensor([[-327.0853],\n",
      "        [ 309.6724],\n",
      "        [-302.0246]], grad_fn=<IndexBackward>) tensor([[ 294.1312],\n",
      "        [-299.6739],\n",
      "        [ 287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0682],\n",
      "        [-0.0122],\n",
      "        [ 1.1706]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-895.4459],\n",
      "        [1024.9220],\n",
      "        [-878.7259]], grad_fn=<IndexBackward>) tensor([[  928.3318],\n",
      "        [-1034.9326],\n",
      "        [  894.5338]], grad_fn=<IndexBackward>) tensor([[-327.0853],\n",
      "        [ 309.6724],\n",
      "        [-302.1605]], grad_fn=<IndexBackward>) tensor([[ 294.1312],\n",
      "        [-299.6739],\n",
      "        [ 287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0682],\n",
      "        [-0.0122],\n",
      "        [ 0.6540]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[ 32],\n",
      "        [ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[-895.4459],\n",
      "        [1024.9220],\n",
      "        [-877.5406]], grad_fn=<IndexBackward>) tensor([[  928.3318],\n",
      "        [-1034.9326],\n",
      "        [  893.7292]], grad_fn=<IndexBackward>) tensor([[-327.0853],\n",
      "        [ 309.6724],\n",
      "        [-302.0246]], grad_fn=<IndexBackward>) tensor([[ 294.1312],\n",
      "        [-299.6739],\n",
      "        [ 287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0682],\n",
      "        [-0.0122],\n",
      "        [ 1.1706]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1147.0951573650043; mean_loss= 1147.0951573650043; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 6 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-896.0994],\n",
      "        [-877.5590]], grad_fn=<IndexBackward>) tensor([[929.2481],\n",
      "        [894.2000]], grad_fn=<IndexBackward>) tensor([[-326.6429],\n",
      "        [-301.4829]], grad_fn=<IndexBackward>) tensor([[293.6870],\n",
      "        [286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1929],\n",
      "        [1.7348]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-896.0994],\n",
      "        [-878.7443]], grad_fn=<IndexBackward>) tensor([[929.2481],\n",
      "        [895.0050]], grad_fn=<IndexBackward>) tensor([[-326.6429],\n",
      "        [-301.6186]], grad_fn=<IndexBackward>) tensor([[293.6870],\n",
      "        [286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1929],\n",
      "        [1.2188]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-896.0994],\n",
      "        [-877.5590]], grad_fn=<IndexBackward>) tensor([[929.2481],\n",
      "        [894.2000]], grad_fn=<IndexBackward>) tensor([[-326.6429],\n",
      "        [-301.4829]], grad_fn=<IndexBackward>) tensor([[293.6870],\n",
      "        [286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1929],\n",
      "        [1.7348]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "complaint 3 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-897.8873],\n",
      "        [-883.6049]], grad_fn=<IndexBackward>) tensor([[930.9492],\n",
      "        [898.7628]], grad_fn=<IndexBackward>) tensor([[-326.3438],\n",
      "        [-301.6346]], grad_fn=<IndexBackward>) tensor([[293.2483],\n",
      "        [286.1522]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0337],\n",
      "        [-0.3246]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-897.8873],\n",
      "        [-883.6049]], grad_fn=<IndexBackward>) tensor([[930.9492],\n",
      "        [898.7628]], grad_fn=<IndexBackward>) tensor([[-326.3438],\n",
      "        [-301.6346]], grad_fn=<IndexBackward>) tensor([[293.2483],\n",
      "        [286.1522]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0337],\n",
      "        [-0.3246]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-897.8873],\n",
      "        [-883.6049]], grad_fn=<IndexBackward>) tensor([[930.9492],\n",
      "        [898.7628]], grad_fn=<IndexBackward>) tensor([[-326.3438],\n",
      "        [-301.6346]], grad_fn=<IndexBackward>) tensor([[293.2483],\n",
      "        [286.1522]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0337],\n",
      "        [-0.3246]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 0 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-899.2991],\n",
      "        [-883.6244]], grad_fn=<IndexBackward>) tensor([[932.3906],\n",
      "        [899.2364]], grad_fn=<IndexBackward>) tensor([[-325.9933],\n",
      "        [-301.0933]], grad_fn=<IndexBackward>) tensor([[292.8048],\n",
      "        [285.7230]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0970],\n",
      "        [ 0.2417]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "complaint -20\n",
      "yay -20\n",
      "yay -80\n",
      "complaint -40\n",
      "complaint -60\n",
      "complaint -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1035.411348859469; mean_loss= 1125.2601769914543; ()\n",
      "mode_hat tensor(0.3625, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4947, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2689, requires_grad=True)\n",
      "yay -320\n",
      "complaint -100\n",
      "complaint -120\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -140\n",
      "complaint -160\n",
      "complaint -180\n",
      "yay -560\n",
      "complaint -200\n",
      "complaint -220\n",
      "epoch 200 loss = 1094.1585824886956; mean_loss= 1105.9148086447399; ()\n",
      "mode_hat tensor(0.6914, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9529, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1483, requires_grad=True)\n",
      "complaint -240\n",
      "yay -620\n",
      "complaint -260\n",
      "complaint -280\n",
      "yay -680\n",
      "complaint -300\n",
      "complaint -320\n",
      "complaint -340\n",
      "yay -740\n",
      "complaint -360\n",
      "complaint -380\n",
      "complaint -400\n",
      "yay -800\n",
      "complaint -420\n",
      "complaint -440\n",
      "yay -860\n",
      "complaint -460\n",
      "complaint -480\n",
      "epoch 300 loss = 1082.3545976877213; mean_loss= 1094.8547390333586; ()\n",
      "mode_hat tensor(0.8554, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3808, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0605, requires_grad=True)\n",
      "complaint -500\n",
      "yay -920\n",
      "complaint -520\n",
      "yay -980\n",
      "complaint -540\n",
      "complaint -560\n",
      "complaint -580\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 1087.588441689809; mean_loss= 1096.7722491669122; ()\n",
      "mode_hat tensor(0.9733, requires_grad=True)\n",
      "ltscale_hat tensor(-1.7674, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2935, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 1107.3921981255212; mean_loss= 1100.4204151002048; ()\n",
      "mode_hat tensor(1.0471, requires_grad=True)\n",
      "ltscale_hat tensor(-2.1159, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.4788, requires_grad=True)\n",
      "yay -1520\n",
      "yay -1580\n",
      "Final mean_losses: 1191.132503198232\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.5700, -4.4365], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.5321904420852661\n",
      "ltscale_hat:\n",
      "-2.185431718826294\n",
      "mode_hat:\n",
      "1.041121482849121\n",
      "0 3 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 3 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 100 4.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1148.6960036158562; mean_loss= 1148.6960036158562; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-982.0763],\n",
      "        [-895.4288]], grad_fn=<IndexBackward>) tensor([[987.7744],\n",
      "        [906.2983]], grad_fn=<IndexBackward>) tensor([[-299.3956],\n",
      "        [-297.3365]], grad_fn=<IndexBackward>) tensor([[293.6870],\n",
      "        [286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0105],\n",
      "        [ 0.1096]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-982.0763],\n",
      "        [-895.4288]], grad_fn=<IndexBackward>) tensor([[987.7744],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [906.2983]], grad_fn=<IndexBackward>) tensor([[-299.3956],\n",
      "        [-297.3365]], grad_fn=<IndexBackward>) tensor([[293.6870],\n",
      "        [286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0105],\n",
      "        [ 0.1096]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-982.0763],\n",
      "        [-895.4288]], grad_fn=<IndexBackward>) tensor([[987.7744],\n",
      "        [906.2983]], grad_fn=<IndexBackward>) tensor([[-299.3956],\n",
      "        [-297.3365]], grad_fn=<IndexBackward>) tensor([[293.6870],\n",
      "        [286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0105],\n",
      "        [ 0.1096]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-767.3843]], grad_fn=<IndexBackward>) tensor([[771.7927]], grad_fn=<IndexBackward>) tensor([[-266.6426]], grad_fn=<IndexBackward>) tensor([[262.1648]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0693]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-767.3843]], grad_fn=<IndexBackward>) tensor([[771.7927]], grad_fn=<IndexBackward>) tensor([[-266.6426]], grad_fn=<IndexBackward>) tensor([[262.1648]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0693]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-767.3843]], grad_fn=<IndexBackward>) tensor([[771.7927]], grad_fn=<IndexBackward>) tensor([[-266.6426]], grad_fn=<IndexBackward>) tensor([[262.1648]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0693]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-770.7404]], grad_fn=<IndexBackward>) tensor([[775.3023]], grad_fn=<IndexBackward>) tensor([[-265.4931]], grad_fn=<IndexBackward>) tensor([[261.0273]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0961]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-770.7404]], grad_fn=<IndexBackward>) tensor([[775.3023]], grad_fn=<IndexBackward>) tensor([[-265.4931]], grad_fn=<IndexBackward>) tensor([[261.0273]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0961]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-770.7404]], grad_fn=<IndexBackward>) tensor([[775.3023]], grad_fn=<IndexBackward>) tensor([[-265.4931]], grad_fn=<IndexBackward>) tensor([[261.0273]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0961]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-773.1738]], grad_fn=<IndexBackward>) tensor([[777.7758]], grad_fn=<IndexBackward>) tensor([[-264.8019]], grad_fn=<IndexBackward>) tensor([[260.3217]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1218]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "complaint -20\n",
      "yay -80\n",
      "complaint -40\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "complaint -60\n",
      "complaint -80\n",
      "epoch 100 loss = 1095.6900973916054; mean_loss= 1120.7735852390504; ()\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4950, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2589, requires_grad=True)\n",
      "complaint -100\n",
      "yay -320\n",
      "complaint -120\n",
      "complaint -140\n",
      "complaint -160\n",
      "yay -380\n",
      "complaint -180\n",
      "complaint -200\n",
      "yay -440\n",
      "complaint -220\n",
      "complaint -240\n",
      "complaint -260\n",
      "yay -500\n",
      "complaint -280\n",
      "complaint -300\n",
      "yay -560\n",
      "complaint -320\n",
      "epoch 200 loss = 1068.5306352376938; mean_loss= 1097.4457112498606; ()\n",
      "mode_hat tensor(0.9712, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9678, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1363, requires_grad=True)\n",
      "complaint -340\n",
      "yay -620\n",
      "complaint -360\n",
      "complaint -380\n",
      "complaint -400\n",
      "yay -680\n",
      "complaint -420\n",
      "complaint -440\n",
      "complaint -460\n",
      "yay -740\n",
      "yay -800\n",
      "complaint -480\n",
      "yay -860\n",
      "complaint -500\n",
      "complaint -520\n",
      "epoch 300 loss = 1102.7691805958748; mean_loss= 1088.007956864764; ()\n",
      "mode_hat tensor(1.1118, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4254, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0891, requires_grad=True)\n",
      "yay -920\n",
      "complaint -540\n",
      "yay -980\n",
      "complaint -560\n",
      "complaint -580\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 1040.3811345100403; mean_loss= 1082.6242144929902; ()\n",
      "mode_hat tensor(1.1151, requires_grad=True)\n",
      "ltscale_hat tensor(-1.8598, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.3454, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -600\n",
      "complaint -620\n",
      "yay -1280\n",
      "complaint -640\n",
      "complaint -660\n",
      "yay -1340\n",
      "complaint -680\n",
      "yay -1400\n",
      "complaint -700\n",
      "complaint -720\n",
      "complaint -740\n",
      "yay -1460\n",
      "complaint -760\n",
      "epoch 500 loss = 1125.8612905144691; mean_loss= 1082.02633188663; ()\n",
      "mode_hat tensor(1.1202, requires_grad=True)\n",
      "ltscale_hat tensor(-2.2779, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5763, requires_grad=True)\n",
      "complaint -780\n",
      "complaint -800\n",
      "yay -1520\n",
      "complaint -820\n",
      "complaint -840\n",
      "yay -1580\n",
      "complaint -860\n",
      "complaint -880\n",
      "yay -1640\n",
      "complaint -900\n",
      "complaint -920\n",
      "complaint -940\n",
      "yay -1700\n",
      "complaint -960\n",
      "complaint -980\n",
      "complaint -1000\n",
      "yay -1760\n",
      "complaint -1020\n",
      "epoch 600 loss = 1066.9284659028053; mean_loss= 1078.8889890143555; ()\n",
      "mode_hat tensor(1.1292, requires_grad=True)\n",
      "ltscale_hat tensor(-2.7225, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.7533, requires_grad=True)\n",
      "complaint -1040\n",
      "yay -1820\n",
      "complaint -1060\n",
      "yay -1880\n",
      "complaint -1080\n",
      "yay -1940\n",
      "complaint -1100\n",
      "complaint -1120\n",
      "complaint -1140\n",
      "yay -2000\n",
      "complaint -1160\n",
      "complaint -1180\n",
      "yay -2060\n",
      "complaint -1200\n",
      "epoch 700 loss = 1056.027327477932; mean_loss= 1077.3101689577352; ()\n",
      "mode_hat tensor(1.1245, requires_grad=True)\n",
      "ltscale_hat tensor(-3.1635, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.9391, requires_grad=True)\n",
      "yay -2120\n",
      "complaint -1220\n",
      "complaint -1240\n",
      "yay -2180\n",
      "complaint -1260\n",
      "complaint -1280\n",
      "complaint -1300\n",
      "yay -2240\n",
      "complaint -1320\n",
      "complaint -1340\n",
      "complaint -1360\n",
      "yay -2300\n",
      "complaint -1380\n",
      "complaint -1400\n",
      "complaint -1420\n",
      "yay -2360\n",
      "complaint -1440\n",
      "complaint -1460\n",
      "epoch 800 loss = 1000.5596666137378; mean_loss= 1075.4433282674954; ()\n",
      "mode_hat tensor(1.1209, requires_grad=True)\n",
      "ltscale_hat tensor(-3.6275, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.0790, requires_grad=True)\n",
      "complaint -1480\n",
      "yay -2420\n",
      "complaint -1500\n",
      "complaint -1520\n",
      "complaint -1540\n",
      "yay -2480\n",
      "complaint -1560\n",
      "complaint -1580\n",
      "complaint -1600\n",
      "yay -2540\n",
      "complaint -1620\n",
      "yay -2600\n",
      "complaint -1640\n",
      "yay -2660\n",
      "epoch 900 loss = 1066.443011045456; mean_loss= 1075.6825793026176; ()\n",
      "mode_hat tensor(1.1241, requires_grad=True)\n",
      "ltscale_hat tensor(-4.0831, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.1912, requires_grad=True)\n",
      "yay -2720\n",
      "yay -2780\n",
      "yay -2840\n",
      "yay -2900\n",
      "yay -2960\n",
      "epoch 1000 loss = 1054.1495082974434; mean_loss= 1074.8686256817662; ()\n",
      "mode_hat tensor(1.1414, requires_grad=True)\n",
      "ltscale_hat tensor(-4.5578, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2884, requires_grad=True)\n",
      "yay -3020\n",
      "yay -3080\n",
      "yay -3140\n",
      "complaint -1660\n",
      "yay -3200\n",
      "complaint -1680\n",
      "complaint -1700\n",
      "yay -3260\n",
      "complaint -1720\n",
      "epoch 1100 loss = 1114.2130104899406; mean_loss= 1075.0393742006208; ()\n",
      "mode_hat tensor(1.1337, requires_grad=True)\n",
      "ltscale_hat tensor(-5.0486, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.3500, requires_grad=True)\n",
      "complaint -1740\n",
      "yay -3320\n",
      "complaint -1760\n",
      "complaint -1780\n",
      "yay -3380\n",
      "complaint -1800\n",
      "complaint -1820\n",
      "yay -3440\n",
      "complaint -1840\n",
      "complaint -1860\n",
      "complaint -1880\n",
      "yay -3500\n",
      "complaint -1900\n",
      "complaint -1920\n",
      "complaint -1940\n",
      "yay -3560\n",
      "complaint -1960\n",
      "epoch 1200 loss = 1134.1443085670471; mean_loss= 1074.8076844512048; ()\n",
      "mode_hat tensor(1.1205, requires_grad=True)\n",
      "ltscale_hat tensor(-5.5407, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.3793, requires_grad=True)\n",
      "complaint -1980\n",
      "complaint -2000\n",
      "yay -3620\n",
      "complaint -2020\n",
      "complaint -2040\n",
      "complaint -2060\n",
      "yay -3680\n",
      "complaint -2080\n",
      "complaint -2100\n",
      "complaint -2120\n",
      "yay -3740\n",
      "complaint -2140\n",
      "complaint -2160\n",
      "complaint -2180\n",
      "yay -3800\n",
      "complaint -2200\n",
      "complaint -2220\n",
      "complaint -2240\n",
      "yay -3860\n",
      "complaint -2260\n",
      "epoch 1300 loss = 1085.323932826519; mean_loss= 1073.1975881272208; ()\n",
      "mode_hat tensor(1.1323, requires_grad=True)\n",
      "ltscale_hat tensor(-6.0267, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4198, requires_grad=True)\n",
      "complaint -2280\n",
      "complaint -2300\n",
      "yay -3920\n",
      "complaint -2320\n",
      "complaint -2340\n",
      "complaint -2360\n",
      "yay -3980\n",
      "complaint -2380\n",
      "complaint -2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complaint -2420\n",
      "yay -4040\n",
      "complaint -2440\n",
      "complaint -2460\n",
      "complaint -2480\n",
      "yay -4100\n",
      "complaint -2500\n",
      "complaint -2520\n",
      "complaint -2540\n",
      "yay -4160\n",
      "complaint -2560\n",
      "epoch 1400 loss = 1132.2261006037395; mean_loss= 1073.4304281670506; ()\n",
      "mode_hat tensor(1.1120, requires_grad=True)\n",
      "ltscale_hat tensor(-6.5097, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4668, requires_grad=True)\n",
      "complaint -2580\n",
      "complaint -2600\n",
      "yay -4220\n",
      "complaint -2620\n",
      "complaint -2640\n",
      "complaint -2660\n",
      "yay -4280\n",
      "complaint -2680\n",
      "complaint -2700\n",
      "complaint -2720\n",
      "yay -4340\n",
      "complaint -2740\n",
      "complaint -2760\n",
      "complaint -2780\n",
      "yay -4400\n",
      "complaint -2800\n",
      "complaint -2820\n",
      "complaint -2840\n",
      "yay -4460\n",
      "complaint -2860\n",
      "epoch 1500 loss = 1140.6264453530312; mean_loss= 1073.1328805845872; ()\n",
      "mode_hat tensor(1.1324, requires_grad=True)\n",
      "ltscale_hat tensor(-6.9530, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4922, requires_grad=True)\n",
      "Final mean_losses: 1073.1328805845872\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6455, -6.3313], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-1.4921988248825073\n",
      "ltscale_hat:\n",
      "-6.9529876708984375\n",
      "mode_hat:\n",
      "1.1324396133422852\n",
      "0 3 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1121.73788523674; mean_loss= 1121.73788523674; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-964.5464],\n",
      "        [-870.4843]], grad_fn=<IndexBackward>) tensor([[975.9847],\n",
      "        [889.3877]], grad_fn=<IndexBackward>) tensor([[-307.1472],\n",
      "        [-307.6026]], grad_fn=<IndexBackward>) tensor([[295.7195],\n",
      "        [288.5600]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0104],\n",
      "        [-0.1392]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-965.9786],\n",
      "        [-872.5975]], grad_fn=<IndexBackward>) tensor([[977.4391],\n",
      "        [891.2830]], grad_fn=<IndexBackward>) tensor([[-307.1964],\n",
      "        [-307.7636]], grad_fn=<IndexBackward>) tensor([[295.7469],\n",
      "        [288.5903]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0110],\n",
      "        [-0.4878]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 7 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-967.4871],\n",
      "        [-872.4608]], grad_fn=<IndexBackward>) tensor([[978.9456],\n",
      "        [891.6465]], grad_fn=<IndexBackward>) tensor([[-306.6624],\n",
      "        [-307.0791]], grad_fn=<IndexBackward>) tensor([[295.1903],\n",
      "        [288.0508]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0136],\n",
      "        [ 0.1573]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-968.9333],\n",
      "        [-877.2007]], grad_fn=<IndexBackward>) tensor([[980.4105],\n",
      "        [895.3312]], grad_fn=<IndexBackward>) tensor([[-306.1762],\n",
      "        [-307.0219]], grad_fn=<IndexBackward>) tensor([[294.6830],\n",
      "        [287.5594]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0160],\n",
      "        [-1.3322]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-877.1271],\n",
      "        [-756.5251]], grad_fn=<IndexBackward>) tensor([[895.7392],\n",
      "        [759.9279]], grad_fn=<IndexBackward>) tensor([[-306.4056],\n",
      "        [-272.1658]], grad_fn=<IndexBackward>) tensor([[287.0683],\n",
      "        [268.7833]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.7252],\n",
      "        [ 0.0204]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-971.7574],\n",
      "        [-879.0775]], grad_fn=<IndexBackward>) tensor([[983.2957],\n",
      "        [897.5253]], grad_fn=<IndexBackward>) tensor([[-305.7561],\n",
      "        [-306.5620]], grad_fn=<IndexBackward>) tensor([[294.2305],\n",
      "        [287.1248]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0127],\n",
      "        [-0.9893]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-972.7984],\n",
      "        [-879.4422]], grad_fn=<IndexBackward>) tensor([[984.3536],\n",
      "        [898.1063]], grad_fn=<IndexBackward>) tensor([[-305.6836],\n",
      "        [-306.4355]], grad_fn=<IndexBackward>) tensor([[294.1438],\n",
      "        [287.0428]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0154],\n",
      "        [-0.7286]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-974.0311],\n",
      "        [-879.4377]], grad_fn=<IndexBackward>) tensor([[985.5781],\n",
      "        [898.4704]], grad_fn=<IndexBackward>) tensor([[-305.3313],\n",
      "        [-305.9724]], grad_fn=<IndexBackward>) tensor([[293.7704],\n",
      "        [286.6812]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0140],\n",
      "        [-0.2585]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[ 32],\n",
      "        [145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-975.1755],\n",
      "        [-878.2905],\n",
      "        [-760.7383]], grad_fn=<IndexBackward>) tensor([[986.7693],\n",
      "        [898.0801],\n",
      "        [764.1543]], grad_fn=<IndexBackward>) tensor([[-304.9744],\n",
      "        [-305.3850],\n",
      "        [-271.5270]], grad_fn=<IndexBackward>) tensor([[293.4051],\n",
      "        [286.3278],\n",
      "        [268.1213]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0244],\n",
      "        [0.7323],\n",
      "        [0.0103]], grad_fn=<IndexBackward>)\n",
      "yay -20\n",
      "complaint 0 assert approx_eq: tensor([[ 32],\n",
      "        [145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-976.5573],\n",
      "        [-878.3470],\n",
      "        [-761.8127]], grad_fn=<IndexBackward>) tensor([[988.1399],\n",
      "        [898.5273],\n",
      "        [765.2452]], grad_fn=<IndexBackward>) tensor([[-304.6353],\n",
      "        [-304.9330],\n",
      "        [-271.2036]], grad_fn=<IndexBackward>) tensor([[293.0359],\n",
      "        [285.9706],\n",
      "        [267.7951]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0169],\n",
      "        [ 1.2179],\n",
      "        [ 0.0241]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "complaint -20\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1234.261866569519; mean_loss= 1121.9511127255453; ()\n",
      "mode_hat tensor(0.4113, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3253, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2196, requires_grad=True)\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -40\n",
      "yay -560\n",
      "complaint -60\n",
      "epoch 200 loss = 1039.4985098838806; mean_loss= 1076.5361385983663; ()\n",
      "mode_hat tensor(0.6657, requires_grad=True)\n",
      "ltscale_hat tensor(-0.6362, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3745, requires_grad=True)\n",
      "yay -620\n",
      "complaint -80\n",
      "yay -680\n",
      "yay -740\n",
      "complaint -100\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 829.104901432991; mean_loss= 1105.630216601575; ()\n",
      "mode_hat tensor(0.8598, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8775, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4370, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n",
      "complaint -120\n",
      "epoch 400 loss = 1098.0433638095856; mean_loss= 1115.2305963360136; ()\n",
      "mode_hat tensor(0.9853, requires_grad=True)\n",
      "ltscale_hat tensor(-1.1981, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4771, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -140\n",
      "yay -1280\n",
      "complaint -160\n",
      "yay -1340\n",
      "complaint -180\n",
      "yay -1400\n",
      "complaint -200\n",
      "yay -1460\n",
      "epoch 500 loss = 887.8575521707535; mean_loss= 1091.4233296509717; ()\n",
      "mode_hat tensor(1.0636, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4899, requires_grad=True)\n",
      "ldfraw_hat tensor(0.5374, requires_grad=True)\n",
      "complaint -220\n",
      "yay -1520\n",
      "yay -1580\n",
      "complaint -240\n",
      "Final mean_losses: 1139.2543085481252\n",
      "file exists: testresults/fit_unparametrized_laplace_10_parts1_N400_S10_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.5388, -4.4617], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.510834813117981\n",
      "ltscale_hat:\n",
      "-1.5666297674179077\n",
      "mode_hat:\n",
      "1.0562845468521118\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 10 40.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1056.615147471428; mean_loss= 1056.615147471428; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-767.6539]], grad_fn=<IndexBackward>) tensor([[770.1849]], grad_fn=<IndexBackward>) tensor([[-266.6241]], grad_fn=<IndexBackward>) tensor([[264.1754]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0823]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-768.7464]], grad_fn=<IndexBackward>) tensor([[771.2695]], grad_fn=<IndexBackward>) tensor([[-266.5505]], grad_fn=<IndexBackward>) tensor([[264.0947]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0674]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-770.9966]], grad_fn=<IndexBackward>) tensor([[773.5279]], grad_fn=<IndexBackward>) tensor([[-266.2102]], grad_fn=<IndexBackward>) tensor([[263.7464]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0674]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-772.1875]], grad_fn=<IndexBackward>) tensor([[774.7173]], grad_fn=<IndexBackward>) tensor([[-265.9217]], grad_fn=<IndexBackward>) tensor([[263.4527]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0608]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-776.3595]], grad_fn=<IndexBackward>) tensor([[778.7264]], grad_fn=<IndexBackward>) tensor([[-264.8303]], grad_fn=<IndexBackward>) tensor([[262.2886]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1748]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-777.6259]], grad_fn=<IndexBackward>) tensor([[779.9885]], grad_fn=<IndexBackward>) tensor([[-264.4013]], grad_fn=<IndexBackward>) tensor([[261.8521]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1865]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-782.1136]], grad_fn=<IndexBackward>) tensor([[783.4061]], grad_fn=<IndexBackward>) tensor([[-264.3752]], grad_fn=<IndexBackward>) tensor([[261.4556]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.6271]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-783.2827]], grad_fn=<IndexBackward>) tensor([[784.6085]], grad_fn=<IndexBackward>) tensor([[-263.9974]], grad_fn=<IndexBackward>) tensor([[261.0833]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.5883]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-782.5275]], grad_fn=<IndexBackward>) tensor([[784.5257]], grad_fn=<IndexBackward>) tensor([[-263.5821]], grad_fn=<IndexBackward>) tensor([[260.8913]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.6926]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-783.9431]], grad_fn=<IndexBackward>) tensor([[785.8950]], grad_fn=<IndexBackward>) tensor([[-263.3924]], grad_fn=<IndexBackward>) tensor([[260.6808]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.7597]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 992.68390583992; mean_loss= 1080.885677555624; ()\n",
      "mode_hat tensor(0.4249, requires_grad=True)\n",
      "ltscale_hat tensor(-0.3295, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1926, requires_grad=True)\n",
      "yay -320\n",
      "yay -380\n",
      "yay -440\n",
      "complaint -20\n",
      "yay -500\n",
      "complaint -40\n",
      "yay -560\n",
      "epoch 200 loss = 985.3052259683609; mean_loss= 1065.905102145786; ()\n",
      "mode_hat tensor(0.7155, requires_grad=True)\n",
      "ltscale_hat tensor(-0.6182, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3278, requires_grad=True)\n",
      "yay -620\n",
      "yay -680\n",
      "complaint -60\n",
      "yay -740\n",
      "yay -800\n",
      "complaint -80\n",
      "yay -860\n",
      "complaint -100\n",
      "epoch 300 loss = 1420.3143997192383; mean_loss= 1057.968721501359; ()\n",
      "mode_hat tensor(0.8800, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8648, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4146, requires_grad=True)\n",
      "yay -920\n",
      "complaint -120\n",
      "yay -980\n",
      "complaint -140\n",
      "yay -1040\n",
      "complaint -160\n",
      "yay -1100\n",
      "complaint -180\n",
      "yay -1160\n",
      "epoch 400 loss = 946.7213598489761; mean_loss= 1047.6779751886054; ()\n",
      "mode_hat tensor(0.9945, requires_grad=True)\n",
      "ltscale_hat tensor(-1.0871, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4567, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -200\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 1065.9284942150116; mean_loss= 1042.8877011485192; ()\n",
      "mode_hat tensor(1.0226, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3053, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4621, requires_grad=True)\n",
      "yay -1520\n",
      "yay -1580\n",
      "yay -1640\n",
      "complaint -220\n",
      "yay -1700\n",
      "complaint -240\n",
      "yay -1760\n",
      "complaint -260\n",
      "epoch 600 loss = 1073.9587156772614; mean_loss= 1051.6007107983812; ()\n",
      "mode_hat tensor(1.0726, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4993, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4380, requires_grad=True)\n",
      "yay -1820\n",
      "complaint -280\n",
      "yay -1880\n",
      "complaint -300\n",
      "yay -1940\n",
      "complaint -320\n",
      "yay -2000\n",
      "complaint -340\n",
      "yay -2060\n",
      "complaint -360\n",
      "epoch 700 loss = 1105.573759675026; mean_loss= 1061.3319005527724; ()\n",
      "mode_hat tensor(1.1103, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6601, requires_grad=True)\n",
      "ldfraw_hat tensor(0.4009, requires_grad=True)\n",
      "yay -2120\n",
      "complaint -380\n",
      "yay -2180\n",
      "complaint -400\n",
      "yay -2240\n",
      "Final mean_losses: 1052.616563668216\n",
      "file exists: testresults/fit_fully_amortized_10_parts1_N400_S10_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-5.5162, -5.7040], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "0.3755020797252655\n",
      "ltscale_hat:\n",
      "-1.7543729543685913\n",
      "mode_hat:\n",
      "1.1008480787277222\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-922.7437],\n",
      "        [-868.7129]], grad_fn=<IndexBackward>) tensor([[947.1041],\n",
      "        [887.7254]], grad_fn=<IndexBackward>) tensor([[-319.0163],\n",
      "        [-306.1700]], grad_fn=<IndexBackward>) tensor([[294.1312],\n",
      "        [287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.5247],\n",
      "        [-0.1508]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1086.2986956834793; mean_loss= 1086.2986956834793; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-922.7437],\n",
      "        [-874.7017]], grad_fn=<IndexBackward>) tensor([[947.5780],\n",
      "        [892.2580]], grad_fn=<IndexBackward>) tensor([[-318.4766],\n",
      "        [-306.3345]], grad_fn=<IndexBackward>) tensor([[293.6870],\n",
      "        [286.5767]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0448],\n",
      "        [-2.2015]], grad_fn=<IndexBackward>)\n",
      "yay 4 fix_m_grad\n",
      "complaint 7 assert approx_eq: tensor([[ 32],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [145]]) \n",
      "              1.. tensor([[-924.0392],\n",
      "        [-874.7198]], grad_fn=<IndexBackward>) tensor([[948.9390],\n",
      "        [892.7276]], grad_fn=<IndexBackward>) tensor([[-318.0920],\n",
      "        [-305.8055]], grad_fn=<IndexBackward>) tensor([[293.2487],\n",
      "        [286.1526]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0565],\n",
      "        [-1.6451]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 6 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-925.1658],\n",
      "        [-873.9899]], grad_fn=<IndexBackward>) tensor([[950.1849],\n",
      "        [892.6880]], grad_fn=<IndexBackward>) tensor([[-317.1382],\n",
      "        [-304.6594]], grad_fn=<IndexBackward>) tensor([[292.2919],\n",
      "        [285.2224]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.1729],\n",
      "        [-0.7389]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-927.0046],\n",
      "        [-872.0675]], grad_fn=<IndexBackward>) tensor([[951.9190],\n",
      "        [891.8349]], grad_fn=<IndexBackward>) tensor([[-316.2558],\n",
      "        [-303.3677]], grad_fn=<IndexBackward>) tensor([[291.3270],\n",
      "        [284.2845]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0144],\n",
      "        [ 0.6842]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-928.4280],\n",
      "        [-872.7783]], grad_fn=<IndexBackward>) tensor([[953.3689],\n",
      "        [892.7760]], grad_fn=<IndexBackward>) tensor([[-315.5262],\n",
      "        [-302.5757]], grad_fn=<IndexBackward>) tensor([[290.5512],\n",
      "        [283.5309]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0340],\n",
      "        [ 0.9530]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-877.0044]], grad_fn=<IndexBackward>) tensor([[896.1138]], grad_fn=<IndexBackward>) tensor([[-302.1073]], grad_fn=<IndexBackward>) tensor([[282.7004]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2975]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-931.0631],\n",
      "        [-877.9107]], grad_fn=<IndexBackward>) tensor([[956.1255],\n",
      "        [897.1895]], grad_fn=<IndexBackward>) tensor([[-313.9370],\n",
      "        [-301.3164]], grad_fn=<IndexBackward>) tensor([[288.9013],\n",
      "        [281.9278]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0267],\n",
      "        [-0.1099]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-932.5430],\n",
      "        [-878.5750]], grad_fn=<IndexBackward>) tensor([[957.6157],\n",
      "        [898.1006]], grad_fn=<IndexBackward>) tensor([[-313.3217],\n",
      "        [-300.6236]], grad_fn=<IndexBackward>) tensor([[288.2288],\n",
      "        [281.2750]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0203],\n",
      "        [ 0.1771]], grad_fn=<IndexBackward>)\n",
      "yay -20\n",
      "complaint 0 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-936.6620],\n",
      "        [-883.1268]], grad_fn=<IndexBackward>) tensor([[961.8694],\n",
      "        [902.5807]], grad_fn=<IndexBackward>) tensor([[-311.4128],\n",
      "        [-298.8109]], grad_fn=<IndexBackward>) tensor([[286.1886],\n",
      "        [279.2944]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0168],\n",
      "        [-0.0626]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1090.182412147522; mean_loss= 1090.3853298960885; ()\n",
      "mode_hat tensor(0.4963, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4871, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2136, requires_grad=True)\n",
      "yay -320\n",
      "complaint -20\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -40\n",
      "yay -560\n",
      "epoch 200 loss = 1020.133015871048; mean_loss= 1074.8637565956242; ()\n",
      "mode_hat tensor(0.8838, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9044, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1695, requires_grad=True)\n",
      "complaint -60\n",
      "yay -620\n",
      "complaint -80\n",
      "yay -680\n",
      "complaint -100\n",
      "yay -740\n",
      "complaint -120\n",
      "yay -800\n",
      "complaint -140\n",
      "yay -860\n",
      "epoch 300 loss = 1147.3091433644295; mean_loss= 1075.7685238849394; ()\n",
      "mode_hat tensor(1.0521, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2247, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0934, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "complaint -160\n",
      "yay -1160\n",
      "epoch 400 loss = 909.9058966636658; mean_loss= 1174.358963632103; ()\n",
      "mode_hat tensor(1.1304, requires_grad=True)\n",
      "ltscale_hat tensor(-1.5991, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1137, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -180\n",
      "yay -1280\n",
      "complaint -200\n",
      "yay -1340\n",
      "complaint -220\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 999.2793460488319; mean_loss= 1087.2510412241297; ()\n",
      "mode_hat tensor(1.0892, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9007, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2137, requires_grad=True)\n",
      "yay -1520\n",
      "yay -1580\n",
      "yay -1640\n",
      "complaint -240\n",
      "yay -1700\n",
      "complaint -260\n",
      "yay -1760\n",
      "epoch 600 loss = 1080.44632512331; mean_loss= 1073.9025846822742; ()\n",
      "mode_hat tensor(1.0856, requires_grad=True)\n",
      "ltscale_hat tensor(-2.1526, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.3462, requires_grad=True)\n",
      "complaint -280\n",
      "yay -1820\n",
      "yay -1880\n",
      "yay -1940\n",
      "yay -2000\n",
      "complaint -300\n",
      "yay -2060\n",
      "Final mean_losses: 1103.8908826749469\n",
      "file exists: testresults/fit_unparametrized_laplace_10_parts1_N400_S50_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.4826, -4.3193], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.4722622036933899\n",
      "ltscale_hat:\n",
      "-2.457211971282959\n",
      "mode_hat:\n",
      "1.0963984727859497\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 50 8.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1065.4173098802567; mean_loss= 1065.4173098802567; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-937.9924],\n",
      "        [-852.6430]], grad_fn=<IndexBackward>) tensor([[958.9471],\n",
      "        [878.0925]], grad_fn=<IndexBackward>) tensor([[-313.7769],\n",
      "        [-311.2563]], grad_fn=<IndexBackward>) tensor([[292.8120],\n",
      "        [285.7300]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0102],\n",
      "        [-0.0769]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-939.1649],\n",
      "        [-853.6526]], grad_fn=<IndexBackward>) tensor([[960.2256],\n",
      "        [879.2355]], grad_fn=<IndexBackward>) tensor([[-313.3709],\n",
      "        [-310.8672]], grad_fn=<IndexBackward>) tensor([[292.3794],\n",
      "        [285.3114]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0691],\n",
      "        [0.0270]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-940.7101],\n",
      "        [-854.8368]], grad_fn=<IndexBackward>) tensor([[961.7585],\n",
      "        [880.4987]], grad_fn=<IndexBackward>) tensor([[-313.0082],\n",
      "        [-310.5011]], grad_fn=<IndexBackward>) tensor([[291.9482],\n",
      "        [284.8942]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0116],\n",
      "        [ 0.0549]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-944.9337],\n",
      "        [-859.0829]], grad_fn=<IndexBackward>) tensor([[966.0795],\n",
      "        [884.7675]], grad_fn=<IndexBackward>) tensor([[-311.3979],\n",
      "        [-309.0164]], grad_fn=<IndexBackward>) tensor([[290.2029],\n",
      "        [283.2014]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0493],\n",
      "        [-0.1304]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-946.2611],\n",
      "        [-859.8564]], grad_fn=<IndexBackward>) tensor([[967.4660],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [885.7507]], grad_fn=<IndexBackward>) tensor([[-310.8516],\n",
      "        [-308.4438]], grad_fn=<IndexBackward>) tensor([[289.6209],\n",
      "        [282.6371]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0258],\n",
      "        [ 0.0876]], grad_fn=<IndexBackward>)\n",
      "yay -20\n",
      "complaint 4 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-947.6471],\n",
      "        [-865.6367]], grad_fn=<IndexBackward>) tensor([[968.8929],\n",
      "        [890.1700]], grad_fn=<IndexBackward>) tensor([[-310.1810],\n",
      "        [-308.3380]], grad_fn=<IndexBackward>) tensor([[288.9152],\n",
      "        [281.9518]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0200],\n",
      "        [-1.8529]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[ 32],\n",
      "        [145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-948.9473],\n",
      "        [-866.5234],\n",
      "        [-739.5901]], grad_fn=<IndexBackward>) tensor([[970.2617],\n",
      "        [891.2322],\n",
      "        [750.9979]], grad_fn=<IndexBackward>) tensor([[-309.5673],\n",
      "        [-307.7139],\n",
      "        [-274.8591]], grad_fn=<IndexBackward>) tensor([[288.2729],\n",
      "        [281.3284],\n",
      "        [263.4638]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0200],\n",
      "        [-1.6767],\n",
      "        [ 0.0125]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-950.3289],\n",
      "        [-865.1697]], grad_fn=<IndexBackward>) tensor([[971.6865],\n",
      "        [890.7578]], grad_fn=<IndexBackward>) tensor([[-309.0418],\n",
      "        [-306.9038]], grad_fn=<IndexBackward>) tensor([[287.7063],\n",
      "        [280.7789]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0221],\n",
      "        [-0.5369]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-951.7094],\n",
      "        [-866.1939]], grad_fn=<IndexBackward>) tensor([[973.1111],\n",
      "        [891.9149]], grad_fn=<IndexBackward>) tensor([[-308.5374],\n",
      "        [-306.3964]], grad_fn=<IndexBackward>) tensor([[287.1601],\n",
      "        [280.2493]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0244],\n",
      "        [-0.4261]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-864.9427]], grad_fn=<IndexBackward>) tensor([[891.5097]], grad_fn=<IndexBackward>) tensor([[-305.6734]], grad_fn=<IndexBackward>) tensor([[279.7690]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.6626]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "complaint -20\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1062.5257180929184; mean_loss= 1105.9107939018927; ()\n",
      "mode_hat tensor(0.4915, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4670, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1989, requires_grad=True)\n",
      "yay -320\n",
      "complaint -40\n",
      "yay -380\n",
      "complaint -60\n",
      "yay -440\n",
      "complaint -80\n",
      "yay -500\n",
      "yay -560\n",
      "complaint -100\n",
      "epoch 200 loss = 1078.0327826738358; mean_loss= 1092.700786666537; ()\n",
      "mode_hat tensor(0.9260, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9176, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1674, requires_grad=True)\n",
      "yay -620\n",
      "complaint -120\n",
      "yay -680\n",
      "yay -740\n",
      "complaint -140\n",
      "yay -800\n",
      "complaint -160\n",
      "yay -860\n",
      "complaint -180\n",
      "epoch 300 loss = 1088.2354007959366; mean_loss= 1084.9055526786613; ()\n",
      "mode_hat tensor(1.1140, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3245, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0328, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "complaint -200\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 1049.9880763292313; mean_loss= 1078.011023383784; ()\n",
      "mode_hat tensor(1.1416, requires_grad=True)\n",
      "ltscale_hat tensor(-1.7122, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1515, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -220\n",
      "yay -1280\n",
      "complaint -240\n",
      "yay -1340\n",
      "complaint -260\n",
      "yay -1400\n",
      "yay -1460\n",
      "complaint -280\n",
      "epoch 500 loss = 949.3389251232147; mean_loss= 1075.6844550503708; ()\n",
      "mode_hat tensor(1.1576, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0707, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.3330, requires_grad=True)\n",
      "Final mean_losses: 1075.6844550503708\n",
      "file exists: testresults/fit_fully_amortized_10_parts1_N400_S50_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6733, -4.8535], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.33300042152404785\n",
      "ltscale_hat:\n",
      "-2.0706729888916016\n",
      "mode_hat:\n",
      "1.1576186418533325\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 100 4.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1157.8412356972694; mean_loss= 1157.8412356972694; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "complaint 9 assert approx_eq: tensor([[ 32],\n",
      "        [145]]) \n",
      "              1.. tensor([[-994.9987],\n",
      "        [-859.2693]], grad_fn=<IndexBackward>) tensor([[996.9188],\n",
      "        [882.1841]], grad_fn=<IndexBackward>) tensor([[-295.0846],\n",
      "        [-309.0493]], grad_fn=<IndexBackward>) tensor([[293.1766],\n",
      "        [286.0822]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0120],\n",
      "        [-0.0523]], grad_fn=<IndexBackward>)\n",
      "yay 1 fix_m_grad\n",
      "complaint 8 assert approx_eq: tensor([[ 40],\n",
      "        [145]]) \n",
      "              1.. tensor([[1024.3091],\n",
      "        [-860.2003]], grad_fn=<IndexBackward>) tensor([[-1032.9956],\n",
      "        [  883.2734]], grad_fn=<IndexBackward>) tensor([[ 306.0052],\n",
      "        [-308.5988]], grad_fn=<IndexBackward>) tensor([[-297.3295],\n",
      "        [ 285.6202]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0108],\n",
      "        [ 0.0945]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-861.4008]], grad_fn=<IndexBackward>) tensor([[884.5477]], grad_fn=<IndexBackward>) tensor([[-308.1935]], grad_fn=<IndexBackward>) tensor([[285.1707]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1241]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-863.0824]], grad_fn=<IndexBackward>) tensor([[886.1517]], grad_fn=<IndexBackward>) tensor([[-307.8139]], grad_fn=<IndexBackward>) tensor([[284.6917]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0529]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-864.4437]], grad_fn=<IndexBackward>) tensor([[887.5369]], grad_fn=<IndexBackward>) tensor([[-307.4238]], grad_fn=<IndexBackward>) tensor([[284.2394]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0912]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-864.4037]], grad_fn=<IndexBackward>) tensor([[887.9633]], grad_fn=<IndexBackward>) tensor([[-306.8611]], grad_fn=<IndexBackward>) tensor([[283.7809]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.4794]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-869.1365]], grad_fn=<IndexBackward>) tensor([[891.6569]], grad_fn=<IndexBackward>) tensor([[-306.8553]], grad_fn=<IndexBackward>) tensor([[283.3166]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.0183]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-870.2251]], grad_fn=<IndexBackward>) tensor([[892.8571]], grad_fn=<IndexBackward>) tensor([[-306.3891]], grad_fn=<IndexBackward>) tensor([[282.8244]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.9327]], grad_fn=<IndexBackward>)\n",
      "yay -20\n",
      "complaint 1 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-868.9500]], grad_fn=<IndexBackward>) tensor([[892.4399]], grad_fn=<IndexBackward>) tensor([[-305.6398]], grad_fn=<IndexBackward>) tensor([[282.3268]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.1768]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[145]]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1.. tensor([[-868.9276]], grad_fn=<IndexBackward>) tensor([[892.8799]], grad_fn=<IndexBackward>) tensor([[-305.0917]], grad_fn=<IndexBackward>) tensor([[281.8798]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.7404]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "complaint -20\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1158.1069669127464; mean_loss= 1136.392046246907; ()\n",
      "mode_hat tensor(0.5026, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4928, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2653, requires_grad=True)\n",
      "yay -320\n",
      "complaint -40\n",
      "yay -380\n",
      "complaint -60\n",
      "yay -440\n",
      "complaint -80\n",
      "yay -500\n",
      "complaint -100\n",
      "yay -560\n",
      "epoch 200 loss = 1095.807698905468; mean_loss= 1114.5102142283822; ()\n",
      "mode_hat tensor(0.9149, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9052, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2162, requires_grad=True)\n",
      "yay -620\n",
      "complaint -120\n",
      "yay -680\n",
      "complaint -140\n",
      "yay -740\n",
      "complaint -160\n",
      "yay -800\n",
      "complaint -180\n",
      "yay -860\n",
      "epoch 300 loss = 1065.3718531131744; mean_loss= 1103.8680790990575; ()\n",
      "mode_hat tensor(1.0834, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2850, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0794, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "complaint -200\n",
      "yay -1040\n",
      "complaint -220\n",
      "yay -1100\n",
      "complaint -240\n",
      "yay -1160\n",
      "epoch 400 loss = 1124.6122636198997; mean_loss= 1100.8227612891635; ()\n",
      "mode_hat tensor(1.1148, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6787, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1466, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "yay -1460\n",
      "epoch 500 loss = 1094.8094909191132; mean_loss= 1096.868173931583; ()\n",
      "mode_hat tensor(1.1391, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0955, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.4153, requires_grad=True)\n",
      "yay -1520\n",
      "yay -1580\n",
      "yay -1640\n",
      "yay -1700\n",
      "yay -1760\n",
      "epoch 600 loss = 1062.5661014914513; mean_loss= 1139.8885692806004; ()\n",
      "mode_hat tensor(1.1322, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4468, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.6138, requires_grad=True)\n",
      "Final mean_losses: 1139.8885692806004\n",
      "complaint -260\n",
      "file exists: testresults/fit_unparametrized_laplace_10_parts1_N400_S100_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6052, -4.5696], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-0.6138016581535339\n",
      "ltscale_hat:\n",
      "-2.446772575378418\n",
      "mode_hat:\n",
      "1.1322178840637207\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 100 4.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1122.0413454174995; mean_loss= 1122.0413454174995; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 9 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-764.5214]], grad_fn=<IndexBackward>) tensor([[767.7818]], grad_fn=<IndexBackward>) tensor([[-267.3676]], grad_fn=<IndexBackward>) tensor([[264.1572]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0500]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-767.3286]], grad_fn=<IndexBackward>) tensor([[770.4982]], grad_fn=<IndexBackward>) tensor([[-266.5708]], grad_fn=<IndexBackward>) tensor([[263.3087]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0925]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-773.8211]], grad_fn=<IndexBackward>) tensor([[776.9455]], grad_fn=<IndexBackward>) tensor([[-264.5204]], grad_fn=<IndexBackward>) tensor([[261.1942]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2018]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-775.1525]], grad_fn=<IndexBackward>) tensor([[778.2584]], grad_fn=<IndexBackward>) tensor([[-264.1407]], grad_fn=<IndexBackward>) tensor([[260.7983]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.2365]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-775.1276]], grad_fn=<IndexBackward>) tensor([[778.6636]], grad_fn=<IndexBackward>) tensor([[-263.5652]], grad_fn=<IndexBackward>) tensor([[260.3610]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.3318]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-779.4844]], grad_fn=<IndexBackward>) tensor([[782.0021]], grad_fn=<IndexBackward>) tensor([[-263.4410]], grad_fn=<IndexBackward>) tensor([[259.8801]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.0432]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-779.4510]], grad_fn=<IndexBackward>) tensor([[782.4032]], grad_fn=<IndexBackward>) tensor([[-262.8592]], grad_fn=<IndexBackward>) tensor([[259.4372]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.4697]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-780.5562]], grad_fn=<IndexBackward>) tensor([[783.5665]], grad_fn=<IndexBackward>) tensor([[-262.4345]], grad_fn=<IndexBackward>) tensor([[259.0230]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.4012]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-779.5305]], grad_fn=<IndexBackward>) tensor([[783.3034]], grad_fn=<IndexBackward>) tensor([[-261.8118]], grad_fn=<IndexBackward>) tensor([[258.6488]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.6100]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-779.6495]], grad_fn=<IndexBackward>) tensor([[783.8067]], grad_fn=<IndexBackward>) tensor([[-261.3511]], grad_fn=<IndexBackward>) tensor([[258.3076]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[1.1137]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1166.215666770935; mean_loss= 1129.1121982999791; ()\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4984, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2580, requires_grad=True)\n",
      "yay -320\n",
      "complaint -20\n",
      "yay -380\n",
      "yay -440\n",
      "yay -500\n",
      "complaint -40\n",
      "yay -560\n",
      "complaint -60\n",
      "epoch 200 loss = 1159.0038637518883; mean_loss= 1110.5566480187204; ()\n",
      "mode_hat tensor(0.9673, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9767, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1468, requires_grad=True)\n",
      "yay -620\n",
      "complaint -80\n",
      "yay -680\n",
      "yay -740\n",
      "yay -800\n",
      "complaint -100\n",
      "yay -860\n",
      "complaint -120\n",
      "epoch 300 loss = 1064.1514167785645; mean_loss= 1099.2204359292125; ()\n",
      "mode_hat tensor(1.1193, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4216, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0685, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "complaint -140\n",
      "yay -1160\n",
      "complaint -160\n",
      "epoch 400 loss = 1058.85224878788; mean_loss= 1095.3151245163072; ()\n",
      "mode_hat tensor(1.0973, requires_grad=True)\n",
      "ltscale_hat tensor(-1.8284, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2458, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -180\n",
      "yay -1280\n",
      "yay -1340\n",
      "yay -1400\n",
      "complaint -200\n",
      "yay -1460\n",
      "epoch 500 loss = 1148.5627226829529; mean_loss= 1094.6415212089462; ()\n",
      "mode_hat tensor(1.1374, requires_grad=True)\n",
      "ltscale_hat tensor(-2.2728, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.4697, requires_grad=True)\n",
      "complaint -220\n",
      "yay -1520\n",
      "complaint -240\n",
      "yay -1580\n",
      "complaint -260\n",
      "yay -1640\n",
      "yay -1700\n",
      "yay -1760\n",
      "epoch 600 loss = 1106.0680460333824; mean_loss= 1092.0277280091905; ()\n",
      "mode_hat tensor(1.1412, requires_grad=True)\n",
      "ltscale_hat tensor(-2.7069, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.6768, requires_grad=True)\n",
      "yay -1820\n",
      "yay -1880\n",
      "yay -1940\n",
      "yay -2000\n",
      "complaint -280\n",
      "yay -2060\n",
      "epoch 700 loss = 1087.5676053762436; mean_loss= 1090.59591092175; ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode_hat tensor(1.1267, requires_grad=True)\n",
      "ltscale_hat tensor(-3.1423, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.8094, requires_grad=True)\n",
      "complaint -300\n",
      "yay -2120\n",
      "yay -2180\n",
      "complaint -320\n",
      "yay -2240\n",
      "complaint -340\n",
      "yay -2300\n",
      "complaint -360\n",
      "yay -2360\n",
      "epoch 800 loss = 1071.7661891579628; mean_loss= 1089.1655048970513; ()\n",
      "mode_hat tensor(1.1362, requires_grad=True)\n",
      "ltscale_hat tensor(-3.6180, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.9684, requires_grad=True)\n",
      "complaint -380\n",
      "yay -2420\n",
      "yay -2480\n",
      "complaint -400\n",
      "yay -2540\n",
      "complaint -420\n",
      "yay -2600\n",
      "complaint -440\n",
      "yay -2660\n",
      "complaint -460\n",
      "epoch 900 loss = 1089.8112194538116; mean_loss= 1088.6737032993935; ()\n",
      "mode_hat tensor(1.1401, requires_grad=True)\n",
      "ltscale_hat tensor(-4.0733, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.0689, requires_grad=True)\n",
      "yay -2720\n",
      "complaint -480\n",
      "yay -2780\n",
      "complaint -500\n",
      "yay -2840\n",
      "complaint -520\n",
      "yay -2900\n",
      "complaint -540\n",
      "yay -2960\n",
      "epoch 1000 loss = 1115.1673165559769; mean_loss= 1088.2596870463976; ()\n",
      "mode_hat tensor(1.1417, requires_grad=True)\n",
      "ltscale_hat tensor(-4.5213, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.1238, requires_grad=True)\n",
      "complaint -560\n",
      "yay -3020\n",
      "complaint -580\n",
      "yay -3080\n",
      "complaint -600\n",
      "yay -3140\n",
      "complaint -620\n",
      "yay -3200\n",
      "yay -3260\n",
      "complaint -640\n",
      "epoch 1100 loss = 1130.3260216116905; mean_loss= 1087.9105398990596; ()\n",
      "mode_hat tensor(1.1300, requires_grad=True)\n",
      "ltscale_hat tensor(-5.0061, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2004, requires_grad=True)\n",
      "yay -3320\n",
      "complaint -660\n",
      "yay -3380\n",
      "complaint -680\n",
      "yay -3440\n",
      "complaint -700\n",
      "yay -3500\n",
      "complaint -720\n",
      "yay -3560\n",
      "epoch 1200 loss = 1147.190438091755; mean_loss= 1087.5474306083215; ()\n",
      "mode_hat tensor(1.1358, requires_grad=True)\n",
      "ltscale_hat tensor(-5.4948, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2627, requires_grad=True)\n",
      "complaint -740\n",
      "yay -3620\n",
      "complaint -760\n",
      "yay -3680\n",
      "complaint -780\n",
      "yay -3740\n",
      "complaint -800\n",
      "yay -3800\n",
      "complaint -820\n",
      "yay -3860\n",
      "epoch 1300 loss = 1099.5039755702019; mean_loss= 1085.9729129047314; ()\n",
      "mode_hat tensor(1.1179, requires_grad=True)\n",
      "ltscale_hat tensor(-5.9806, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2967, requires_grad=True)\n",
      "yay -3920\n",
      "complaint -840\n",
      "yay -3980\n",
      "yay -4040\n",
      "complaint -860\n",
      "yay -4100\n",
      "complaint -880\n",
      "yay -4160\n",
      "epoch 1400 loss = 1091.855809390545; mean_loss= 1085.1086799744683; ()\n",
      "mode_hat tensor(1.1279, requires_grad=True)\n",
      "ltscale_hat tensor(-6.4714, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.3747, requires_grad=True)\n",
      "yay -4220\n",
      "complaint -900\n",
      "yay -4280\n",
      "complaint -920\n",
      "yay -4340\n",
      "complaint -940\n",
      "yay -4400\n",
      "complaint -960\n",
      "yay -4460\n",
      "complaint -980\n",
      "epoch 1500 loss = 1091.7656795978546; mean_loss= 1084.73949534274; ()\n",
      "mode_hat tensor(1.1144, requires_grad=True)\n",
      "ltscale_hat tensor(-6.8650, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4135, requires_grad=True)\n",
      "yay -4520\n",
      "complaint -1000\n",
      "yay -4580\n",
      "complaint -1020\n",
      "yay -4640\n",
      "complaint -1040\n",
      "yay -4700\n",
      "complaint -1060\n",
      "yay -4760\n",
      "complaint -1080\n",
      "epoch 1600 loss = 1025.877462029457; mean_loss= 1082.7028885667141; ()\n",
      "mode_hat tensor(1.1298, requires_grad=True)\n",
      "ltscale_hat tensor(-7.3371, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4799, requires_grad=True)\n",
      "yay -4820\n",
      "complaint -1100\n",
      "yay -4880\n",
      "complaint -1120\n",
      "yay -4940\n",
      "complaint -1140\n",
      "yay -5000\n",
      "complaint -1160\n",
      "yay -5060\n",
      "complaint -1180\n",
      "epoch 1700 loss = 1192.2434655427933; mean_loss= 1085.6695236410364; ()\n",
      "mode_hat tensor(1.1264, requires_grad=True)\n",
      "ltscale_hat tensor(-7.8199, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5257, requires_grad=True)\n",
      "yay -5120\n",
      "complaint -1200\n",
      "yay -5180\n",
      "complaint -1220\n",
      "yay -5240\n",
      "complaint -1240\n",
      "yay -5300\n",
      "complaint -1260\n",
      "yay -5360\n",
      "epoch 1800 loss = 1156.205012023449; mean_loss= 1084.5243610615253; ()\n",
      "mode_hat tensor(1.1188, requires_grad=True)\n",
      "ltscale_hat tensor(-8.3064, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5548, requires_grad=True)\n",
      "complaint -1280\n",
      "yay -5420\n",
      "complaint -1300\n",
      "yay -5480\n",
      "complaint -1320\n",
      "yay -5540\n",
      "complaint -1340\n",
      "yay -5600\n",
      "complaint -1360\n",
      "yay -5660\n",
      "epoch 1900 loss = 1111.6663403511047; mean_loss= 1083.4002209939601; ()\n",
      "mode_hat tensor(1.1496, requires_grad=True)\n",
      "ltscale_hat tensor(-8.8007, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5800, requires_grad=True)\n",
      "complaint -1380\n",
      "yay -5720\n",
      "complaint -1400\n",
      "yay -5780\n",
      "complaint -1420\n",
      "yay -5840\n",
      "complaint -1440\n",
      "yay -5900\n",
      "complaint -1460\n",
      "yay -5960\n",
      "epoch 2000 loss = 1075.6643983721733; mean_loss= 1081.8003091973849; ()\n",
      "mode_hat tensor(1.1438, requires_grad=True)\n",
      "ltscale_hat tensor(-9.2975, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5985, requires_grad=True)\n",
      "Final mean_losses: 1081.8003091973849\n",
      "file exists: testresults/fit_fully_amortized_10_parts1_N400_S100_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6455, -6.4893], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-1.5984972715377808\n",
      "ltscale_hat:\n",
      "-9.297492027282715\n",
      "mode_hat:\n",
      "1.1438052654266357\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 399 1.0025062656641603\n",
      "complaint 9 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-851.3810]], grad_fn=<IndexBackward>) tensor([[875.8784]], grad_fn=<IndexBackward>) tensor([[-311.4382]], grad_fn=<IndexBackward>) tensor([[287.0066]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0657]], grad_fn=<IndexBackward>)\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1049.5599966347218; mean_loss= 1049.5599966347218; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "complaint 8 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-855.4335]], grad_fn=<IndexBackward>) tensor([[880.0073]], grad_fn=<IndexBackward>) tensor([[-310.3546]], grad_fn=<IndexBackward>) tensor([[285.7152]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0656]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-856.4187],\n",
      "        [-761.6487]], grad_fn=<IndexBackward>) tensor([[881.1337],\n",
      "        [762.9382]], grad_fn=<IndexBackward>) tensor([[-309.9463],\n",
      "        [-268.4170]], grad_fn=<IndexBackward>) tensor([[285.2827],\n",
      "        [267.1021]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0515],\n",
      "        [-0.0254]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-857.6276]], grad_fn=<IndexBackward>) tensor([[882.4139]], grad_fn=<IndexBackward>) tensor([[-309.5804]], grad_fn=<IndexBackward>) tensor([[284.8650]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0710]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-859.2078]], grad_fn=<IndexBackward>) tensor([[883.9495]], grad_fn=<IndexBackward>) tensor([[-309.2713]], grad_fn=<IndexBackward>) tensor([[284.4592]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0705]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-859.9033],\n",
      "        [-765.3735]], grad_fn=<IndexBackward>) tensor([[884.8786],\n",
      "        [766.6786]], grad_fn=<IndexBackward>) tensor([[-308.8613],\n",
      "        [-267.3061]], grad_fn=<IndexBackward>) tensor([[284.0585],\n",
      "        [265.9837]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.1725],\n",
      "        [-0.0173]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-861.7109],\n",
      "        [-766.5428]], grad_fn=<IndexBackward>) tensor([[886.5710],\n",
      "        [767.8782]], grad_fn=<IndexBackward>) tensor([[-308.5958],\n",
      "        [-266.9444]], grad_fn=<IndexBackward>) tensor([[283.6690],\n",
      "        [265.6281]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0667],\n",
      "        [ 0.0190]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[145],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [153]]) \n",
      "              1.. tensor([[-863.1721],\n",
      "        [-767.8949]], grad_fn=<IndexBackward>) tensor([[888.0262],\n",
      "        [769.2001]], grad_fn=<IndexBackward>) tensor([[-308.2973],\n",
      "        [-266.6116]], grad_fn=<IndexBackward>) tensor([[283.2878],\n",
      "        [265.2805]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1554],\n",
      "        [-0.0260]], grad_fn=<IndexBackward>)\n",
      "yay -20\n",
      "complaint 1 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-867.9913],\n",
      "        [-769.0180]], grad_fn=<IndexBackward>) tensor([[891.7834],\n",
      "        [770.3697]], grad_fn=<IndexBackward>) tensor([[-308.3760],\n",
      "        [-266.2340]], grad_fn=<IndexBackward>) tensor([[282.8874],\n",
      "        [264.9146]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.6965],\n",
      "        [ 0.0323]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[145]]) \n",
      "              1.. tensor([[-869.2064]], grad_fn=<IndexBackward>) tensor([[893.0709]], grad_fn=<IndexBackward>) tensor([[-308.0226]], grad_fn=<IndexBackward>) tensor([[282.4839]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-1.6741]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "complaint -20\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1009.8749426603317; mean_loss= 1033.1111454885213; ()\n",
      "mode_hat tensor(0.4849, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4745, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2732, requires_grad=True)\n",
      "yay -320\n",
      "complaint -40\n",
      "yay -380\n",
      "complaint -60\n",
      "yay -440\n",
      "complaint -80\n",
      "yay -500\n",
      "yay -560\n",
      "complaint -100\n",
      "epoch 200 loss = 1010.8555357158184; mean_loss= 1014.690602382917; ()\n",
      "mode_hat tensor(0.8035, requires_grad=True)\n",
      "ltscale_hat tensor(-0.8277, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1981, requires_grad=True)\n",
      "yay -620\n",
      "complaint -120\n",
      "yay -680\n",
      "yay -740\n",
      "yay -800\n",
      "complaint -140\n",
      "yay -860\n",
      "complaint -160\n",
      "epoch 300 loss = 1004.230059325695; mean_loss= 1005.0373507249453; ()\n",
      "mode_hat tensor(1.1100, requires_grad=True)\n",
      "ltscale_hat tensor(-1.2318, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0035, requires_grad=True)\n",
      "yay -920\n",
      "complaint -180\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "complaint -200\n",
      "yay -1160\n",
      "complaint -220\n",
      "epoch 400 loss = 1003.0394271612167; mean_loss= 1001.0000109924875; ()\n",
      "mode_hat tensor(1.1550, requires_grad=True)\n",
      "ltscale_hat tensor(-1.6444, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.2878, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "complaint -240\n",
      "yay -1340\n",
      "complaint -260\n",
      "yay -1400\n",
      "yay -1460\n",
      "complaint -280\n",
      "epoch 500 loss = 1000.3742684423923; mean_loss= 998.3518449420167; ()\n",
      "mode_hat tensor(1.1233, requires_grad=True)\n",
      "ltscale_hat tensor(-2.0343, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.4831, requires_grad=True)\n",
      "yay -1520\n",
      "complaint -300\n",
      "yay -1580\n",
      "yay -1640\n",
      "yay -1700\n",
      "yay -1760\n",
      "complaint -320\n",
      "epoch 600 loss = 999.8250660002232; mean_loss= 996.4153219433541; ()\n",
      "mode_hat tensor(1.1408, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4694, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.7872, requires_grad=True)\n",
      "yay -1820\n",
      "complaint -340\n",
      "yay -1880\n",
      "complaint -360\n",
      "yay -1940\n",
      "complaint -380\n",
      "yay -2000\n",
      "complaint -400\n",
      "yay -2060\n",
      "epoch 700 loss = 994.9605753719807; mean_loss= 995.190735791472; ()\n",
      "mode_hat tensor(1.1467, requires_grad=True)\n",
      "ltscale_hat tensor(-2.8916, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.9904, requires_grad=True)\n",
      "yay -2120\n",
      "complaint -420\n",
      "yay -2180\n",
      "complaint -440\n",
      "yay -2240\n",
      "complaint -460\n",
      "yay -2300\n",
      "complaint -480\n",
      "yay -2360\n",
      "complaint -500\n",
      "epoch 800 loss = 993.2496815025806; mean_loss= 994.5912115388851; ()\n",
      "mode_hat tensor(1.1402, requires_grad=True)\n",
      "ltscale_hat tensor(-3.1838, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.0954, requires_grad=True)\n",
      "yay -2420\n",
      "complaint -520\n",
      "yay -2480\n",
      "yay -2540\n",
      "complaint -540\n",
      "yay -2600\n",
      "yay -2660\n",
      "epoch 900 loss = 992.835910409689; mean_loss= 994.9650654395364; ()\n",
      "mode_hat tensor(1.1654, requires_grad=True)\n",
      "ltscale_hat tensor(-3.4289, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.2758, requires_grad=True)\n",
      "yay -2720\n",
      "yay -2780\n",
      "yay -2840\n",
      "yay -2900\n",
      "complaint -560\n",
      "yay -2960\n",
      "epoch 1000 loss = 994.728026330471; mean_loss= 995.0914874644858; ()\n",
      "mode_hat tensor(1.1252, requires_grad=True)\n",
      "ltscale_hat tensor(-3.6683, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4418, requires_grad=True)\n",
      "yay -3020\n",
      "yay -3080\n",
      "yay -3140\n",
      "complaint -580\n",
      "yay -3200\n",
      "yay -3260\n",
      "complaint -600\n",
      "epoch 1100 loss = 990.5220284461975; mean_loss= 993.4168197291553; ()\n",
      "mode_hat tensor(1.1287, requires_grad=True)\n",
      "ltscale_hat tensor(-3.8996, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5852, requires_grad=True)\n",
      "yay -3320\n",
      "complaint -620\n",
      "yay -3380\n",
      "complaint -640\n",
      "yay -3440\n",
      "complaint -660\n",
      "yay -3500\n",
      "complaint -680\n",
      "yay -3560\n",
      "complaint -700\n",
      "epoch 1200 loss = 994.1518635451794; mean_loss= 993.4997371991044; ()\n",
      "mode_hat tensor(1.1131, requires_grad=True)\n",
      "ltscale_hat tensor(-4.0614, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6185, requires_grad=True)\n",
      "yay -3620\n",
      "complaint -720\n",
      "Final mean_losses: 1000.2454605221023\n",
      "file exists: testresults/fit_unparametrized_laplace_10_parts1_N400_S399_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6052, -4.6052], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-1.63157057762146\n",
      "ltscale_hat:\n",
      "-4.077867031097412\n",
      "mode_hat:\n",
      "1.1229616403579712\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 399 1.0025062656641603\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1052.9934682548046; mean_loss= 1052.9934682548046; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "complaint 9 assert approx_eq: tensor([[145],\n",
      "        [153]]) \n",
      "              1.. tensor([[-941.8530],\n",
      "        [-771.4432]], grad_fn=<IndexBackward>) tensor([[944.0848],\n",
      "        [774.0912]], grad_fn=<IndexBackward>) tensor([[-283.2968],\n",
      "        [-265.9946]], grad_fn=<IndexBackward>) tensor([[281.0539],\n",
      "        [263.2431]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0111],\n",
      "        [-0.1035]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-772.8860]], grad_fn=<IndexBackward>) tensor([[775.4773]], grad_fn=<IndexBackward>) tensor([[-265.6932]], grad_fn=<IndexBackward>) tensor([[262.9131]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1888]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[ 40],\n",
      "        [145],\n",
      "        [153]]) \n",
      "              1.. tensor([[ 983.8537],\n",
      "        [-944.6716],\n",
      "        [-773.5599]], grad_fn=<IndexBackward>) tensor([[-998.6901],\n",
      "        [ 946.9302],\n",
      "        [ 776.3493]], grad_fn=<IndexBackward>) tensor([[ 302.6029],\n",
      "        [-282.5820],\n",
      "        [-265.3070]], grad_fn=<IndexBackward>) tensor([[-287.7542],\n",
      "        [ 280.3337],\n",
      "        [ 262.5866]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0123],\n",
      "        [0.0103],\n",
      "        [0.0690]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-776.1005]], grad_fn=<IndexBackward>) tensor([[778.8927]], grad_fn=<IndexBackward>) tensor([[-264.6811]], grad_fn=<IndexBackward>) tensor([[261.9438]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.0549]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-777.7175]], grad_fn=<IndexBackward>) tensor([[780.8200]], grad_fn=<IndexBackward>) tensor([[-263.9641]], grad_fn=<IndexBackward>) tensor([[261.3149]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.4533]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[ 40],\n",
      "        [153]]) \n",
      "              1.. tensor([[ 975.9860],\n",
      "        [-782.3427]], grad_fn=<IndexBackward>) tensor([[-990.8990],\n",
      "        [ 784.3371]], grad_fn=<IndexBackward>) tensor([[ 299.4425],\n",
      "        [-264.0411]], grad_fn=<IndexBackward>) tensor([[-284.5415],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 261.0057]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0120],\n",
      "        [-1.0410]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[ 40],\n",
      "        [153]]) \n",
      "              1.. tensor([[ 974.4296],\n",
      "        [-782.5385]], grad_fn=<IndexBackward>) tensor([[-989.3537],\n",
      "        [ 784.8926]], grad_fn=<IndexBackward>) tensor([[ 298.8425],\n",
      "        [-263.6364]], grad_fn=<IndexBackward>) tensor([[-283.9293],\n",
      "        [ 260.7146]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0109],\n",
      "        [-0.5678]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-782.7380]], grad_fn=<IndexBackward>) tensor([[785.4507]], grad_fn=<IndexBackward>) tensor([[-263.2347]], grad_fn=<IndexBackward>) tensor([[260.4255]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0966]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-782.9354]], grad_fn=<IndexBackward>) tensor([[786.0074]], grad_fn=<IndexBackward>) tensor([[-262.8316]], grad_fn=<IndexBackward>) tensor([[260.1350]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.3755]], grad_fn=<IndexBackward>)\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-783.1324]], grad_fn=<IndexBackward>) tensor([[786.5641]], grad_fn=<IndexBackward>) tensor([[-262.4286]], grad_fn=<IndexBackward>) tensor([[259.8442]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[0.8473]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "yay -80\n",
      "yay -140\n",
      "yay -200\n",
      "yay -260\n",
      "epoch 100 loss = 1020.9689840376377; mean_loss= 1033.2927128110177; ()\n",
      "mode_hat tensor(0.5050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.5050, requires_grad=True)\n",
      "ldfraw_hat tensor(0.3097, requires_grad=True)\n",
      "yay -320\n",
      "complaint -20\n",
      "yay -380\n",
      "complaint -40\n",
      "yay -440\n",
      "complaint -60\n",
      "yay -500\n",
      "complaint -80\n",
      "yay -560\n",
      "complaint -100\n",
      "epoch 200 loss = 1005.2581389546394; mean_loss= 1012.2899845712926; ()\n",
      "mode_hat tensor(1.0039, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9880, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2735, requires_grad=True)\n",
      "yay -620\n",
      "complaint -120\n",
      "yay -680\n",
      "complaint -140\n",
      "yay -740\n",
      "complaint -160\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 1000.3955569267273; mean_loss= 1002.769857407326; ()\n",
      "mode_hat tensor(1.1462, requires_grad=True)\n",
      "ltscale_hat tensor(-1.4718, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.1781, requires_grad=True)\n",
      "yay -920\n",
      "yay -980\n",
      "yay -1040\n",
      "yay -1100\n",
      "complaint -180\n",
      "yay -1160\n",
      "epoch 400 loss = 996.9925034940243; mean_loss= 998.848735508119; ()\n",
      "mode_hat tensor(1.1380, requires_grad=True)\n",
      "ltscale_hat tensor(-1.9610, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.6550, requires_grad=True)\n",
      "yay -1220\n",
      "complaint -200\n",
      "yay -1280\n",
      "yay -1340\n",
      "complaint -220\n",
      "yay -1400\n",
      "complaint -240\n",
      "yay -1460\n",
      "complaint -260\n",
      "epoch 500 loss = 995.7703957557678; mean_loss= 996.6710401088285; ()\n",
      "mode_hat tensor(1.1450, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4524, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.0914, requires_grad=True)\n",
      "yay -1520\n",
      "complaint -280\n",
      "yay -1580\n",
      "complaint -300\n",
      "yay -1640\n",
      "yay -1700\n",
      "complaint -320\n",
      "yay -1760\n",
      "complaint -340\n",
      "epoch 600 loss = 994.9106560945511; mean_loss= 994.9750101469829; ()\n",
      "mode_hat tensor(1.1404, requires_grad=True)\n",
      "ltscale_hat tensor(-2.9468, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4295, requires_grad=True)\n",
      "yay -1820\n",
      "complaint -360\n",
      "yay -1880\n",
      "yay -1940\n",
      "complaint -380\n",
      "yay -2000\n",
      "complaint -400\n",
      "yay -2060\n",
      "epoch 700 loss = 993.9073235690594; mean_loss= 993.7376652771704; ()\n",
      "mode_hat tensor(1.1400, requires_grad=True)\n",
      "ltscale_hat tensor(-3.4422, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.4973, requires_grad=True)\n",
      "yay -2120\n",
      "yay -2180\n",
      "complaint -420\n",
      "yay -2240\n",
      "complaint -440\n",
      "yay -2300\n",
      "complaint -460\n",
      "yay -2360\n",
      "epoch 800 loss = 993.3409965634346; mean_loss= 993.0176264709924; ()\n",
      "mode_hat tensor(1.1475, requires_grad=True)\n",
      "ltscale_hat tensor(-3.9392, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5327, requires_grad=True)\n",
      "complaint -480\n",
      "yay -2420\n",
      "complaint -500\n",
      "yay -2480\n",
      "yay -2540\n",
      "complaint -520\n",
      "yay -2600\n",
      "complaint -540\n",
      "yay -2660\n",
      "epoch 900 loss = 992.4513393938541; mean_loss= 992.1723643461926; ()\n",
      "mode_hat tensor(1.1475, requires_grad=True)\n",
      "ltscale_hat tensor(-4.4372, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5791, requires_grad=True)\n",
      "complaint -560\n",
      "yay -2720\n",
      "complaint -580\n",
      "yay -2780\n",
      "complaint -600\n",
      "yay -2840\n",
      "yay -2900\n",
      "complaint -620\n",
      "yay -2960\n",
      "epoch 1000 loss = 991.6900280714035; mean_loss= 991.4809517806663; ()\n",
      "mode_hat tensor(1.1431, requires_grad=True)\n",
      "ltscale_hat tensor(-4.9358, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5943, requires_grad=True)\n",
      "yay -3020\n",
      "complaint -640\n",
      "yay -3080\n",
      "yay -3140\n",
      "yay -3200\n",
      "complaint -660\n",
      "yay -3260\n",
      "epoch 1100 loss = 991.0508364140987; mean_loss= 990.9510230748607; ()\n",
      "mode_hat tensor(1.1378, requires_grad=True)\n",
      "ltscale_hat tensor(-5.4345, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.5933, requires_grad=True)\n",
      "complaint -680\n",
      "yay -3320\n",
      "yay -3380\n",
      "complaint -700\n",
      "yay -3440\n",
      "yay -3500\n",
      "complaint -720\n",
      "yay -3560\n",
      "epoch 1200 loss = 990.4775986671448; mean_loss= 990.2739277011784; ()\n",
      "mode_hat tensor(1.1381, requires_grad=True)\n",
      "ltscale_hat tensor(-5.9334, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6128, requires_grad=True)\n",
      "complaint -740\n",
      "yay -3620\n",
      "yay -3680\n",
      "complaint -760\n",
      "yay -3740\n",
      "complaint -780\n",
      "yay -3800\n",
      "yay -3860\n",
      "complaint -800\n",
      "epoch 1300 loss = 988.5975169837475; mean_loss= 989.5264286485445; ()\n",
      "mode_hat tensor(1.1437, requires_grad=True)\n",
      "ltscale_hat tensor(-6.4321, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6300, requires_grad=True)\n",
      "yay -3920\n",
      "complaint -820\n",
      "yay -3980\n",
      "complaint -840\n",
      "yay -4040\n",
      "complaint -860\n",
      "yay -4100\n",
      "complaint -880\n",
      "yay -4160\n",
      "epoch 1400 loss = 988.2738193571568; mean_loss= 988.8347923119437; ()\n",
      "mode_hat tensor(1.1429, requires_grad=True)\n",
      "ltscale_hat tensor(-6.9307, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6161, requires_grad=True)\n",
      "yay -4220\n",
      "complaint -900\n",
      "yay -4280\n",
      "complaint -920\n",
      "yay -4340\n",
      "complaint -940\n",
      "yay -4400\n",
      "complaint -960\n",
      "yay -4460\n",
      "epoch 1500 loss = 988.6476068794727; mean_loss= 988.2218643877063; ()\n",
      "mode_hat tensor(1.1403, requires_grad=True)\n",
      "ltscale_hat tensor(-7.4291, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6310, requires_grad=True)\n",
      "complaint -980\n",
      "yay -4520\n",
      "complaint -1000\n",
      "yay -4580\n",
      "complaint -1020\n",
      "yay -4640\n",
      "complaint -1040\n",
      "yay -4700\n",
      "complaint -1060\n",
      "yay -4760\n",
      "epoch 1600 loss = 987.5474667847157; mean_loss= 987.710249977615; ()\n",
      "mode_hat tensor(1.1408, requires_grad=True)\n",
      "ltscale_hat tensor(-7.9274, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6528, requires_grad=True)\n",
      "complaint -1080\n",
      "yay -4820\n",
      "complaint -1100\n",
      "yay -4880\n",
      "complaint -1120\n",
      "yay -4940\n",
      "complaint -1140\n",
      "yay -5000\n",
      "complaint -1160\n",
      "yay -5060\n",
      "epoch 1700 loss = 981.2384862005711; mean_loss= 987.1388002028898; ()\n",
      "mode_hat tensor(1.1387, requires_grad=True)\n",
      "ltscale_hat tensor(-8.4258, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6409, requires_grad=True)\n",
      "complaint -1180\n",
      "yay -5120\n",
      "complaint -1200\n",
      "yay -5180\n",
      "complaint -1220\n",
      "yay -5240\n",
      "complaint -1240\n",
      "yay -5300\n",
      "complaint -1260\n",
      "yay -5360\n",
      "epoch 1800 loss = 986.9341360330582; mean_loss= 986.9412279535086; ()\n",
      "mode_hat tensor(1.1446, requires_grad=True)\n",
      "ltscale_hat tensor(-8.9245, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6399, requires_grad=True)\n",
      "complaint -1280\n",
      "yay -5420\n",
      "complaint -1300\n",
      "yay -5480\n",
      "complaint -1320\n",
      "yay -5540\n",
      "complaint -1340\n",
      "yay -5600\n",
      "complaint -1360\n",
      "yay -5660\n",
      "epoch 1900 loss = 986.7863665521145; mean_loss= 986.3957067673202; ()\n",
      "mode_hat tensor(1.1396, requires_grad=True)\n",
      "ltscale_hat tensor(-9.4235, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6787, requires_grad=True)\n",
      "complaint -1380\n",
      "yay -5720\n",
      "complaint -1400\n",
      "yay -5780\n",
      "complaint -1420\n",
      "yay -5840\n",
      "complaint -1440\n",
      "yay -5900\n",
      "complaint -1460\n",
      "yay -5960\n",
      "epoch 2000 loss = 982.8347672224045; mean_loss= 985.8291423630719; ()\n",
      "mode_hat tensor(1.1422, requires_grad=True)\n",
      "ltscale_hat tensor(-9.9227, requires_grad=True)\n",
      "ldfraw_hat tensor(-1.6950, requires_grad=True)\n",
      "Final mean_losses: 985.8291423630719\n",
      "file exists: testresults/fit_fully_amortized_10_parts1_N400_S399_mu1.0_sigma2.0_nu3.0.csv\n",
      "guidename fully_amortized\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "globalpsi:\n",
      "tensor([-4.6052, -4.6052], grad_fn=<SliceBackward>) (10 elems)\n",
      "latentpsi:\n",
      "tensor([-4.6052], requires_grad=True)\n",
      "ldfraw_hat:\n",
      "-1.694962501525879\n",
      "ltscale_hat:\n",
      "-9.92269515991211\n",
      "mode_hat:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1422489881515503\n",
      "1 1 ['unparametrized_laplace', 'fully_amortized']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 3 ['unparametrized_laplace', 'fully_amortized'] [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "1\n",
      "testresults/scenario_N400_mu1.0_sigma2.0_nu3.0.csv from file\n",
      "400\n",
      "tensor([-7.9208,  0.5222,  0.8593,  2.3906])\n",
      "Sizes: torch.Size([400]) torch.Size([400])\n",
      "guidename unparametrized_laplace\n",
      "sourceparams [{'modal_effect': 1.0, 'df': 3.0, 't_scale': 2.0}]\n",
      "sizes torch.Size([400]) 400 100 4.0\n",
      "yay 7 fix_m_grad\n",
      "epoch 0 loss = 1241.752417663733; mean_loss= 1241.752417663733; ()\n",
      "mode_hat tensor(0.0050, requires_grad=True)\n",
      "ltscale_hat tensor(-0.0050, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0050, requires_grad=True)\n",
      "yay 4 fix_m_grad\n",
      "yay 1 fix_m_grad\n",
      "yay -20\n",
      "yay -80\n",
      "yay -140\n",
      "complaint 9 assert approx_eq: tensor([[153],\n",
      "        [199],\n",
      "        [200]]) \n",
      "              1.. tensor([[-727.9871],\n",
      "        [ 619.5005],\n",
      "        [ 637.6830]], grad_fn=<IndexBackward>) tensor([[ 760.3495],\n",
      "        [-620.1140],\n",
      "        [-642.0941]], grad_fn=<IndexBackward>) tensor([[-273.2678],\n",
      "        [ 219.4906],\n",
      "        [ 226.6858]], grad_fn=<IndexBackward>) tensor([[ 240.9860],\n",
      "        [-218.8625],\n",
      "        [-222.2916]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0807],\n",
      "        [ 0.0147],\n",
      "        [-0.0169]], grad_fn=<IndexBackward>)\n",
      "complaint 8 assert approx_eq: tensor([[153],\n",
      "        [199],\n",
      "        [200]]) \n",
      "              1.. tensor([[-727.9871],\n",
      "        [ 619.5005],\n",
      "        [ 637.6830]], grad_fn=<IndexBackward>) tensor([[ 760.3495],\n",
      "        [-620.1140],\n",
      "        [-642.0941]], grad_fn=<IndexBackward>) tensor([[-273.2678],\n",
      "        [ 219.4906],\n",
      "        [ 226.6858]], grad_fn=<IndexBackward>) tensor([[ 240.9860],\n",
      "        [-218.8625],\n",
      "        [-222.2916]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0807],\n",
      "        [ 0.0147],\n",
      "        [-0.0169]], grad_fn=<IndexBackward>)\n",
      "complaint 7 assert approx_eq: tensor([[153],\n",
      "        [199],\n",
      "        [200]]) \n",
      "              1.. tensor([[-727.9871],\n",
      "        [ 619.5005],\n",
      "        [ 637.6830]], grad_fn=<IndexBackward>) tensor([[ 760.3495],\n",
      "        [-620.1140],\n",
      "        [-642.0941]], grad_fn=<IndexBackward>) tensor([[-273.2678],\n",
      "        [ 219.4906],\n",
      "        [ 226.6858]], grad_fn=<IndexBackward>) tensor([[ 240.9860],\n",
      "        [-218.8625],\n",
      "        [-222.2916]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[ 0.0807],\n",
      "        [ 0.0147],\n",
      "        [-0.0169]], grad_fn=<IndexBackward>)\n",
      "complaint 6 assert approx_eq: tensor([[153],\n",
      "        [199]]) \n",
      "              1.. tensor([[-729.3739],\n",
      "        [ 618.4250]], grad_fn=<IndexBackward>) tensor([[ 761.7114],\n",
      "        [-619.0408]], grad_fn=<IndexBackward>) tensor([[-273.1685],\n",
      "        [ 219.0626]], grad_fn=<IndexBackward>) tensor([[ 240.7785],\n",
      "        [-218.4349]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0524],\n",
      "        [ 0.0120]], grad_fn=<IndexBackward>)\n",
      "complaint 5 assert approx_eq: tensor([[153],\n",
      "        [199]]) \n",
      "              1.. tensor([[-729.3739],\n",
      "        [ 618.4250]], grad_fn=<IndexBackward>) tensor([[ 761.7114],\n",
      "        [-619.0408]], grad_fn=<IndexBackward>) tensor([[-273.1685],\n",
      "        [ 219.0626]], grad_fn=<IndexBackward>) tensor([[ 240.7785],\n",
      "        [-218.4349]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0524],\n",
      "        [ 0.0120]], grad_fn=<IndexBackward>)\n",
      "complaint 4 assert approx_eq: tensor([[153],\n",
      "        [199]]) \n",
      "              1.. tensor([[-729.3739],\n",
      "        [ 618.4250]], grad_fn=<IndexBackward>) tensor([[ 761.7114],\n",
      "        [-619.0408]], grad_fn=<IndexBackward>) tensor([[-273.1685],\n",
      "        [ 219.0626]], grad_fn=<IndexBackward>) tensor([[ 240.7785],\n",
      "        [-218.4349]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0524],\n",
      "        [ 0.0120]], grad_fn=<IndexBackward>)\n",
      "complaint 3 assert approx_eq: tensor([[153],\n",
      "        [199]]) \n",
      "              1.. tensor([[-732.8667],\n",
      "        [ 615.2295]], grad_fn=<IndexBackward>) tensor([[ 765.3477],\n",
      "        [-615.8329]], grad_fn=<IndexBackward>) tensor([[-272.2379],\n",
      "        [ 217.3639]], grad_fn=<IndexBackward>) tensor([[ 239.6958],\n",
      "        [-216.7320]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0612],\n",
      "        [ 0.0285]], grad_fn=<IndexBackward>)\n",
      "complaint 2 assert approx_eq: tensor([[153],\n",
      "        [199]]) \n",
      "              1.. tensor([[-732.8667],\n",
      "        [ 615.2295]], grad_fn=<IndexBackward>) tensor([[ 765.3477],\n",
      "        [-615.8329]], grad_fn=<IndexBackward>) tensor([[-272.2379],\n",
      "        [ 217.3639]], grad_fn=<IndexBackward>) tensor([[ 239.6958],\n",
      "        [-216.7320]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0612],\n",
      "        [ 0.0285]], grad_fn=<IndexBackward>)\n",
      "complaint 1 assert approx_eq: tensor([[153],\n",
      "        [199]]) \n",
      "              1.. tensor([[-732.8667],\n",
      "        [ 615.2295]], grad_fn=<IndexBackward>) tensor([[ 765.3477],\n",
      "        [-615.8329]], grad_fn=<IndexBackward>) tensor([[-272.2379],\n",
      "        [ 217.3639]], grad_fn=<IndexBackward>) tensor([[ 239.6958],\n",
      "        [-216.7320]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.0612],\n",
      "        [ 0.0285]], grad_fn=<IndexBackward>)\n",
      "yay -200\n",
      "complaint 0 assert approx_eq: tensor([[153]]) \n",
      "              1.. tensor([[-734.1959]], grad_fn=<IndexBackward>) tensor([[766.6781]], grad_fn=<IndexBackward>) tensor([[-272.0555]], grad_fn=<IndexBackward>) tensor([[239.4499]], grad_fn=<IndexBackward>) \n",
      "              2.. tensor([[-0.1234]], grad_fn=<IndexBackward>)\n",
      "complaint 0\n",
      "complaint -20\n",
      "complaint -40\n",
      "yay -260\n",
      "complaint -60\n",
      "epoch 100 loss = 1176.5886102716129; mean_loss= 1154.0628859026804; ()\n",
      "mode_hat tensor(0.4304, requires_grad=True)\n",
      "ltscale_hat tensor(-0.4878, requires_grad=True)\n",
      "ldfraw_hat tensor(0.2404, requires_grad=True)\n",
      "complaint -80\n",
      "complaint -100\n",
      "yay -320\n",
      "complaint -120\n",
      "complaint -140\n",
      "complaint -160\n",
      "yay -380\n",
      "complaint -180\n",
      "complaint -200\n",
      "yay -440\n",
      "complaint -220\n",
      "complaint -240\n",
      "yay -500\n",
      "complaint -260\n",
      "complaint -280\n",
      "yay -560\n",
      "complaint -300\n",
      "complaint -320\n",
      "epoch 200 loss = 1115.2653836607933; mean_loss= 1123.0570509051508; ()\n",
      "mode_hat tensor(0.7828, requires_grad=True)\n",
      "ltscale_hat tensor(-0.9438, requires_grad=True)\n",
      "ldfraw_hat tensor(0.1545, requires_grad=True)\n",
      "complaint -340\n",
      "yay -620\n",
      "complaint -360\n",
      "yay -680\n",
      "complaint -380\n",
      "complaint -400\n",
      "yay -740\n",
      "complaint -420\n",
      "yay -800\n",
      "yay -860\n",
      "epoch 300 loss = 1097.1397004127502; mean_loss= 1110.2674336688644; ()\n",
      "mode_hat tensor(0.9977, requires_grad=True)\n",
      "ltscale_hat tensor(-1.3717, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.0495, requires_grad=True)\n",
      "complaint -440\n",
      "yay -920\n",
      "complaint -460\n",
      "complaint -480\n",
      "complaint -500\n",
      "yay -980\n",
      "complaint -520\n",
      "yay -1040\n",
      "yay -1100\n",
      "yay -1160\n",
      "epoch 400 loss = 1015.158568918705; mean_loss= 1109.3521921639974; ()\n",
      "mode_hat tensor(1.0559, requires_grad=True)\n",
      "ltscale_hat tensor(-1.7734, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.3095, requires_grad=True)\n",
      "yay -1220\n",
      "yay -1280\n",
      "complaint -540\n",
      "complaint -560\n",
      "yay -1340\n",
      "complaint -580\n",
      "complaint -600\n",
      "complaint -620\n",
      "yay -1400\n",
      "complaint -640\n",
      "complaint -660\n",
      "yay -1460\n",
      "complaint -680\n",
      "epoch 500 loss = 1112.0868510802586; mean_loss= 1106.0201739615904; ()\n",
      "mode_hat tensor(1.1368, requires_grad=True)\n",
      "ltscale_hat tensor(-2.1176, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.5141, requires_grad=True)\n",
      "complaint -700\n",
      "yay -1520\n",
      "yay -1580\n",
      "yay -1640\n",
      "yay -1700\n",
      "yay -1760\n",
      "epoch 600 loss = 1072.0033892591796; mean_loss= 1115.8749861685567; ()\n",
      "mode_hat tensor(1.1039, requires_grad=True)\n",
      "ltscale_hat tensor(-2.4304, requires_grad=True)\n",
      "ldfraw_hat tensor(-0.6925, requires_grad=True)\n",
      "yay -1820\n",
      "complaint -720\n",
      "complaint -740\n",
      "yay -1880\n",
      "complaint -760\n",
      "complaint -780\n",
      "yay -1940\n",
      "complaint -800\n",
      "complaint -820\n",
      "complaint -840\n",
      "yay -2000\n",
      "complaint -860\n",
      "complaint -880\n",
      "> c:\\users\\jameson\\dropbox\\eipython\\eipython\\multisitet.py(628)fix_df_grad()\n",
      "-> if torch.any(torch.isnan(ddfr.grad)):\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "#vals = MIDDF35\n",
    "guidenames = [\"unparametrized_laplace\",\"fully_amortized\",]#\"meanfield\",\"amortized_laplace\",]#,\"unamortized_laplace\",\"meanfield\",]#,\"MLE\"]#,\"laplace\"\n",
    "if True:\n",
    "\n",
    "    for aniter in range(5):\n",
    "        for trueparams in [ndom_fat_params]:#,ndom_norm_params]:#,tdom_fat_params,tdom_norm_params,]:\n",
    "            for nsamps,nparticles in [(10,1),(50,1),(100,1),(399,1),(100,3),]:#(10,1),(10,3),(50,1),(50,3),(100,1),(400,1)]:#(44,5)]:#]:#,\n",
    "                    for guidename in guidenames:\n",
    "                        #\n",
    "\n",
    "                        print(aniter,nparticles,guidenames,trueparams)\n",
    "                        result = trainGuide(guidename,nparticles,trueparams,\n",
    "                                            filename=\"testresults/demoT_2.csv\",\n",
    "\n",
    "                                            subsample_N = nsamps,\n",
    "                                           name_offset=10)\n",
    "                        print(aniter,nparticles,guidenames)\n",
    "                        for line in range(10):\n",
    "                            print()\n",
    "                    \n",
    "else:\n",
    "    plt.plot([getMLE(.01 + x/100,1.,0,1.) for x in range(500)])\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\\n",
    "MCMC, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "/anaconda3/bin/python\n",
      "Reloading polytopize.\n",
      "addHess tested\n",
      "addHess tested\n",
      "making:\n",
      "1\n",
      "testresults/scenario_N44_mu1.0_sigma2.0_nu-1.0.csv from file\n",
      "44\n",
      "tensor([32.5364, -3.3454,  3.2929, -2.5415])\n",
      "{'hessian': tensor([[1., 0., 1.],\n",
      "        [0., 2., 0.],\n",
      "        [1., 0., 1.]]), 'logPosterior': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"running...\")\n",
    "\n",
    "import sys, os\n",
    "\n",
    "print(sys.executable)\n",
    "#print(sys.path)\n",
    "\n",
    "from importlib import reload\n",
    "import multisiteT #import *\n",
    "reload(multisiteT)\n",
    "from multisiteT import *\n",
    "import cProfile as profile\n",
    "\n",
    "print(\"making:\")\n",
    "\n",
    "multisiteMod = createScenario(tdom_fat_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-78d7c46ccca3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-78d7c46ccca3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    multisiteMod.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "multisiteMod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27889007329940796\n"
     ]
    }
   ],
   "source": [
    "maxError = torch.max(errors)\n",
    "print(float(maxError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
